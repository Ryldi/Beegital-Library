-- phpMyAdmin SQL Dump
-- version 5.2.1
-- https://www.phpmyadmin.net/
--
-- Host: 127.0.0.1
-- Generation Time: Jun 14, 2024 at 06:20 AM
-- Server version: 10.4.32-MariaDB
-- PHP Version: 8.2.12

SET SQL_MODE = "NO_AUTO_VALUE_ON_ZERO";
START TRANSACTION;
SET time_zone = "+00:00";


/*!40101 SET @OLD_CHARACTER_SET_CLIENT=@@CHARACTER_SET_CLIENT */;
/*!40101 SET @OLD_CHARACTER_SET_RESULTS=@@CHARACTER_SET_RESULTS */;
/*!40101 SET @OLD_COLLATION_CONNECTION=@@COLLATION_CONNECTION */;
/*!40101 SET NAMES utf8mb4 */;

--
-- Database: `db_test`
--

-- --------------------------------------------------------

--
-- Table structure for table `ms_file`
--
-- Creation: Mar 22, 2024 at 07:14 AM
--

CREATE TABLE `ms_file` (
  `file_id` int(11) NOT NULL,
  `file_name` varchar(255) NOT NULL,
  `file_content` mediumtext NOT NULL,
  `file_url` varchar(255) NOT NULL,
  `file_content_vector` text NOT NULL
) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4 COLLATE=utf8mb4_general_ci;

--
-- Dumping data for table `ms_file`
--

INSERT INTO `ms_file` (`file_id`, `file_name`, `file_content`, `file_url`, `file_content_vector`) VALUES
(3, 'Signed-Perturbed_Sums_Estimation_of_ARX_Systems__Exact_Coverage_and_Strong_Consistency_(Extended_Version)', '4202beF81]YS.ssee[1v82511.2042:viXraSigned-Perturbed Sums Estimation of ARX Systems:Exact Coverage and Strong ConsistencyExtended Version∗Algo Car`e†Erik Weyer‡Bal´azs Cs. Cs´aji§Marco C. Campi†AbstractSign-Perturbed Sums (SPS) is a system identiﬁcation method that constructs conﬁdenceregions for the unknown system parameters. In this paper, we study SPS for ARX systems,and establish that the conﬁdence regions are guaranteed to include the true model parameterwith exact, user-chosen, probability under mild statistical assumptions, a property that holdstrue for any ﬁnite number of observed input-output data. Furthermore, we prove the strongconsistency of the method, that is, as the number of data points increases, the conﬁdence regiongets smaller and smaller and will asymptotically almost surely exclude any parameter valuediﬀerent from the true one. In addition, we also show that, asymptotically, the SPS regionis included in an ellipsoid which is marginally larger than the conﬁdence ellipsoid obtainedfrom the asymptotic theory of system identiﬁcation. The results are theoretically proven andillustrated in a simulation example.∗The work of A. Car`e was supported in part by the Australian Research Council (ARC) under Grant DP130104028and in part by the European Research Consortium for Informatics and Mathematics (ERCIM). The work of E. Weyerwas supported in part by the Australian Research Council (ARC) under Grant DP130104028. The work of B. Cs.Cs´aji was supported in part by the European Union within the framework of the National Laboratory for AutonomousSystems, RRF-2.3.1-21-2022-00002, and by the TKP2021-NKTA-01 grant of the National Research, Developmentand Innovation Oﬃce (NRDIO), Hungary. The work of M. C. Campi was supported in part by the Ministry ofUniversity and Research (MUR), PRIN 2022 Project N.2022RRNAEX (CUP: D53D23001440006).†Dept. of Information Engineering, University of Brescia, IT (algo.care@unibs.it, marco.campi@unibs.it).‡Dept. of Electrical and Electronic Engineering, The University of Melbourne, AU (ewey@unimelb.edu.au).§Inst. for Computer Science and Control (SZTAKI), Hungarian Research Network (HUN-REN), Budapest, HU;Dept. of Probability Theory and Statistics, E¨otv¨os Lor´and University (ELTE), Budapest, HU (csaji@sztaki.hu).1      1IntroductionEstimating parameters of unknown systems based on noisy observations is a classical problemin system identiﬁcation, as well as signal processing, machine learning and statistics. Standardsolutions such as the method of least squares (LS) or, more generally, prediction error methodsIn many situations (for example when the safety, stability or qualityprovide point estimates.of a process has to be guaranteed), a point estimate needs to be complemented by a conﬁdenceregion that certiﬁes the accuracy of the estimate and serves as a basis for ensuring robustness. Ifthe noise is known to belong to a given bounded set, set membership approaches can be used tocompute the region of the parameter values that are consistent with the observed data, see e.g.,[39, 41, 32, 40, 46, 12, 29]. The need for deterministic priors on the noise can be relaxed by workingin a probabilistic framework, see e.g., [27, 16]. However, traditional methods for construction ofconﬁdence regions in a probabilistic setting rely on approximations based on asymptotic results andare valid only if the number of observed data tends to inﬁnity, see e.g., [37]. In spite of the well-known fact that evaluating ﬁnite-sample estimates based on asymtptic results can lead to misleadingconclusions [22], the study of the ﬁnite-sample properties of system identiﬁcation algorithms hasremained a niche research topic until recent times.In this regard, a notable (and now-expanding) literature has investigated the connection betweencertain characteristics of the system model at hand and the rates at which the system parameterscan be learnt from data. Seminal works in this line, which addressed in particular the learning ofﬁnite impulse response (FIR) and autoregressive exogenous (ARX) models, are [67, 23, 68, 65, 59];nowadays, several ﬁnite-sample studies for various classes of linear [43, 49, 28, 42, 54, 55, 47, 51] andnonlinear [60, 21, 48, 38] systems are available, also in connection to relevant control frameworks,[1, 6, 18, 20], see [56] for a recent survey and more references. While conﬁdence regions for theunknown parameters are easily obtained as valuable side products of the investigations mentionedabove, these regions are typically conservative as they have rigid shapes, parametrised by someknown characteristics of the system or of the noise, and their validity relies on uniform bounds. Onthe other hand, the goal of producing nonconservative conﬁdence regions by exploiting the observeddataset along more ﬂexible approaches was pursued by another, complementary research eﬀort, aproduct of which is the Sign-Perturbed Sums (SPS) algorithm, which forms the subject of this paper.SPS was introduced in [13] with the aim of constructing ﬁnite-sample regions that include theunknown parameter with an exact, user-chosen probability in a quasi distribution-free set-up. Thereader is referred to [11] for a discussion on the mutual relation between SPS and other ﬁnite-samplemethods, such as the bootstrap-style Perturbed Dataset Methods of [34] and the Leave-Out Sign-Dominant Correlation Regions (LSCR) method, a (more conservative) predecessor of SPS that wasintroduced in [9] and then extended to quite general classes of systems, and applied in a variety of2contexts, see e.g., [17, 25, 2, 26].SPS was studied from a computational point of view in [33] and extended to a distributedset-up in [69]. Applications of SPS can be found in several domains, ranging from mechanicalengineering [62, 63] and technical physics [19, 24, 61], to wireless sensor networks [8] and socialsciences [53]. Moreover, the SPS idea constitutes a core technology of several recent algorithms,including techniques for state estimation [44], for the identiﬁcation of state-space systems [3, 52],error-in-variables systems [30, 31], and for kernel-based estimation [15, 4].From the strictly theoretical point of view, SPS was studied in [14] for linear regression modelswhere the regressors are independent of the noise, which is, in particular, the case for open-loop FIRsystems. In that setting, it was shown that SPS provides exact conﬁdence regions for the parametervector and it guarantees the inclusion of the least-squares estimate (LSE) in the conﬁdence region.The main assumptions on the noise in [14] are that it forms an independent sequence and that itsdistribution is symmetric about zero; however, the distribution is otherwise unknown and it canchange, even in each time-step.At the beginning of this paper, we extend the SPS method to autoregressive exogenous (ARX)systems and show that it has the same ﬁnite-sample properties as SPS for FIR systems. In therest of the paper, we develop an asymptotic analysis of the extended SPS method. Although thecharacterising property of SPS is that of providing exact, ﬁnite-sample guarantees, its asymptoticproperties are also of interest because they shed light on the capability of SPS to exploit the in-formation carried by a growing amount of data. For example, they play a role in the importantproblem of detecting model misspeciﬁcations, see [10]. The asymptotic analysis of the SPS algo-rithm of [14] was carried out in [66], but that analysis does not apply to the ARX case becauseof the existing correlation between the regressor vector and the system output. In this paper weshow that also SPS for ARX systems is strongly consistent, in the sense that the conﬁdence regionshrinks around the true parameter and, asymptotically, all parameter values diﬀerent from the trueone will be excluded. Moreover, the asymptotic size and shape of the SPS conﬁdence region isshown to be included in a marginally inﬂated version of the conﬁdence ellipsoids obtained usingasymptotic system identiﬁcation theory.Structure of the paperThe paper is organized as follows. In the next section we introduce the problem setting and recallthe least-squares estimate for ARX models. Then, in Section 3, the SPS method for ARX systems ispresented along with its fundamental ﬁnite-sample properties (Theorem 1). The asymptotic resultsare provided in Section 5 (Theorems 2 and 3). A simulation example illustrating the theoreticalproperties is given in Section 6, and conclusions are drawn in Section 7. The proofs of the theorems3are all postponed to Appendices.A preliminary version of the SPS algorithm for ARX systems was presented in [13], where atheorem on its ﬁnite-sample guarantees was proven under slightly stronger assumptions than thoseof Theorem 1 in this paper. The Strong Consistency Theorem (Theorem 2) and the AsymptoticShape Theorem (Theorem 3) are stated and proven in this paper for the ﬁrst time.2 Problem Setting2.1 Data generating system and problem formulationThe data generating ARX system is given byYt + a∗1Yt−1 +a∗naYt−na = b∗1Ut−1 +b∗nbUt−nb + Nt,· · ·· · ·(1)R is the output, Ut ∈where Yt ∈written in linear regression form asR the input and Nt ∈R the noise at time t. Equation (1) can beYt = ϕTt θ∗ + Nt,ϕt , [1, . . . ,Ytθ∗ , [ a∗1, . . . , a∗na, b∗1, . . . , b∗nb ]T.na, UtYt−−−−−1, . . . , Utnb ]T,(2)(3)(4)−Aim: Construct a conﬁdence region with a user-chosen coverage probability p for the true pa-rameter θ∗ from a ﬁnite sample of size n, that is, from the regressors ϕ1, . . . , ϕn and the outputsY1, . . . , Yn.We make the following two assumptions.Assumption 1. θ∗ is a deterministic vector, and the orders na and nb are known.nb in ϕ1) and the input se-Assumption 2. The initial conditions (Y0, . . . , Y1quence U1, . . . , Un are deterministic, and the stochastic noise sequence N1, . . . , Nn is symmetrically, t = 1, 2, . . . , n, the joint probability distribu-distributed about zero (that is, for every st ∈ {tion of (s1N1, . . . , snNn) is the same as that of (N1, . . . , Nn)), and otherwise generic.na and U0, . . . , U11−1,}−−Assumption 2 implies that E[Nt] = 0 and that noise samples are uncorrelated, i.e., E[NtNt−τ∀= 0, when the expected values exist. However, independence ofcan be a function of the past valuesτ ] =is not assumed (e.g.,, . . .). Moreover, the marginal0,the value ofNt}{,Nt+1||Nt||Nt|1|−46distribution of Nt can be time-varying (that is, the noise is not necessarily identically distributed).The assumption that the input is deterministic corresponds to an open-loop conﬁguration. We alsois stochastic and thenote that the results remain valid with some additional generality whenassumption that N1, . . . , Nn is symmetrically distributed about zero holds conditionally onUt}{.Ut}{2.2 Least-squares estimate (LSE)Let θ be a generic parameterθ = [ a1, . . . , ana, b1, . . . , bnb ]T.and let d = na + nb be the number of elements in θ. Let the predictors be given byand the prediction errors byˆYt(θ) , ϕTt θ,ˆNt(θ) , Yt −The LSE is found by minimising the sum of the squared prediction errors, that is,ˆYt(θ) = Yt −t θ.ϕTˆθn , arg minθRd∈nt=1XˆN 2t (θ) = arg minRdθ∈nt=1X(Yt −t θ)2.ϕTThe solution can be found by solving the normal equation,ϕt ˆNt(θ) =nt=1Xnt=1Xϕt(Yt −ϕTt θ) = 0,which, whennt=1 ϕtϕTtis invertible, has the (unique) solutionPˆθn =nϕtϕTt1−n(cid:18)t=1X(cid:19)(cid:18)t=1XϕtYt.(cid:19)3 Construction of an Exact Conﬁdence Region(5)(6)(7)(8)(9)Before presenting the SPS method for ARX systems, we ﬁrst recall the construction of the conﬁdencenb]T are independent ofregion for FIR systems when na = 0 and the regressors ϕt = [Utthe noise sequence (this is the case dealt with in [14]).1, . . . , Ut−−53.1 SPS when the regressors are independent of the noiseThe fundamental step of the SPS algorithm consists in generating m1 sign-perturbed sums byrandomly perturbing the signs of the prediction errors in the normal equation (8), that is, fori = 1, . . . , m1, we deﬁne−−Hi(θ) ==nt=1Xnt=1Xαi,tϕt(Yt −ϕTt θ)αi,tϕtϕTt˜θ +αi,tϕtNt,nt=1Xare random signs, i.e., i.i.d. random variables that take on the valueswhere ˜θ = θ∗θ, and−αi,t}{±1 with probability 1/2 each. For a given θ, the reference sum is instead deﬁned asH0(θ) =nt=1Xϕt(Yt −ϕTt θ) =ϕtϕTt˜θ +nt=1XϕtNt.nt=1XFor θ = θ∗, these sums can be simpliﬁed tonH0(θ∗) =ϕtNt,t=1XnnHi(θ∗) =αi,tϕtNt =ϕtNt,±±t=1Xt=1Xinstead of αi,t for intuitive understanding. Thewhere in the last equation we have writtencrucial observation is that, since the regressors are independent of the noise, and the noise is jointlysymmetric, it follows that H0(θ∗) and Hi(θ∗) have the same distribution, and there is no reason whyH0(θ∗)1.kk2 is the kth largest one in the orderingIn fact, in [14] it was proven that the probability that2of the m valuesis exactly 1/m, and the probability that it is among the q largestqones is qm was then deﬁned in [14] as the set of θ’s suchHi(θ∗)thatm−i=0}1m . The SPS region with conﬁdence 12 (, H0(θ∗)TH0(θ∗)) should be bigger or smaller than any other2 is not among the qth largest values in the ordering of2, i = 1, . . . , m·H0(θ)H0(θ∗)Hi(θ∗)Hi(θ∗){k−−kkkkk12m1i=0 .−kkAnother crucial observation is the following. For “large enough”k}, we will have that{k˜θkknt=1X(cid:13)(cid:13)(cid:13)(cid:13)n2nϕtϕTt˜θ +ϕtNt>n˜θ +ϕtϕTt±ϕtNt±with “high probability” sinceon the right-hand side. Hence, for˜θϕtϕTt2 dominates in the ordering ofmi=0 , and values away from θ∗ will therefore be excluded from the conﬁdence region,t=1X˜θ on the left-hand side increases faster thannt=1 ϕtϕTt˜θH0(θ)klarge enough,nt=1 ±Hi(θ)t=1Xt=1XPPkkk−21(cid:13)(cid:13)(cid:13)(cid:13){ksee [66] for a detailed analysis.k}2,(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)63.2 Main idea behind SPS for ARX systems{ϕtNt}is diﬀerent from the distribution of the perturbed one,In the ARX case, the idea illustrated above cannot be applied directly since the distribution of theαi,tϕtNt}unperturbed sequence,because ϕt depends on the unperturbed noise. Therefore, the distribution of H0(θ∗) is diﬀerentfrom that of Hi(θ∗), i = 1, . . . , m1. A key idea in the SPS algorithm for ARX systems is to generateregressors, denoted ¯ϕi,t(θ), such thathave the same distribution. Theelements of ¯ϕi,t(θ) include, instead of the observed outputs, the outputs of the system correspondingto the parameter θ fed with the perturbed noise. Thus, the perturbed output sequence¯Yi,1(θ), . . . , ¯Yi,n(θ) is generated for every θ = [ a1, . . . , ana, b1, . . . , bnb ]T according to equationαi,t ¯ϕi,t(θ∗)Nt}ϕtNt}αi,tNt(θ)Nt}and−{{{{{}¯Yi,t(θ) + a1 ¯Yi,t−1(θ) +· · ·ana¯Yi,t−na(θ) , b1Ut−1 +bnbUt−· · ·nb + αi,t ˆNt(θ),(10)where ˆNt(θ) is given by (6), and the initial conditions for ¯Yi,t(θ) are ¯Yi,t(θ) , Yt for 1The re-generated regressor is then given asna ≤t−≤0.¯ϕi,t(θ) , [¯Yi,t−1(θ), . . . ,¯Yi,t−−na(θ), Ut−1, . . . , Ut−nb]T.(11)−Using this perturbed regressor, the analogue of functions H0(θ) and Hi(θ) deﬁned above can beconstructed, and we denote them S0(θ) and Si(θ) to avoid confusion. Moreover, for θ = θ∗, S0(θ∗)and Si(θ∗) have the same ordering property as H0(θ∗) and Hi(θ∗) for FIR systems.3.2.1 SPS for ARX systemsThe SPS method for ARX systems is now detailed in two distinct parts. The ﬁrst, which is called“initialisation”, sets the main global parameters of SPS and generates the random objects needed forthe construction. In the initialisation, the user provides the desired conﬁdence probability p. Thesecond part evaluates an indicator function which decides whether or not a particular parametervalue θ is included in the conﬁdence region.nnnThe pseudocode for the initialisation and the indicator function is given in Tables 1 and2, respectively. Note that in point 4 of Table 2, in the computation of S0(θ) and Si(θ), thevectors 1nαi,t ¯ϕi,t(θ) ˆNt(θ) have been premultiplied by the matrices R−nϕt ˆNt(θ) and 1n( 1nconﬁdence regions, as discussed in Section 5.3. The permutation π in point 3 in the initialisation2 or(Table 1) is only used in the indicator function (Table 2) to decide which function2 take on the same value. More precisely, given m2 . The reason is that this results in better shaped2 is the “larger” ift=1Pi,n (θ) = ( 1n¯ϕi,t(θ) ¯ϕT2 and R−t=1Pt )−i,t(θ))−2 andϕtϕTn =Si(θ)t=1Pt=1P||||121211Sj(θ)Sj(θ)Si(θ)||||||||||||7real numbers, i = 0, . . . , mZi}{−1, we deﬁne a strict total order≻π byZk ≻π Zj if and only if( Zk > Zj ) or ( Zk = Zj and π(k) > π(j) ) .(12)The p-level SPS conﬁdence region with p = 1qm is given as−Θn ,θ∈Rd : SPS-INDICATOR( θ ) = 1.(cid:8)b(cid:9)Observe that the LS estimate, ˆθn, has by deﬁnition the property that S0(ˆθn) = 0. Therefore,the LSE is included in the SPS conﬁdence region, except for the very unlikely situation in whichq other Si(θ) functions (besides S0(θ)) are null at ˆθn and ranked smaller than S0(θ) by π.1m−Pseudocode: SPS-Initialization1. Given a (rational) conﬁdence probability p(0, 1),set integers m > q > 0 such that p = 1−2. Generate n(m1) i.i.d. random signswith·−P(αi,t = 1) = P(αi,t =∈q/m;αi,t}{1) = 12,−1, . . . , n};for i1, . . . , m1and t}3. Generate a random permutation π of the set∈ {∈ {−0, . . . , m1, where each of the m! possible{permutations has the same probability 1/(m!)−}to be selected.Table 11It is worth mentioning some substantial diﬀerences between the construction here proposed and the one of [64].In [64], extra data (the so-called instrumental variables) are assumed to be available to the user, and are required toNt. Under this condition, the construction of [64] deliversbe correlated withguaranteed regions around the instrumental-variable estimate. On the other hand, the algorithm proposed in thispaper does not require any extra data besides the regressors and the system outputs, and is purely LSE-based.but independent of the noiseϕt{}{}8Pseudocode: SPS-Indicator ( θ )1. For the given θ, compute the prediction errorsfor t∈ {2. Build m1, . . . , n}ˆNt(θ) , Yt −1 sequences of sign-perturbedt θ;ϕT−prediction errors (αi,t ˆNt(θ)), t = 1, . . . , n3. Construct m1 perturbed output trajectories−¯Yi,1(θ), . . . , ¯Yi,n(θ),i = 1, . . . , m1,−according to equation (10) with¯Yi,t(θ) , Yt for 1na ≤t−≤0.Form ¯ϕi,t(θ) according to (11).4. EvaluateS0(θ) , R−n12n1nt=1Pi,n (θ) 1n12ϕt ˆNt(θ),nαi,t ¯ϕi,t(θ) ˆNt(θ),Si(θ) , R−for i∈ {t=1P, where1, . . . , m1−n}ϕtϕTt ,nRn , 1nt=1PRi,n(θ) , 1nt=1P¯ϕi,t(θ) ¯ϕTi,t(θ),1and −2 denotes the inverse (or pseudoinverse) ofthe principal square root matrix.5. Order scalars{k6. Compute the rankSi(θ)2according tok}(θ) ofS0(θ)≻π (see (12));2 in the ordering,RS0(θ)kk2 is the smallest in thewhereRordering,(θ) = 1 ifk(θ) = 2 ifkS0(θ)Rsmallest, and so on.k2 is the secondk6. Return 1 if(θ)R≤m−q, otherwise return 0.Table 294 Exact ConﬁdenceLike its FIR counterpart, the SPS algorithm for ARX system generates conﬁdence regions that haveexact conﬁdence probabilities for any ﬁnite number of data points. The following theorem holds.{θ∗= 1Theorem 1. Under Assumptions 1 and 2, the conﬁdence region constructed by the SPS algorithmin Table 1 and 2 has the property that PrΘn}bThe proof of Theorem 1 can be found in Appendix A.1. The simulation examples in Section 6also demonstrate that, when the noise is stationary, the SPS conﬁdence regions compare in size withthe heuristic conﬁdence regions obtained by applying the asymptotic system identiﬁcation theory.However, unlike the asymptotic regions, the SPS regions are theoretically guaranteed for any ﬁniten, and also maintain their guaranteed validity with nonstationary noise patterns.qm−∈5 Asymptotic ResultsIn addition to the probability of containing the true parameter, another important aspect is thesize and the shape of the SPS conﬁdence regions.In this section, under some additional mildassumptions, we prove i) a Strong Consistency theorem which guarantees that the SPS conﬁdencesets get smaller and smaller as the number of data points gets larger, and ii) an Asymptotic ShapeTheorem stating that, as both n and m tend to inﬁnity, the conﬁdence sets are included in amarginally inﬂated version of those produced by the asymptotic system identiﬁcation theory.These two theorems will be proved under the basic Assumptions 1 and 2, plus some assumptionsthat are now introduced and discussed. We ﬁrst discuss the common identiﬁability assumptionsunder which both the strong consistency and the asymptotic shape results are proven. Then, weisolate and discuss an additional simplifying assumption that is only used to prove the asymptoticshape theorem.5.1 AssumptionsLet us deﬁne A(z−the delay operator. In this way, (1) can be compactly rewritten as1; θ) = 1 + a1z−na and B(z−+ anaz−1 +· · ·1; θ) = b1z−1 +· · ·+ bnbz−nb, with z−1and (10) asA(z−1; θ∗)Yt , B(z−1; θ)Ut + Nt,A(z−1; θ) ¯Yi,t(θ) , B(z−1; θ)Ut + αi,t ˆNt(θ).10The following one is a standard assumption for identiﬁability of the “true” parameter.Assumption 3 (coprimeness). The polynomials A(z−1; θ∗), B(z−1; θ∗) are coprime.The set of values of θ that are allowed to be included in the conﬁdence region is normally limitedby a priori knowledge on the system and, in general, it will be a proper subset of Rd. Althoughoccasionally it can be left implicit, in this paper the subset of values of θ will be denoted by Θc andalways assumed to be a compact set.Assumption 4 (uniform-stability). The families of ﬁltersare uniformly stable.{1A(z−1;θ) : θΘc}∈and{B(z−1;θ)A(z−1;θ) : θΘc}∈We brieﬂy recall the deﬁnition of uniform stability, see [37] for details. First,A(z−1;θ) andΘc. Then, we can deﬁne the coeﬃcients h0(θ), h1(θ), . . .t = B(z−1;θ)A(z−1;θ) .A(z−1;θ), and g1(θ), g2(θ), . . . from relation∞t=1 gt(θ)z−∈1B(z−1;θ)A(z−1;θ) must be stable for every θ1t =from relationUniform stability means thatP∞t=0 ht(θ)z−PsupθΘc |∈ht(θ)| ≤¯htandsupθΘc |∈gt(θ)¯gt,t,∀| ≤for some ¯ht and ¯gt such that∞¯ht <t=0X∞and∞t=1X¯gt <.∞Basically, Assumption 4 excludes that the dynamics of the system can be arbitrarily slow and thatthe static gain can be arbitrarily large.The following type of conditions are standard for consistency analysis.Assumption 5 (independent noise, moment growth rate).dom variables. Moreover, the limitNt}{is a sequence of independent ran-exists and1nlim supn→∞1nlimn→∞nE[N 2t ]t=1XnE[N 8t ] <.∞t=1X(13)(14)We will assume that the input sequence is persistently exciting (p.e.). Precisely, following[35], we say that the input sequence is persistently exciting of order na + nb if the limits m =11limnare ﬁnite for every k, and the matrix→∞nt=1 Ut (we call m the mean) and cU,k = limn1nPcU,0...cU,na+nb−11. . . cU,na+nb−. . .. . ....cU,0is positive deﬁnite.P1n→∞nt=1(Ut −m)(Ut−k −m) exist andAssumption 6 (persistent excitation and limited growth rate). The sequenceexciting of order na + nb. Moreover,Ut}{is persistently1nlim supn→∞nt=1XU 4t <.∞(15)The reader may be interested in comparing the realisation-wise condition (15) and the process-wise condition (14): in this regard, it is worth noting that the process-wise condition (14) impliesthatholds with probability 1.1nlim supn→∞nN 4t <∞t=1X(16)In all that follows, we will consider Yt to be the output of system (1) with zero initial conditions0).and causal input signals (Nt, Ut are zero for t≤5.1.1 Extra assumption for the asymptotic shapeRemarkably, stationarity of the noise terms has not been assumed so far. Present assumptionsallow us to prove the strong consistency result. However, to show that the conﬁdence sets of SPSasymptotically have the same shape as the standard conﬁdence ellipsoids, we will work under thefollowing additional simplifying assumption.Assumption 7 (i.i.d. noise sequence). The sequencedistributed random variables.Nt}{is made up of independent and identicallyWe will denote E[N 2t ] by σ2.5.2 Strong consistencyOur ﬁrst result shows that SPS is strongly consistent, in the sense that the conﬁdence sets shrinkaround the true parameter as the sample size increases, and eventually exclude any other parametersθ′= θ∗.126In the following theorem, Bε(θ∗) denotes the closed Euclidean norm-ball centred at θ∗ withradius ε > 0, i.e.Bε(θ∗) ,θ{Theorem 2 states that the conﬁdence regionscentred at the true parameter, θ∗.Rd :θθ∗ε.k∈Θn will eventually be included in any given norm-ballk ≤−}bTheorem 2 (Strong Consistency). Under the assumptions 1, 2, 3, 4, 5, for all ε > 0Θn ⊆bA detailed proof of the theorem is provided in Appendix A.2, preceded by an outline.n=¯n n\\Bε(θ∗)¯n=1[= 1.o\"#∞∞Pr5.3 Asymptotic shapeIn this section we study the shape of the SPS conﬁdence regions when n and m tend to inﬁnity, andwe show that the SPS conﬁdence region is included in a marginally inﬂated version of the conﬁdenceellipsoids obtained using asymptotic system identiﬁcation theory. Before we present our results, theconﬁdence ellipsoids based on the asymptotic system identiﬁcation theory are brieﬂy reviewed, seee.g., [37] for a more detailed presentation.5.3.1 Conﬁdence ellipsoids based on the asymptotic system identiﬁcation theory{Nt}zero mean and i.i.d., and under some other technical assumptions, √n (ˆθn −θ∗) can beForproved to converge in distribution to the Gaussian distribution with zero mean and covarianceσ2 (ˆθn −, where σ2 is the variance of Nt, and ¯Rmatrix σ2 ¯R−∗∗(ˆθn−θ∗)T ¯Rθ∗) converges in distribution to the χ2 distribution with dim(θ∗) = d degrees of freedom.∗This result was derived under uniform stability and input boundedness conditions in [37, Chapters8-9].Rn. As a consequence, n= limn→∞1An approximate conﬁdence region can then be obtained by replacing matrix ¯R∗with its ﬁnite-sample estimate Rn,Θn ,θ : (θ(cid:26)ˆθn)T Rn (θˆθn)−≤−µσ2n,(cid:27)eΘn is approximately Fχ2(µ), where Fχ2 iswhere the probability that θ∗ is in the conﬁdence regionthe cumulative distribution function of the χ2 distribution with d degrees of freedom. In the limite13as n tends to inﬁnity, θ∗ is contained in the setoften replaced with its estimateΘn with probability Fχ2(µ). In practice, also σ2 isenσ2n, 1n−dϕTtˆθn)2.(yt −t=1XIn the wake of [66], the theorem on the asymptotic shape of p-level SPS regions is here given interms of a relaxed version of the asymptotic conﬁdence ellipsoids, which are deﬁned asbΘn(η) ,θ∈(cid:26)Θc : (θ−ˆθn)TRn(θˆθn)−≤µ σ2 + ηn,(cid:27)e−where µ is such that Fχ2(µ) = p, and η > 0 is a margin. In the theorem, both n and m (recallthat mΘn,m for1 is the number of sign-perturbed sums) go to inﬁnity, and we use the notationthe SPS region to indicate explicitly the dependence on n and m.Θn,m is constructed with theparameter q equal to qm =is the largest integer less than or equalqmp fromto (1−above as mp)m, so that Theorem 1 gives a conﬁdence probability of pm , 1m , and pm →p) m, where⌋p) m⌋(1⌊(1⌊−−−bb.→ ∞Theorem 3 (Asymptotic Shape). Under Assumptions 1, 2, 3, 4, 5, 6 and 7, there exists a doubly-indexed set of random variablessuch that{εn,m}limm→∞εn,m = 0 w.p.1,limn→∞and, for every m, n, it holds that5.3.2 Comment on the proofΘn,m ⊆bΘn(εn,m).ekkkk−−S0(θ)2 andkSi(θ)2, i = 1, . . . , mSi(θ)2, i = 1, . . . , mThe proof is deferred to Appendix A.3. Although from a bird’s eye point of view the proof followsthe same line as the proof in [66], from a technical point of view a crucial diﬀerence is that, while in1, were all quadratic functions, in the present[66] functionsk1, are ratios of polynomials whose orders increase linearlysetting functionswith n, so that the uniform evaluation of these functions over θ as nrequires much moreattention. Nevertheless, results similar to those in [66] can be obtained by exploiting i) the linearityof the system, ii) the continuity (to be deﬁned in a suitable sense, and proven to hold) of the¯Yi,t(θ)with respect to small variations of θ, and iii) Theorem 2, which guarantees that,sequenceas n grows, the eﬀects ofhave to be evaluated in a smaller and smaller neighbourhoodof θ∗. Another caveat is that independence conditions that were satisﬁed in the FIR case are lostin the ARX case. Nonetheless, similar results can be proven thanks to random sign-perturbations,which play a crucial role in making theorems on martingales applicable to our setting.¯Yi,t(θ)→ ∞}{}{146 Numerical ExampleConsider the following simple data generating systemYt =a∗Yt1 + b∗Ut−1 + Nt,−−with zero initial conditions. a∗ =is asequence of i.i.d. Laplacian random variables with zero mean and variance 0.1. The input signalwas generated according to0.7 and b∗ = 1 are the true system parameters andNt}−{Ut = 0.75 Ut−1 + Vt,Vt}wherepredictor is given by{was a sequence of i.i.d. Gaussian random variables with zero mean and variance 1. TheYt(θ) =a1Yt1 + bUt−−1 = ϕTt θ,−Yt−where θ = [ a b ]T is the model parameter, and ϕt = [b1 Ut−1 ]T is the regressor at time t.−A 95 % conﬁdence region for θ∗ = [a∗ b∗]T based on n = 40 data points, namely (ϕt, Yt),t = 1, . . . , 40, was constructed by choosing m = 100 and leaving out those values of θ for whichS0(θ)was among the 5 largest values ofS99(θ)S0(θ)S1(θ), . . . ,,.kkkkkkkkTrue valueLSEAsymptoticSPS1.11.081.061.04b1.0210.980.960.94-0.76-0.74-0.72-0.7-0.66-0.64-0.62-0.6-0.68aFigure 1: 95% conﬁdence regions, n = 40, m = 100.The SPS conﬁdence region is shown in Figure 1 together with the approximate conﬁdenceellipsoid based on asymptotic system identiﬁcation theory (with the noise variance estimated asσ2 = 138ˆθn)2).ϕTt40t=1(Yt −Pb15True valueLSEAsymptoticSPS1.11.081.061.04b1.0210.980.960.94-0.76-0.74-0.72-0.7-0.66-0.64-0.62-0.6-0.68aFigure 2: 95% conﬁdence regions, n = 400, m = 100.1.11.081.061.04b1.0210.980.960.94True valueLSEAsymptoticSPS (m=4000)SPS (m=100)zoomed region1.021.0151.011.00510.9950.990.985-0.705-0.7-0.695-0.69-0.76-0.74-0.72-0.7-0.66-0.64-0.62-0.6-0.68aFigure 3: 95% conﬁdence regions, n = 4000, m = 4000 and m = 100.16It can be observed that the non-asymptotic SPS region is similar in size and shape to theasymptotic conﬁdence region, but it has the advantage that it is guaranteed to contain the trueparameter with exact probability 95%.In agreement with Theorem 2, the size of the region decreases when n is increased, see Figures2 and 3. In Figure 3, m is also increased to 4000, and one can observe that there is very littlediﬀerence between the SPS region and the asymptotic conﬁdence ellipsoid demonstrating the resultestablished in Theorem 3.7 Concluding Remarks and Open ProblemsIn this paper, we have presented the SPS method for ARX systems. SPS delivers conﬁdence regionsaround the least-squares estimate that contain with exact, user-chosen, probability the true systemparameter under mild assumptions on the data generation mechanism. These regions are builtfrom a ﬁnite (and possibly small) sample of input-output data. Besides the exact ﬁnite-sampleguarantees, we have proven under additional and rather mild assumptions that the method isstrongly consistent and that the conﬁdence regions are included in a slightly enlarged version of theapproximate conﬁdence ellipsoids obtained using the asymptotic theory.Finally, we want to mention an important further direction of research. While the SPS regionshave many desirable features, the exact calculation of the regions is computationally demanding. ForFIR systems, an eﬀective ellipsoidal outer approximation of the conﬁdence regions can be practicallycomputed by using convex programming techniques ([14], see also [33]). Obtaining similar resultsfor ARX system is an ongoing challenge of great practical importance. On the other hand, the SPSalgorithm presented in this paper lends itself nicely to problems where the indicator function has tobe evaluated for a ﬁnite, moderate set of values of θ, which is the case in certain change detectionor hypothesis testing problems. Moreover, when the dimension of the parameter vector θ is small,the SPS region can be computed by checking whether points on a ﬁne grid of the parameter spacebelong to the conﬁdence set.A ProofsA.1 Proof of Theorem 1: exact conﬁdenceWe begin with a deﬁnition and two lemmas taken from [14].17Deﬁnition 1. Let Z1, . . . , Zk be a ﬁnite collection of random variables andIf for all permutations i1, . . . , ik of indices 1, . . . , k we have≻t.o. a strict total order.P(Zik ≻t.o. Zik−1 ≻ · · · ≻t.o. Zi1) =1k!,then we callZi}{uniformly ordered w.r.t. order≻t.o..Lemma 1. Let α, β1, . . . , βk be i.i.d. random signs, then the random variables α, αare i.i.d. random signs.·β1, . . . , αβk·The following lemma highlights an important property of thein Section 3.≻π relation that was introducedLemma 2. Let Z1, . . . , Zk be real-valued, i.i.d. random variables. Then, they are uniformly orderedw.r.t.≻π.We are now ready to prove Theorem 1.By construction, the parameter θ∗ is in the conﬁdence region if1, . . . , mthatprobability 1/m, thus its rank is at most mk≻π) of the variableskq with probability 1mi=0 are uniformly ordered, henceq in the ascending order (w.r.t.−Si(θ∗)S0(θ∗){kk}k−21S0(θ∗)2 takes one of the positionsmi=0 . We will prove}2 takes each position in the ordering withkSi(θ∗){kk−21−Note that all the functions Si(θ∗) depend on the sequencevia the same function forall i, which we denote as S(αi,1N1, . . . , . . . , αi,nNn) , Si(θ∗). This is true also for S0(θ∗); in fact,, it holds that α0,t ˆNt(θ∗) = α0,tNt = Nt, so ¯Y0,t = Yt andrecalling that α0,t , 1, t¯ϕ0,t = ϕt.1, . . . , n}∈ {{q/m.−αi,tNt}Let b1, b2, . . . be a sequence of random signs independent ofandNt}αi,t}{, and deﬁne σt(Nt)asσt(Nt) =sign(Nt)(bt{= 0if Nt 6if Nt = 0{Nt|· |σt(Nt)}for every value of Nt. By Assumption 2,{is an i.i.d. sequence. Now, we will work conditioning onClearly, Nt = σt(Nt)are, byindependent, andfrom all the other random elements: let us ﬁx a realisation ofexploiting the independence ofNt|}(all the other random elements are distributed according to their marginal{|distribution). Then, for all i and t, we introduce γi,t = αi,tσt(Nt).1 are i.i.d.random signs independent of the other random elements and of σt(N1), . . . , σt(Nn) in particular.1, t = 1, . . . , n, are i.i.d. random signs. Thus, Si(θ∗) canUsing Lemma 1, γi,t, i = 0, . . . , mσt(Nt)}Nt|}, i = 1, . . . , m, and call itNt|}αi,t}Nt|}vt}and{|{|{|−{{−18be equivalently expressed as Si(θ∗) = Zi, where Zi , S(γi,1v1, . . . , γi,nvn). Since Zi’s are obtainedby applying the same function to diﬀerent realisations of an i.i.d. sample, they are also uniformly≻π (Lemma 2). Thus, the uniform ordering property has been proven for aordered with respect toNt|}ﬁxed realisation ofwas arbitrary, the uniform ordering property. As the realisation of1Si(θ∗)ofmi=0 holds unconditionally, and the theorem follows.Nt|}{|{|−2{kk}A.2 Proof of Theorem 2A.2.1 Outline of the proofWe deﬁne ˆθi,n(θ) as the value of ˆθRd that minimises∈ni=1X( ¯Yi,t(θ)−¯ϕi,t(θ)T ˆθ)2,(17)i.e., as the least-squares estimate if the output sequence were ¯Yi,1(θ), . . . , ¯Yi,n(θ), cf. (7).2 ˆθi,n(θ)satisﬁesnn¯ϕi,t(θ) ¯ϕi,t(θ)T(ˆθi,n(θ)αi,t ˆNt(θ) ¯ϕi,t(θ).(18)1nt=1Xθ) =−1nt=1XAssuming ˆθi,n(θ) is unique (we will show that this is the case for n large enough), it is straightforwardto check that2 can be written asSi(θ)kkwhere Ri,n(θ) = 1nasPSi(θ)2 =1Ri,n(θ)2 (θˆθi,n(θ))2,kkknt=1 ¯ϕi,t(θ) ¯ϕTi,t(θ) (as deﬁned in Table 2). Similarly,−ki = 1, . . . , m1,−(19)S0(θ)kk2 can be rewrittenS0(θ)2 =12Rn (θ2,ˆθn)nkkkθ∗ with probability 1, and hence that−where ˆθn is the least-squares estimate (9), and Rn = 1prove that ˆθn →2 eventually stays away from zerooutside a ball centred at θ∗. The second step is proving the uniform convergence of ˆθi,n(θ) to θ.3 Todo this, we ﬁrst prove that Ri,n(θ), i = 1, . . . , m1, converge uniformly in Θc to a matrix function−¯R(θ) that is positive deﬁnite, with eigenvalues that are uniformly bounded away from 0 and from.∞Second, we show that the right hand side of (18) goes to zero uniformly in Θc with probability 1.4t (as deﬁned in Table 2). First, wekϕtϕTt=1PkS0(θ)kn(20)2In the terminology of [34] this is the minimiser of the cost function corresponding to the “perturbed dataset”.3This requires some caution because ¯Yi,1(θ), . . . , ¯Yi,n(θ) is the output of the non-standard system (10), where Utaﬀects future noise terms through ˆNt+1(θ), ˆNt+2(θ), . . ., and the expected value of ˆN 2t+1(θ) given the past is notuniformly bounded. Thus, traditional consistency results such as those in [36], although quite general and inclusiveof closed-loop set-ups, do not apply to this setting.4This step is carried out by using martingale arguments that are inspired by the proof in [36], together with asuitable “conditioning trick”.19Combining these two facts, we will conclude that ˆθi,n(θ) converges to θ, andThis implies that, for n large enough,the values of θthe conﬁdence region.0 uniformly.2 for allΘc outside a small ball centred at θ∗, so that such values of θ are excluded fromk1 are smaller than2, i = 1, . . . , m→S0(θ)kSi(θ)Si(θ)−∈kkkk2A.2.2 ProofThe following two lemmas are the key results to prove the theorem. In the statements of these twolemmas, the assumptions of Theorem 2 are left implicit. Their proofs are in Appendix A.2.3.Lemma 3. The limit matrix¯R∗, limn→∞1nnϕtϕTtt=1Xexists and is ﬁnite w.p.1. Moreover, there exists a matrix ¯R(θ), independent offunction of θsupθ∈1. ¯R(θ) is continuous in θΘc.5for all θRi,n(θ)Θc k→∞∈Θc, ¯R(θ∗) = ¯R, and there exist ρ1, ρ2 > 0 such that Iρ1 ≺∗Θc, such that limn= 0, i = 1, . . . , mNt}¯R(θ)αi,t},and{1, with probability¯R(θ)Iρ2≺−−∈{k∈Lemma 4. It holds w.p.1 thatMoreover, for every i = 1, . . . , m1,−1nlimn→∞nt=1XNtϕt = 0.holds true with probability one.1nnt=1Xlimn→∞supθΘc (cid:12)∈(cid:12)(cid:12)(cid:12)(cid:12)αi,t ˆNt(θ) ¯ϕi,t(θ)= 0(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)We ﬁrst study the asymptotic behaviour of the quadratic reference functionS0(θ)2.kkBy deﬁnition, the least-squares estimate ˆθn must satisfy the normal equation (see (8))1nnt=1XϕtϕTt (ˆθn −θ∗) =1nnt=1XNtϕt.(21)The convergence (a.s.) of ˆθn to θ∗ follows by taking the norm of both the right- and left-handside of (21) and noting that the right-hand side goes to zero by Lemma 4. On the other hand,5The symbol “positive deﬁnite.≺” denotes the Loewner partial ordering, i.e., given two matrices A and B, AB≺⇐⇒A isB−20because of ¯Rnt=1 ϕtϕT= limn∗and only if ˆθn converges to θ∗. Thus,→∞t ≻P0 (Lemma 3), the left-hand side goes to zero as nUsing (20), we conclude thatˆθn −kθ∗k −→n→∞0 w.p.1.S0(θ)k2k→∞ k−→n1¯R2∗(θ∗θ)k−2 (uniformly in Θc) w.p.1.if→ ∞(22)(23)Now we study the asymptotic behaviour of the functionsSi(θ)2, i = 1, . . . , mkk1.−By deﬁnition, ˆθi,n(θ) satisﬁes (18). By taking the norm of both sides of (18) and by using2 = 0 w.p.1, while, by Lemma 4, we haveΘc, for n large enough. These twoRi,n(θ)(ˆθi,n(θ)ˆθi,n(θ)θsupθ2θ)→∞θ)−k2 for all θk∈−Θc k∈ρ21 · k≥−kLemma 3 we get limnRi,n(θ)(ˆθi,n(θ)supθΘc k∈facts yieldlimn→∞supθΘc k∈ˆθi,n(θ)θk−2 = 0 w.p.1.(24)Using (23) and Lemma 3, we conclude that there exists w.p.1 a (realisation dependent) ¯n0 suchthatS0(θ)kk2 > ρ1ǫ2θ :∀θ||−θ∗||> ǫfor every n > ¯n0. W.p.1, there also exists a (realisation dependent) ¯n large enough such that, for2 < ρ1ǫ2every n > ¯n, Ri,n(θ),ρ21 (Lemma 3), and such thatΘc, i = 1, . . . , mˆθi,n(θ)Iρ2,θk−kθ∀∈1 (24), which implies−≺Θc, i = 1, . . . , m−θ∀∈Si(θ)kk2 < ρ1ǫ2θ∀∈Θc,i = 1, . . . , m1.−Therefore, for every realisation on a set of probability 1, there exist (realisation dependent) ¯n0 and¯n such that for every n > max(¯n0, ¯n) it holds that1, for everyBǫ(θ∗), and this implies the theorem statement.θ /∈, i = 1, . . . , mS0(θ)Si(θ)−>kkkkA.2.3 Proofs of Lemmas 3 and 4Preliminarily, we state some asymptotic results that are useful throughout. In all the lemmas statedin this proof, the assumptions of Theorem 2 are left implicit.Lemma 5. W.p.1 it holds that1.a limn→∞1.b limn→∞δ0 = 11n1nnt=1 Nt = 0Pnt=1 NtNt−k = δk ·limn1n→∞nt=1E[N 2t ]<∞, where δk = 0 for every k= 0 andP(cid:0)(cid:1)P2161.c limn1.d limn1n1n→∞→∞nt=1 NtUt−k = 0 for every kPnt=1 NtYt−k = 0 for every k1.≥For every k∈PZ, there exist cY,k <and cY U,k <∞∞such that, w.p.1,2.a limn2.b limn1n1n→∞→∞nt=1 YtYt−k = cY,k for every kPnt=1 YtUt−k = cY U,k for every k.PW.p.1 it holds that3.a lim supn3.b lim supn1n1n→∞→∞ni=1 Y 4t <Pnt=1 kϕtk∞4 <3.c supθ3.d supθΘc∈Θc∈Plim supn(cid:16)lim supn1n1n→∞→∞∞ˆN 4nt=1t (θ)(cid:17)¯ϕi,t(θ)Pnt=1 k<4k(cid:1)∞≤C <∞, where C depends onNt}{but not on.αi,t}{For every θ(cid:0)Θc and k∈∈PZ, there exist c ¯Y ,k(θ) <and c ¯Y U,k(θ) <∞∞such that, w.p.1,4.a limn4.b limn1n1n→∞→∞nt=1¯Yi,t(θ) ¯Yi,t−k(θ) = c ¯Y ,k(θ) for every k and every i = 1, . . . , m1.−Pnt=1¯Yi,t(θ)Ut−k = c ¯Y U,k(θ) for every k and every i = 1, . . . , m1.−Pevery kProof. [1.a, 1.b, 1.c] We prove 1.b, since 1.a is easier.= 0, E[NtNtFork] = 0.−Schwarz inequality (once to E[] and once to·t−k]t2 ≤lim supn≤lim supnby AssumptionPq5(14), and the result follows from the Kolmogorov’s Strong Law of Large Numbers (Theorem 8 inAppendix B). The case k = 0 and 1.c can be proven similarly.E[N 4t ]t2P∞nt=1E[N 4PPPqqq→∞→∞Moreover, by applying twicen), we get lim supnt=1E[N 4E[N 4t−k]t ]t2t2nt=1 ·nt=1→∞<nt=1the Cauchy-E[(NtNt−k)2]t2knthent=1 Nt(nτ =1 gτ ( 1Pk =τ +k = 1nnt=1 NtUtkP−∞τ =0 hτ Ntk−−τ ) =expression Yt−nτ =0 hτ Nt−[1.d] By usingn1t=1 NtYtn−P−ilarly. Using the fact that Nt = 0 for all tPsupτ =1,2,...stability,nP1n |it is possible to choose M such that for each nPMτ =0 hτ ( 1τ )τ , we writeτ ) +−τ ). We focus on the ﬁrst term, the second one can be dealt with sim-0, the Cauchy-Schwarz inequality yieldst . Fix ǫ > 0. Byhτ |2C . Thus,≥+ ǫ2, which can be made < ǫ by taking∞τ =1 gτ Utk−−nt=1 NtNt−≤t . Deﬁne C = limnnt=1 N 2∞τ =M +1 |nτ =1 gτ UtP−nt=1 NtNt−nτ =0 hτ ( 1P1n→∞M,τ =0 hτ ( 1nt=1 N 2τ | ≤< ǫτ +PPPτ )Pnt=1 NtNt−nt=1 NtNt−PP1n−−−−−nnnkkkkk| ≤ |||PPPP226n large enough, because maxτ =1,...,M |that all go to zero in virtue of 1.b.1n(nt=1 NtNt−k−τ )|Pis the max over a set of ﬁnite (M) terms[2.a, 2.b] The proof of 2.a is similar to 2.b, so we focus on 2.a. Consider kt with t′ = tfor τk, and use the same argument. Rewrite 1nnt=1 YtYt−0)−0, otherwise replacek as (it is intended that gτ = 0≥≤nPnnt=1  τ =0XXnn1n=Xℓ=0nnτ =0X+!  Xℓ=0ℓ) +(hτ Nt−τ + gτ Ut−τ )(hℓNt−k−ℓ + gℓUt−ℓ)k−!hτ hℓ(Nt−τ Nt−k−nn1nt=1X1nnnhτ gℓ(n1nτ =0Xℓ=0Xnnt=1Xn1ngτ hℓ(Utτ Nt−k−−ℓ) +gτ gℓ(Utτ Ut−ℓ)k−−Ntτ Ut−ℓ)k−−τ =0Xt=1XAll of these terms can be dealt with similarly, so we focus on the ﬁrst one.Ntℓ), for M < n, can be rewritten asτ =0Xt=1XXℓ=0Xℓ=0τ Ntk−−−nτ =0nℓ=0 hτ hℓ( 1nPPnt=1PMMhτ hℓ(1nτ =0XXℓ=0Mn+t=1X1nhτ hℓ(nnMNtτ Nt−k−ℓ) +hτ hℓ(τ =M +1XXℓ=0nn−n1nnt=1XNt−τ Nt−ℓ)k−n1nNtτ Nt−k−−ℓ) +hτ hℓ(Ntτ Nt−ℓ).k−−1nτ =0Xt=1Xnt=1 Nt−k−, and in virtue of the stability of the system, the limit as nXℓ=M +1t , which converges to a constant as nIn virtue of supτ,ℓ=0,...,n |of all the terms exceptgrows tofor the ﬁrst one can be made arbitrarily close to zero if M is chosen large enough. We are leftMto deal with the truncated sum limnℓ), which is Cauchy inM because of the stability of the system, and therefore can be made arbitrarily close to limnPτ =M +1Xnt=1 N 2ℓ=0 hτ hℓ( 1nt=1 Nt−Xℓ=M +1ℓ| ≤→ ∞t=1XMτ =0τ Ntτ NtPPPP∞→∞1n−−−nk→∞ℓ). More precisely, its argument can be further decomposed asnℓ=0 hτ hℓ( 1nnτ =0nt=1 Nt−τ Nt−k−hτ hℓ(1nnt=1XNt−τ Nt−ℓ).k−=k+τPPPhτ hk+τ (n1nN 2t−τ ) +k−t=1XXτ =0,...,M→ ∞τ =0,...,M ;ℓ=0,...,M ;ℓXτ =0,...,MThe limit for nto a ﬁnite number of choices of τ and ℓ, while limnc0Nt}[3.a, 3.b ,3.c, 3.d] The sequencePYt}{is ({Ut} ∗ {(k hτ hk+τ does not depend on the speciﬁcYt}{gt(θ∗)Ut} ∗ {Nt} ∗ {ht(θ∗)})t′ == ({Nt} ∗ {ht(θ∗)})t′ ={∞τ =0 Nt′∞τ =1 Ut′Pgt(θ∗)) + ({{}−−→∞.of the second term goes to zero because of Lemma 5(1.b) appliedτ ) =k hτ hk+τ ( 1τ =0,...,Mnnt=1 N 2t−−PPcan be written as the sum of two convolutions,i.e.,), where the t′-th sample of the ﬁrst convolutionτ hτ (θ∗), and the t′-th sample of the second convolution isτ gτ (θ∗). Let 1 (P ) denote the indicator function that is equal to}−P2361 when proposition P is true, and is 0 otherwise. For every t and k, deﬁne Nt|and, similarly, Ut|k). Clearly, for every ﬁxed n,k), Yt|k , Nt · 1 (tk , Ut · 1 (tYt|k{≤n}k4 =≤ kkk , Yt · 1 (tht(θ∗)≤({({Nt|Nt|n} ∗ {n} ∗ {}ht(θ∗)) + ({k4 +)Ut|(k{n} ∗ {Ut|}n} ∗ {gt(θ∗))k4}gt(θ∗))k4.}k),≤(25)Using Young’s convolution inequality for sequences (see e.g., [7], page 315)(Nt|{kn} ∗ {ht(θ∗))}Ntk4 ≤ k{|nn}k4 · k{t )1/4N 4(·ht(θ∗)}k1∞t=0Xht(θ∗)),||(≤t=1Xand similarly for the input term. Due to the stability assumption,gt(θ∗)k{}k1 ≤C ′′ <∞. Hence, we getht(θ∗)k{}k1 ≤C ′ <∞andt=1Xand, from (16) and (15), we conclude that lim supnt=1Xn1nY 4t ≤8C ′4 1nnN 4t + 8C ′′nU 4t ,4 1nt=1Xt <nt=1 Y 41n→∞w.p.1.∞Inequalityimmediately follows fromkP42 <ϕtkk∞1nlim supn→∞nt=1Xnanbϕtk2 ≤Yt+k|−|Ut−.k||Xk=1Nt|+Xk=1θ∗Moreover,Here, supθˆNt(θ)|θ∗|Θc k∈=−t (θ∗Nt + ϕTϕtk2 ·ϕtk2 · k|θk2 is ﬁnite because Θc is compact and we can conclude thatk2 ≤ |Nt|| ≤ |θ)−−+kkθsupθsupθΘc  ∈lim supn→∞1nnt=1XˆN 4t (θ)<.∞!(26)(27)(28)Θc k∈θ∗θk2.−(29)The same reasoning that led to (26) and (27) can be applied toK ′ <(gt(θ)Ut} ∗ {Θcby Assumption 4, we immediately get), and, noting that supθ∞t=1 |ht(θ)| ≤}∈{∞{∞PP¯Yt(θ)= (}and supθαi,t ˆNt(θ)∞t=1 |{Θc∈} ∗ {gt(θ)ht(θ)) +}K ′′ <| ≤supθΘc  ∈lim supn→∞1nnkt=1X¯ϕi,t(θ)42k!<,∞(30)where the ﬁnite bound does not depend on the sequence.αi,t}{24τn−−Pτ (θ) +naℓ=1 Yt−nbℓ′=1 UtP−τ , wherePτ =0 hτ (θ)αi,t ˆNtθ) +P−aℓ) +nτ =0 hτ (θ)αi,tϕTt−−nτ =1 gτ (θ)Ut−ℓ′(b∗ℓ′¯Yi,t(θ) =τ (θ∗ℓ(a∗ℓ −4.b] Writing[4.a,αi,tNtτ ) +nτ =0 hτ (θ)αi,t(Pnnτ =0(hτ (θ)τ =1 gτ (θ)Utτ =−nτ =0(hτ (θ)αi,tϕTθ)) =t−bℓ′)), we observe that, modulo the pres-τ−ence of random signs, most of the terms involved in this sum are the same as those encoun-Ptered in the proofs of results 2.a and 2.b, and they can be dealt with similarly. The termaℓ) requires some extra care as it gives rise to cross-terms of theℓYtℓ. These terms can beλ−Nt}, the sequence; in fact, conditionally on∞t=1 is independent so that Kolmogorov’s Strong Law of Large= k + λ,kind (a∗ℓ −Pdealt with by conditioning on a ﬁxed sequencePαi,tλ{Numbers (Theorem 8 in Appendix B) applies. In this way, we can conclude that, when τ1nℓ goes to zero w.p.1, while the case τ = k + λ reduces to 2.a.nτ =0 hτ (θ)αi,taℓ)2λ=0 hτ (θ)hλ(θ) 1naℓ=1 Yt−nτ =0PPℓαi,tnt=1 αi,t−ℓ(a∗ℓ −Nt}τ (θ∗Pℓαi,tℓαi,tℓYtℓYtℓYtℓYtℓYtℓ}PPP−−τn{{−−−−−−−−−−−−−−−−−−−nλλkkkkkkττττττ−−−−−−−nt=1 αi,t−PThe following lemma ensures that there is some continuity (on average) in the behaviour of¯Yi,1(θ), . . . , ¯Yi,n(θ) as θ varies in Θc.Lemma 6. For every ǫ > 0 there exists a δ > 0 such thatlim supn→∞θ1,θ2with probability one.supΘc;θ1∈Bδ(θ2)∈1nn|t=1X¯Yt(θ1)¯Yt(θ2)|−2 < ǫ,Proof. The proof follows along the same line as the proof of Lemma 5, 3.a, 3.b, 3.c., by writing,for each n (and i),αi,t ˆNtn(θ2){|ht(θ1)+{∆θ , θ1 −¯Yi,t¯Yi,t¯Yi,t{{−|||gt(θ2)ht(θ2)Utn} ∗ {−}{}|n + αi,tϕTn(θ∗ht(θ2)θ2)+t−|f (θ2) for a generic function f , we can writen(θ1)} −} ∗ {θ2 + [θ2 −−} ∗. Using the notationαi,t ˆNtn(θ2){}|n + αi,tϕTn(θ∗t|gt(θ2)n(θ1)} − {={Utn} ∗ {|} ∗ {αi,tNt{|θ2, ∆f , f (θ1)¯Yi,t|αi,tNt|gt(θ1)ht(θ1)θ1])n(θ1)+}gt(θ1)n(θ2)} ∗ {−−=={}}}∆ ¯Yi,t|k{n}k2 ≤ k{+αi,tNtn} ∗ {|αi,tϕTn∆θt|(Young’s inequality)∆ht}k2 +ht(θ1)} ∗ {k{≤αi,tϕTt|k{}k2 +k{n(θ∗Ut|−n} ∗θ2)} ∗ {∆gtk2∆ht}k2≤ k{Ntn}k2 · k{|ϕtk2 · k{k|θ∗∆ht}k1 + (−kht(θ1)nk2}k2 · k{θ2k2 · k{kUt}k1 +k{|ϕtnk2}k2)|n}k2 · k{∆θk· k{∆gt}k1,∆ht}k1 +(31)which is a ﬁnite quantity in view of Assumption 4. Denoting supθ1,θ2∆htk1 ≤sup2Bδ (θ2) for short as∈∆gtk1 ≤ht(θ)∆θ<δ kk. From (31), using (16) and (15), Assumptions 4 and Lemma 5 (3.b), it<δ, we have sup∆θkk<∞t=1 supθΘc;θ1∈and sup∞t=0 supθ<δ kkΘc |∈gt(θ)∞<∆θ2|kkΘc |∈|∞PP256follows that w.p.1 there are (possibly realisation-dependent) constants C1, C2, C3, C4 such thatlim supn→∞≤sup∆θ<δ rkkC1 sup<δ k{∆θkk∆ ¯Yi,t|1n}k2n k{∆ht}k1 + C2 sup∆θ<δ k{k∆ht}k1 + δ·C3 + C4 sup<δ k{k∆θk∆gt}k1 <.∞kMoreover,sup∆θk<δ k{k<δ k{kk∆θsup∆ht}k1 ≤∆ht}k1 can be made arbitrarily small∞t=0 sup∆θand the following Proposition holds.∆ht|can be made arbitrarily small for a positive δ′ small enough.k<δ |k∆ht|for δ small enough becauseProposition 1.P∞t=0 sup∆θk<δ′k|PProof. First, write∞t=0Xsup∆θ<δ′ |kk∆ht|=≤M1−t=0X1M−t=0Xsup∆θ<δ′ |kk∆ht|+sup∆θ<δ′ |k∆ht|sup∆θ<δ′ |kk∆ht|+ 2supθΘc |∈ht(θ),|∞kt=MX∞t=MXand note that for any ǫ′ we can choose an M > 0 large enough, such thatsupθΘc |∈ht(θ)<|ǫ′4.∞t=MXNow we prove that there exists δ′ > 0 such thattion 4, the t-th coeﬃcient ht(θ) of the Laurent seriesPten as ht(θ) = 12π< ǫ′2 . By Assump-A(θ;z−1) can be writ-A(θ;e−ιω) eιωtdω (ι denotes the imaginary unit), see e.g., [45]. From whichP−πdω, where K ,1R2πππ || ≤−1. Note that K is ﬁnite by Assumption 4; in fact, by Assumption 4 thereRA(θ;e−ιω)|=ht(θ1)|supθexists a ﬁnite K ′ such that, for all θ and ω it holds that K ′ >∆ht|<δ′∆θ|kt =∞t=0 ht(θ)z−A(θ1;e−ιω)A(θ2;e−ιω)A(θ1 −M1t=0 sup−Θc,ω2π K 2ht(θ2)θ2; e−θ2;e−ιω)−Rιω)dωA(θ1π,π]−≤[∈1ht(θ)−−−−∈|πππk1111|1A(θ;e−ιω)||θ′θnak−. Since3n2ak1 ≤A(θ|θk−−θ′(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(a1 −θ′; e−k2, the result follows by choosing δ′ < n−ιω + (a2 −a′1)e−ιω)−=1||aa′2)e−P32∞t=0 |ι2ω +πǫ′K 2 .M·| ≥ |+ (ana −P· · ·∞t=0 ht(θ)eιωtιnaωa′na)e−|| ≤The same argument holds for sup∆θk<δ kk∆gtk1, and from this the theorem statement follows.Proof of Lemma 3Proof. The limit ¯R∗excitation condition onand B(θ∗; z−positive deﬁnite, see e.g., [58], Lemma 10.3, and [35].(Assumption 6), together with the fact that polynomials A(θ∗; z−P1) are of known orders (Assumption 1) and coprime (Assumption 3), entails that ¯R∗t exists and is ﬁnite by Lemma 5 (2.a, 2.b). The persistent1)is= limnUt}nt=1 ϕtϕT→∞{26From Lemma 5, 4.a and 4.b, it follows that for each θ the limit matrix ¯R(θ) exists and is. When θ = θ∗, the perturbed outputandindependent of i and of the realisations ofgenerated by (10) is statistically equivalent to the original output, so that ¯R(θ∗) = ¯R∗αi,t}Nt}{{.Let θ0 be an arbitrary element of Θc; we use the notation ∆f to denote the diﬀerence f (θ)We ﬁrst show thatǫ > 0∀δ > 0 s.t.∃lim supn→∞supθ,θ0:θBδ(θ0) k∈∆Ri,nk< ǫ,f (θ0).−(32)where the domain Θc is implicitly assumed, and it will be omitted in what follows. To prove (32),we focus on the matrix ∆Ri,n entry by entry and we study the limiting behaviour of entries ofthe kind ∆rn, where rn(θ) , 1τ (θ), for some ℓ, τ between 1 and na, while othernentries in ∆Ri,n that involve Ut can be dealt with similarly. WritePℓ(θ) ¯Yi,t−¯Yi,t−rn(θ0)rn(θ)nt=1=−|n1n|≤vuut+nt=1X1nt=1X1nvuut¯Yi,t−ℓ(θ0)∆ ¯Yi,t−τ +∆ ¯Yi,t−ℓ ¯Yi,t−τ (θ0) +1n1nnt=1Xnt=1X1nvuut∆ ¯Y 2i,t−¯Y 2i,t−ℓ(θ0)nvuut∆ ¯Y 2i,tn1nt=1Xn1n−ℓvuut∆ ¯Y 2i,t−τ +∆ ¯Y 2i,tτ .−|nt=1X1nℓvuut∆ ¯Yi,t−ℓ∆ ¯Yi,t−τ |nt=1X¯Y 2i,t−τ (θ0)t=1Xt=1XBδ(θ0) on both sides, it is immediate from Lemma 5, 4.a, and Lemma 6 that∈can be made arbitrarily small for every n large enough by choosing δrn(θ0)By taking the supθ,θ0:θsupθ,θ0:θ−small enough and (32) is established. SinceBδ(θ0) |∈rn(θ)|supθ,θ0:θBδ(θ0) k∈∆ ¯R=ksupBδ(θ0)θ,θ0:θ∈lim suplim supn→∞ksupθ,θ0:θBδ(θ0) k∈∆Ri,nk∆Ri,nk,≤n∈≺ρ2I for all θ→∞(32) entails uniform continuity of ¯R(θ) over Θc, and therefore there exists a ﬁnite ρ2 > 0 such thatΘc. As for the uniform convergence of Ri,n(θ) to ¯R(θ), it can be found a δ > 0¯R(θ)and a ﬁnite number, say Mδ, of δ-balls Bδ(θ(1)) that cover Θc and are such that, forRi,n(θ(j)all n large enough, it holds true that i) maxj=1,...,Mδ supθ(in view0 )Ri,n(θ(j)(in view of pointwise convergence at the ballof (32) ), ii) maxj=1,...,Mδ k0 )¯R(θ)centres), and iii) maxj=1,...,Mδ supθ(in view of uniform continuity of0 ) kk¯R(θ)¯R(θ)). Then, for any n large enough, supθ kMδj=1 supθBδ(θ(j)∈¯R(θ(j)Ri,n(θ)0 )++Pkk¯R(θ(j)< ǫ0 )3Mδk¯R(θ(j)0 )Ri,n(θ)−Ri,n(θ(j)0 )¯R(θ)−Mδj=1( ǫ3Mδ0 ), . . . , Bδ(θ(Mδ)−Bδ(θ(j)∈k ≤¯R(θ(j)0 )Ri,n(θ(j)0 )0 ) k¯R(θ)Bδ(θ(j)∈< ǫ3Mδ< ǫ3Mδk ≤+Ri,n(θ)Ri,n(θ)0 )(0 ) k≤−−−−−kkkkk)0Mδj=1 supθ+ ǫ3MδǫP3MδBδ(θ(j)∈) = ǫ.P27{1n∈= (Θc, recall thatIρ1 for all θ≻Ut}{(in a set of probability 1) is “uncorrelated” withTo see that ¯R(θ)g(θ)Ut} ∗ {), where(}{αi,t ˆNt(θ)alisation of{t=1 αi,t ˆNt(θ)Utnlimnorder in the sense of [35], for every θLemma 10.2 in [58]) it follows immediately that ¯R(θ) is invertible for every θalready that ¯R(θ∗) = ¯Rthe whole Θc for some ρ1 > 0.¯Yi,t(θ)) +is persistently exciting of order na + nb (Assumption 6). Any re-in the sense thatis persistently exciting of every= θ∗.6 Applying standard results on identiﬁability (e.g.,. We knewIρ1 overθ∗}0 so that, by continuity of ¯R(θ), we can conclude that ¯R(θ)≻τ = 0 for every τ ; moreover,αi,t ˆNt(θ)αi,t ˆNt(θ)Θc \\ {Ut}ht(θ)} ∗ {∗ ≻P→∞∈}}{}{{}−Proof of Lemma 4Proof. The ﬁrst statement follows from Lemma 5 (1.c, 1.d). As for the second statement, we ﬁrstprove pointwise convergence, i.e., we prove that for all θΘc,∈nαi,t ˆNt(θ) ¯ϕi,t(θ) = 0 w.p.1.1nlimn→∞t=1XNt}We work conditioning on a sequence{αi,t}is independent of the sign-sequences{the probabilities and expected values are with respect to the random sign-sequences1, . . . , mset of probability 1, then it holds unconditionally w.p.1. For a ﬁxed θ, i.e., we ﬁx a realisation of the noise, which we recall1. Therefore, in what follows, all, i =in a{1, only. Since the result that we prove holds conditionally on any realisationαi,t}Nt}{Θ (and i), deﬁne, i = 1, . . . , m−−∈1(θ) + 1t αi,t ˆNt(θ) ¯ϕi,t(θ)zi,t(θ) = zi,t−zi,0 = 0We aim at showing that each component of zi,n(θ) is a martingale with bounded variance. Fromthis, convergence of 1nt=1 αi,t ˆNt(θ) ¯ϕi,t(θ) to zero as nwill be easily proved.n→ ∞] <P∞Clearly, E[|zi,t|for all t. Denote byuntil time t, i.e., by αi,1, . . . , αi,t. Since E[zi,t+1|At] = zt, the sequencecomponent of the vector zi,t is a martingale. Moreover, E[z(j)1|At−1t (Nt + ϕT1)2, from which the useful identity1 = (z(j)i,t z(j)t (θ∗i,ti,t−{z(j)i,t }1] = (z(j)i,t−At the σ-algebra generated by the sequence{αi,t}formed by the j-th1)2 + E[αi,t|At1]−·−θ)) ¯ϕi,t(θ)(j)z(j)i,t−E[(z(j)−z(j)i,t−1)2|At−1] = E[(z(j)i,t )2(z(j)i,t−1)21]|At−−(33)i,t −6Proving these claims is easy if we ﬁx a realisation of, and only the signs are left random; then it is just a{matter of checking that the conditions for the Kolmogorov’s Strong Law of Large Numbers (Theorem 8) are met bythe conditionally independent sequencesαt ˆNt(θ)UtandNt}.αt ˆNt(θ)αt−j ˆNt−j(θ)}{{}286(34)(35)(36)(37)(38)(39)(40)follows. Thus,tE[(z(j)i,t )2] =E[(z(j)i,k )2]tXk=1 (cid:16)E[[E[(z(j)i,k )2E[(z(j)i,k−−1)2](cid:17)(z(j)i,k−1)2−1]]|Ak−===≤Xk=1tXk=1tXk=1tXk=1E[(z(j)i,k −z(j)i,k−1)2] (by (33))1k2ˆN 2k (θ)E[ ¯ϕ(j)i,k (θ)2]ˆN 2k (θ)E1kk(cid:2)1ktt¯ϕi,k(θ)2k(cid:3)≤ vuutXk=1t≤ vuutXk=11k21k2ˆN 4k (θ)ˆN 4k (θ)vuutXk=1tEvuut\"Xk=11k2E[k¯ϕi,k(θ)k2]2 (Cauchy-Schwarz)1k2 k¯ϕi,k(θ)4k#(Jensen’s inequality),{{∞αi,t}z(j)i,t |z(j)i,t }, this is bounded in virtue of Lemmaand, keeping in mind that the expected value is only w.r.t.5 (3.b, 3.d). Thus, we have proved thatis a martingale with bounded variance uniformlyE[] <w.r.t. t, therefore supt, and we can apply Doob’s Theorem (Theorem 10 in Appendix|zi,t is, w.p.1, a limit vector with ﬁnite-valued components. Finally, byB), to conclude that limtt=1 αi,t ˆNt(θ) ¯ϕi,t(θ) =zi,t =Kronecker’s Lemma, limtimplies limn0. As for uniform convergence, using Lemma 6 and Lemma 5, one can easily show that there existsa positive δ such that, for n large enough, the valuesareǫ-close to each other, no matter what θ′ is. Since Θc is compact, a ﬁnite number δ-balls cover the1whole set Θc and thereforecan be made arbitrarily small uniformly on then|whole Θc for n large enough.Pt=0 αi,t ˆNt(θ) ¯ϕi,t(θ)t=0 αi,t ˆNt(θ) ¯ϕi,t(θ)αi,t ˆNt(θ) ¯ϕi,t(θ)tBδ(θ′): θ∞t=1PP∞→∞→∞→∞{|<∈1n1n}nnn||PA.3 Proof of Theorem 3We need some preliminary deﬁnitions and re-writings. Deﬁneγi,n(θ) , 1nΓi,n(θ) , 1nnt=1Xnt=1X29αi,t ¯ϕi,t(θ)Nt,αi,t ¯ϕi,t(θ)ϕTt .(41)(42)Let Pi(θ) = nSi(θ)k· k2, i = 0, . . . , m−1. P0(θ) can be written asP0(θ) = √n(θˆθn)TRn(θˆθn)√n,−−while Pi(θ), for i = 1, . . . , m−1, can be written as Pi(θ) = nSi(θ)2= (√nγi,n(θ) + √nΓi,n(θ)(θ∗θ))T(Ri,n(θ))−−· kk1(√nγi,n(θ) + √nΓi,n(θ)(θ∗θ)).−Let ¯P (θ) = [P1(θ)Pm−· · ·1(θ)]T. ¯P (θ) is rewritten as¯P (θ) = p1(θ) + p2(θ) + p3(θ),wherep1(θ) = [p1,1(θ)p2(θ) = [p2,1(θ)p3(θ) = [p3,1(θ)and, for i = 1, . . . , m1,−p1,m−p2,m−p3,m1(θ)]T,1(θ)]T,1(θ)]T,−· · ·· · ·· · ·θ)θ)Tq1,i(θ)(θ∗p1,i(θ) = (θ∗−−1q1,i(θ) = √nΓTi,n(θ)Γi,n(θ)√ni,n(θ)R−1p2,i(θ) = √nγTi,n(θ)γi,n(θ)√ni,n(θ)R−θ)Tq3,i(θ)1i,n(θ)γi,n(θ)√n.p3,i(θ) = (θ∗q3,i(θ) = 2√nΓTi,n(θ)R−−With the notation γi,n(θ) , γi,n(θ)−γi,n(θ∗), we can further decompose p2,i(θ) as followsp2,i(θ) = ¯p2,i(θ) + p′2,i(θ) + p′′2,i(θ),where¯p2,i(θ) = √nγTp′2,i(θ) = 2√n∆γTp′′2,i(θ) = √n∆γTi,n(θ∗)R−1i,n(θ)γi,n(θ∗)√n1i,n(θ)γi,n(θ∗)√ni,n(θ)R−1i,n(θ)∆γi,n(θ)√n.i,n(θ)R−We denote byk · kthe Frobenius norm of a matrix, and deﬁneq1(θ) = [kq1,1(θ)q3(θ) = [kq3,1(θ), . . . ,, . . . ,kkkkq1,m−1(θ)q3,m−1(θ)]T,]T,kk30¯p2(θ) = [¯p2,1(θ), . . . , ¯p2,m−p′2(θ) = [p′2,1(θ), . . . , p′2,m−p′′2(θ) = [p′′2,1(θ), . . . , p′′2,m−1(θ)]T,1(θ)]T,1(θ)]T.The SPS conﬁdence set is contained in the set of θ’s for whichP0(θ)qm≤¯P (θ),qm¯P (θ) means that P0(θ) is less than or equal to qm or more of the elements inwhere P0(θ)the vector on the right-hand side (see point 6 in Table 2). In what follows, the sup operator isunderstood to be applied component-wise when it is applied to a vector. We have≤Θn,m ⊆ {b⊆ {θ : P0(θ)θ : P0(θ)θ : P0(θ)⊆ {qm≤qm≤qm≤where¯P (θ)}θsupbΘn,m∈¯Un,m},¯P (θ)}Un,m , sup¯θ∗2q1(θ)θk−ˆθn,m kθ∈+ supbΘn,mθ∈+ supθΘn,m kb∈” (“ d→p→¯p2(θ) + supbΘn,mθ∈q3(θ).θ∗θp′2(θ) + supbΘn,m∈θp′′2(θ)(43)−kIn all that follows, symbol “pendix A.4, under the assumptions of Theorem 3, we will prove the following Lemma”) denotes convergence in probability (distribution). In Ap-Lemma 7. As n,→ ∞supΘn,m kb∈θθ∗θk−2q1(θ)θsupbΘn,m∈supbΘn,mp′2(θ)p′′2(θ)q3(θ)θk−θ∈θ∗whilesupΘn,m kb∈θp→p→p→p→0000¯p2(θ) d→σ2χ2m−1,supbΘn,m∈θ31(44)(45)(46)(47)(48)where χ2m−of freedom.1 is a vector of m−1 independent χ2 distributed random variables with dim(θ∗) degreesFrom (43) and Lemma 7, in view of Slutsky’s Theorem (Theorem 6 in Appendix B), we can(49)conclude that¯Un,md→σ2χ2m−Denote byµn,m the value of the qm largest element of 1σ2bor, equivalently,1 as n.→ ∞¯Un,m. Hence,µn,mσ2,Θn,m ⊆bΘn,m ⊆b≤θ : P0(θ)≤(cid:8)bˆθn)TRn(θµ)σ2θ : (θnµσ2n(+−µn,m −n(cid:9)ˆθn)−,(cid:27)µn,m−b(where, we recall, µ is such that Fχ2(µ) = p). Let εn,m , (µ)σ2. In order to prove the theorem,µn,m = µ a.s.. The function selecting the qmth largest elementwe must show that limmin a vector is a continuous function, and hence, by the continuous mapping theorem (Theorem 5µn,m has the same distribution as the qmth largest element of thein Appendix B),vector χ2mµm converges a.s. to µ as m, which concludes the proof.limn→∞→∞bbµm , limn→∞1. We next show thatb1 values x1, . . . , xmb−Given m1 independent χ2 distributed ran-1 which are realisations of mb−dom variables, consider the following empirical estimate for the cumulative χ2 distribution function−−→ ∞1−11m−i=1Xm1 (xi ≤z),Fm(z) =b) is the indicator function. From the Glivenko-Cantelli Theorem (Theorem 7 in Appendixwhere 1 (B), we have·Fm(µm) = 1Sinceconclude that limm−→∞bb|qmb1 = pm →−m−µm = µ w.p.1.1supzFm(z)Fχ2(z)−| →0a.s. as m.→ ∞(50)p and Fχ2(µ) = p, with Fχ2 continuous and invertible, we canA.4 Proof of Lemma 7bFirst, we state a useful result. (In all the lemmas in this section, the assumptions of Theorem 3 areleft implicit.)32Lemma 8.1nlim supn→∞(the expected value is w.r.t. to).Nt}{nt=1XE[kϕtk4] <∞(51)Proof. Take the expected value of both sides of (26) and use Assumptions 5(14) and 6(15) toconclude that lim supn. The Lemma is proven by using (28).E[Y 4t ] <nt=11n→∞∞Statements (44), (45), (46), (47) and (48) will be proven by building on the next two LemmasP(throughout, we keep using the notation ∆f (θ) = f (θ)f (θ∗)).−Lemma 9. supθbΘn,m∈√nk∆Γi,n(θ)and supθbΘn,m∈√nkk∆γi,n(θ)kgo to zero in probability as n→ ∞Proof. Let us considersupbΘn,m∈θ√nk∆Γi,n(θ)k= supθbΘn,m (cid:13)(cid:13)∈(cid:13)(cid:13)(cid:13)1√nnt=1Xαi,tϕt∆ ¯ϕi,t(θ)T(cid:13)(cid:13)(cid:13)(cid:13)(cid:13),(52)kk(here,Θn,m √n∆γi,n(θ)as supθb∈nt=1 αi,tϕt∆ ¯ϕi,t(θ)T1√n|uniformly over θcorresponding to past Ut values) or ∆ ¯Yi,t−bcan be dealt with in the same line. We show that each element ofis the entry-wise absolute value) goes to zero in probabilityΘn,m. The components of ∆ ¯ϕi,t(θ) are either zero (the nb exogenous componentsna. So, a non-zero entry of the matrix1and, overall,√n≤, say entry (ℓ, k), is of the kindnt=1 αi,tϕt∆ ¯ϕi,t(θ)Tt=1 αi,tϕ(ℓ)1√n||(52) can be bounded by a ﬁnite sum of terms liket ∆ ¯Yi,tk(θ), 1k(θ)| · |P≤∈k−n|PP(cid:12)(cid:12)(cid:12)which, as we shall see in what follows, go to zero in probability.θt=1XsupbΘn,m (cid:12)(cid:12)∈(cid:12)(cid:12)(cid:12)For any ﬁxed pair (ℓ, k), write ¯Yi,t−k(θ) =nτ =0 hτ (θ)αi,tkP−−τ Ntit holds that ¯Yi,tFor n≥−−nτ =0 hτ (θ)αi,tτ Ntτ +k−−−nk(θ∗) =τ =0 hτ (θ∗)αi,t−−nτ =0(hτ (θ)αi,tτ Ntτ +P−−nτ =0 ∆hτ (θ)αi,t−we get ¯Yi,tP−k(θ) =Pk,−−−tkkkkkk(θ)−,(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(53)τ (θ) +nτ =0 hτ (θ)αi,tP−τ ϕTtk−−τ +∞τ =0 hτ (θ)αi,tk−−τ ˆNtk−−θ) +−nτ =1 gτ (θ∗)UtP−nτ =1 ∆gτ (θ)Utθ)) +−τ ˆNtk−−nτ =1 gτ (θ)Utτ (θ) +P−−nτ =1 gτ (θ)Utτ .kP−−thus ∆ ¯Yi,tτ ,−τ .∞τ =1 gτ (θ)Utτ .k−−τ =−Similarly,k(θ) =k−−τ ϕTt−τ (θ∗Pτ (θ∗−−−−kkkkk−(cid:12)(cid:12)(cid:12)t ∆ ¯Yi,tαi,tϕ(ℓ)n1√nPPP33Finally,1√n=nt=1X1√nαi,tϕ(ℓ)t ∆ ¯Yi,tnαi,tϕ(ℓ)t−k(θ)nt=1Xn1√n++1√nt=1Xnt=1Xτ =0Xnαi,tϕ(ℓ)tαi,tϕ(ℓ)tτ =0Xnτ =1X∆hτ (θ)αi,tτ Nt−k−kτ−−hτ (θ)αi,t−τ ϕTt−kk−−τ (θ∗θ)−∆gτ (θ)Ut−τ .k−(54)Thus, (53) can be bounded by taking the sum of the sup of the absolute value of the three termsin (54). As the ﬁrst and the third term can be dealt with similarly, we focus only on the ﬁrst one;the second one will be brieﬂy considered later.DeﬁneGi,τn =|1√nnt=1Xαi,tϕ(ℓ)t αi,t−k−τ Nt−τ ,k−which is a random variable with 0 mean and varianceE[G2i,τn] =|1nnE[(ϕ(ℓ)t )2N 2t−k−τ ]nt=1 qX1n≤nE[kϕtk4]E[N 4t−k−τ ]nt=1X1nt=1X≤ vuutE[kϕtk4]1nvuutE[N 4t−k−τ ].t=1XBy Assumption 5(14) and Lemma 8, there is a number V > 0 such that E[G2i,τn and τ . We have thatn]|≤V <∞for all1√nθsupbΘn,m (cid:12)(cid:12)∈(cid:12)(cid:12)(cid:12)≤nnαi,tϕ(ℓ)t∆hτ (θ)αi,t−t=1XnsupbΘn,m |∈θτ =0Xτ =0X∆hτ (θ)Gi,τ.n||||τ Nt−k−kτ−n= supθbΘn,m (cid:12)τ =0(cid:12)∈X(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)∆hτ (θ)Gi,τn|(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)For all δ≥0 deﬁne H(δ) as follows:H(δ) , ∞τ =0XθsupBδ (θ∗)∈Θc |∩∆hτ (θ).|(55)(56)For all δ0(Proposition 1 in the proof of Lemma 6). The right-hand side of (55) is either zero or can be0, H(δ) is a ﬁnite number by Assumption 4. Moreover, H(δ) goes to zero as δ→≥34rewritten asnHθ∗supΘn,m kb∈θθ−k! \"τ =0XsupθbΘn,m |∈H(supθbΘn,m k∈∆hτ (θ)|θθ∗ −kGi,τ) |.|n|#(57)We denote by Bn the term in squared brackets. Bn is in the formsupθ∈ bH(supθ∈ bE[B2n]1 by deﬁnition of H(), (56)) and Xτ =) (cτ ≤∆hτ (θ)|θ∗Gi,τΘn,m kΘn,m |−·|kθ|V in view of the following proposition (which follows easily from the Jensen’s inequality).nτ =1 cτ Xτ , with cτ =. Then, we have thatPn|≤Proposition 2. Let X1, . . . , Xn be random variables with E[X 2j ]∞1. Then, E[(c1, . . . , cn be non-negative numbers 01 withC <≤n, j = 1, . . . , n, and letnj=1 cjXj)2]C.≤P)θk−θ∗bΘn,m k∈Moreover, H(supθ0, by Theorem 2. Now we prove that (57) goes to zero inprobability, that is, for every ǫ, δ > 0, the probability that (57) exceeds ǫ can be made smaller thanδ for any n large enough. In fact, for every n large enough, H(supθ) can be madesmaller than any positive constant, say λ = ǫAppendix B). Then, the probability that (57) exceeds ǫ is bounded by Pr2 + λ2E[B2λǫconverges to zero in probability.Consider now the second term in (54), i.e.,kδ2 (Theorem 11 in) >θ∗bΘn,m k∈= δ (where we used Chebyshev’s inequality). Therefore, (57)δ2V , on an event of probability 1−H(supθλBn > ǫbΘn,m k+ Pr} ≤qθ∗−−k}{{n]θθ∈δcj ≤≤w.p.1→j=1 cj ≤P1√nnnαi,tϕ(ℓ)tt=1Xτ =0Xhτ (θ)αi,t−τ ϕTt−kk−−τ (θ∗−θ).(58)Rewrite the scalar product ϕTt−absolute value of (58) asτ (θ∗k−−θ) asna+nbj=1 ϕ(j)kt−(j)τ (θ∗−−θ(j)), and the sup of thePDeﬁningθsupbΘn,m (cid:12)(cid:12)∈(cid:12)(cid:12)(cid:12)nna+nbhτ (θ)(j)(θ∗τ =0Xj=1Xθ(j))1√nnt=1X−nαi,tϕ(ℓ)t αi,t−k−τ ϕ(j)kt−τ−(59).(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)G′i,j,τn =|1√nαi,tϕ(ℓ)t αi,t−k−τ ϕ(j)kt−−τ ,t=1Xwhich, similarly as before, is a random variable with variance bounded by a certain 0 < V ′ <for all j = 1, . . . , na + nb, (59) can be upper-bounded by∞θ∗supΘn,m kb∈θθ2k·−nτ =0XsupΘn,m |b∈θhτ (θ)|na+nbj=1XG′i,j,τ|n||≤θ∗( supΘn,m kbθ∈θ2k·−∞ν=0XsupΘn,m |b∈θhν(θ))|n· \"τ =0X35Phτ (θ)supθ∞ν=0 supθbΘn,m |∈bΘn,m |∈|hν(θ)na+nb|j=1XG′i,j,τ|n|#|(60) where the term in the square brackets has second moment bounded by (na + nb)2and, as before, is multiplied by a term that goes to zero w.p.1 as nto zero in probability.→ ∞V ′, for every n, so that overall (60) goes·In what follows, we denote by [vj]kIk is the identity matrix of size k andthe Kronecker product.⊗j=1 the vector obtained by stacking k vectors v1, . . . , vk together;Lemma 10.[√nγi,n(θ∗)]m−i=11d→G(0, σ2Im−1 ⊗¯R∗).Proof. By the Cramer-Wold Theorem (Theorem 4 in Appendix B), the Lemma statement followsd1)(na + nb)-dimensional vector a, it holds true that aTif, for every (m−→¯RG(0, σ2aT(Im)a). For the sake of simplicity, we set m = 3, the extension to any m being1 ⊗∗−straightforward.√n[γi,n(θ∗)]m−i=1·1Deﬁne, for kn,≤ξn,k = aT1√n α1,k ¯ϕ1,k(θ∗)Nk1√n α2,k ¯ϕ2,k(θ∗)Nk #· \"and ξn,k = 0, for k > n. With this notation,σ-algebra generated byand E[ξn,k|FkConsider the conditional variance of ξn,k deﬁned asαi,t}Nt}andP{{−until time t = k∞k=1 ξn,k = aT∞−1] = 0 for every n and k, that is, ξn,1, ξn,2, . . . is a martingale diﬀerence for every n.[√nγi,n(θ∗)]2i=1. DeﬁningFk·1, it is easy to check that E[ξn,k||−1 as the] <n,k = E[ξ2σ2n,k|Fk1],−Rna+nb. We getE[(aT1 ¯ϕ1,k(θ∗)α1,kNk + aT2 ¯ϕ2,k(θ∗)α2,kNk)21]|Fk−and write aT = [aT1 aT2 ], a1, a2 ∈σ2n,k =n1nXk=1∞Xk=1n= aT 1nXk=1E[N 2k |Fk−E[N 2k |Fk−1] ¯ϕ1,k(θ∗) ¯ϕ1,k(θ∗)Ta1 + aT2E[N 2k |Fk−1] ¯ϕ2,k(θ∗) ¯ϕ2,k(θ∗)Ta2.1] = σ2 (by Assumption 7), and, by taking the limit w.r.t. n, we get (by Lemma 3)n1nXk=1limn→∞∞Xk=1nk = σ2(aTσ21¯R∗a1 + aT2¯R∗a2) w.p.1Note also that ξn,1, ξn,2, . . . have second moments,1n σ2(aT1(recall that E[kE[ ¯ϕ1,k(θ∗) ¯ϕ1,k(θ∗)T]a1 + aT2ϕkkE[ ¯ϕ2,k(θ∗) ¯ϕ2,k(θ∗)T]a2)≤]2). We now prove that2] = E[k¯ϕi,k(θ∗)kin fact we have E[ξ2a2ka1kn,k] = E[σ22)E[ϕk(θ∗)kn σ2(2 +kkk1n,k] =2] <∞E[ξ4n,k] = 0.limn→∞∞Xk=136(61)∞Xk=1E[ξ4n,k]n==Xk=11n2E[E[ξ4n,k|Fk1]]−nEE[N 4k |Fk1]−·(aT1 ¯ϕ1,k(θ∗))41 ¯ϕ1,k(θ∗))3(aT2 ¯ϕ2,k(θ∗)2 ¯ϕ2,k(θ∗))21 ¯ϕ1,k(θ∗))(aT2 ¯ϕ2,k(θ∗))3(cid:2)Xk=1+ 4 E[N 4+ 6 E[N 4+ 4 E[N 4+ E[N 4= E[N 41 ]··−1](aTk α1,kα2,k|Fk−1 ¯ϕ1,k(θ∗))2(aT(aT1]k |Fkk α1,kα2,k|Fk(aT1]k |Fk·n1n21](aT2 ¯ϕ2,k(θ∗))4(aTE−−(cid:3)1 ¯ϕ1,k(θ∗))4 + 6(aT1 ¯ϕ1,k(θ∗))2(aT2 ¯ϕ2,k(θ∗))2 + (aT2 ¯ϕ2,k(θ∗))4)1n≤E[N 41 ]Xk=1·  k(cid:2)a1k4 1n+ 62a1kka2kk2 1nnXk=1n(cid:3)E[k¯ϕ1,k(θ∗)4]kXk=1E[k¯ϕ1,k(θ∗)¯ϕ2,k(θ∗)2kk2] +k4 1na2kknXk=1E[k¯ϕ2,k(θ∗)4]k,!(where we used the i.i.d. Assumption 7 onLemma 8 (it is useful to note that 1nnk=1nk=1E[k¯ϕ2,k(θ∗)4] = 1nknk=1E[kϕkkP1nqPNt}{¯ϕ1,k(θ∗), and E[N 4¯ϕ2,k(θ∗)t ] <2]2kkk≤∞E[k), which is bounded by1nnk=1E[k¯ϕ1,k(θ∗)4]k·4]). Thus, there is a constant C such thatPqPE[ξ4n,k]Cn,≤∞Xk=1for all n. Letting n tend to inﬁnity yields the sought result.For every ǫlimnthat→∞∞k=1nk=1 ξn,kP≥E[ξ2d→ξ2n,k · 1 (|0, E[ξ4E[ξ2> ǫ)]. Hence, (61) impliesn,k · 1 (n,k]|≥> ǫ)] = 0 and Theorem 9 in Appendix B can be applied to concludeξn,k|n,k1 (|¯RG(0, σ2(aT1∗a1 + aT2ǫ2E[ξ2ξn,k|ξn,k|> ǫ)]a2)).¯R∗n,k ·≥PWe are now ready to prove Lemma 7, starting with (45),(46).Proof of (45),(46)11write R−i,n(θ) = R−i,n(θ∗) + ∆R−In view of Lemma 3, w.p.1 Ri,n(θ) is invertible for n large enough, so we can1i,n(θ). Writing R−1i,n(θ)1= R−1i,n(θ∗)1¯R−∗−+ ∆R−1i,n(θ∗), we getsupΘn,m kb∈θR−1i,n(θ)−¯R−1∗ k ≤ kR−1i,n(θ∗)37¯R−∗¯R−−−1∗ k+ supθΘn,m kb∈∆R−1i,n(θ),kwhich goes to zero w.p.1 by Lemma 3, the Strong Consistency Theorem (Theorem 2) and thecontinuity of the inverse operator. Hence,supbΘn,m k∈θR−1i,n(θ)w.p.1→ k¯R−1∗ k.k(62)∈∈kR−bΘn,m k√n∆γi,n(θ)and Bn =Bn, where An = 2 supθbΘn,m p′2,i(θ)≤∈1i,n(θ)bΘn,m kkAn ·supθk ·. An goes to zero in probability, by Lemma 9. Bn is the product of a termsupθthat converges in distribution to the norm of a bounded-variance, normally distributed vector¯R−, (62). So, by Slutsky’s Theorem (Theoremby Lemma 10 and a term that converges to6 in Appendix B), the distribution of Bn converges weakly to the distribution of the norm ofa bounded variance, normally distributed vector. By another application of Slutsky’s Theorem,An ·(supθbΘn,m k∈probability (Lemma 9), and (46) follows from (62) and Slutsky’s Theorem.Bn goes to zero in distribution and therefore in probability. Similarly, supθ1i,n(θ)≤, where the ﬁrst bounding factor goes to zero inΘn,m p′′2,i(θ)b∈√n∆γi,n(θ)√nγi,n(θ∗)bΘn,m k∈1∗ ksupθR−)2kkkk·Proof of (44), (47) We show (44), i.e., that for all i = 1, . . . , m1,−Prθ∗{θsupΘn,m kb∈θ2kk−q1,i(θ)k> ǫ} →0, n.→ ∞(47) can be proven similarly.Deﬁne βn = supθbΘn,m k∈θ∗θk−2. For i = 1, . . . , m1, it holds that−θ∗θ2kk−q1,i(θ)kθsupbΘn,m k∈(β1/3n≤supΘn,m kb∈θ√nΓi,n(θ)(β1/3n)k·supΘn,m kb∈θR−1i,n(θ)(β1/3n)k·supΘn,m kb∈θ√nΓi,n(θ)).kWrite supθΘn,m k∈√n∆Γi,n(θ)Θn,m k∈supθand so does the product βequality, for every K > 0,k√nΓi,n(θ), where supθn supθ13k= supθΘn,m k∈√n∆Γi,n(θ)Θn,m k∈Θn,m k∈√n∆Γi,n(θ)kk√nΓi,n(θ∗) + √n∆Γi,n(θ)+goes to zero in probability (Lemma 9),√nΓi,n(θ∗)k ≤ kk. Consider√nΓi,n(θ∗)kk. By Chebyshev’s in-Pr{k√nΓi,n(θ∗)> Kk} ≤1K 2E[k√nΓi,n(θ∗)2],k38andE[k√nΓi,n(θ∗)2] =k==1n1n1nnt=1XnE[kE[αi,t ¯ϕi,t(θ∗)ϕTt k2]ntr(αi,tαi,k ¯ϕi,t(θ∗)ϕTt ϕk ¯ϕi,k(θ∗)T)]t=1XnXk=1tr(E[ ¯ϕi,t(θ∗)ϕTt ϕt ¯ϕi,t(θ∗)T]) =nt=1X1nE[k¯ϕi,t(θ∗)4]n1nE[kϕtk4]C <1nnt=1XE[k¯ϕi,t(θ∗)22]ϕtkkkk≤t=1X≤ vuutvuutfor all n, by Lemma 8. Fix ǫ > 0. Then, since βn0, for every K > 0 we can ﬁnd some1Cn0(K) such that β3nK 2(Theorem 11 in Appendix B). This entails that, for every K > 0, for every n large enough it1holds that Pr2 CK 2 , and therefore β0. Due to (62) and3n} ≤pβn0. The conclusion follows from the fact that the product→kof random variables that go to zero in probability goes to zero in probability.ǫK for all n > n0(K) with probability arbitrarily large, e.g., at least 1√nΓi,n(θ∗)kR−supθbΘn,m k1β3n{k0, also β1/3> ǫ1i,n(θ)t=1Xw.p.1→√nΓi,n(θ∗)w.p.1→p→∞−≤kk∈n¯R1 ⊗∗1) matrix with diagonal blocks Ri,n(θ), i = 1, . . . , mProof of (48) By Lemma 10, the vector vn = [√nγi,n(θ∗)]m−i=1Gaussian vector v with zero mean and covariance σ2Im. Denoting by Rn(θ) the d(m−d(mdiagonal of the matrix V Tby d(md(m×1, we can observe that ¯p(θ) is the−1n (θ)Vn where Vn is a matrix whose ﬁrst column is √nγ1,n(θ∗) followed2) zeros, the second column is a vector of d zeros followed by √nγ2,n(θ∗) followed byconverges in distribution to a3) zeros, etc. Thus,n R−1)−−−1−supˆθm,n∈θ¯p2(θ) = diagV Tn R−1n (θ∗)Vn(cid:0)+ supˆθm,nθ∈(cid:1)diagV Tn ∆R−1n (θ)Vn.(cid:0)(cid:1)The ﬁrst term is a product, i.e., it can be written as f (vn, R−tion in the elements of vn and of the matrix R−n (θ∗)) dAppendix B) applies and we conclude that f (vn, R−→as σ2χ21V Tn (θ)Vnn ∆R−mfollows again by Theorem 6. Indeed,1n (θ∗)), where f is a continuous func-1n (θ∗). Therefore, Slutsky’s Theorem (Theorem 6 in), which is distributedgoes to zero in probability, (48)1. If the remaining term supθˆθm,n diag∈f (v, Im¯R−∗1 ⊗−−11(cid:0)(cid:1)diagV Tn ∆R−1n (θ)VnVnk≤ ksupˆθm,n∈θ2 supθˆθm,n k∈∆R−1n (θ)p→k0,2 dsince(Theorem 2) and the continuity of the inverse operator.2 and supθ1n (θ)Vnk∆R−→ kkkkv∈(cid:0)ˆθm,n k0 by Lemma 3, the Strong Consistency Theorem(cid:1)w.p.1→39B Main Theoretical Tools for the ProofsIn this Appendix we have collected some results from probability theory which have been usedpin the proofs. Let) denote convergence in→distribution (probability). The following results can be found in e.g., [57].and X be random vectors in Rd, and let d→Xn}{(Theorem 4 (Cramer-Wold). XnX if and only if aTXnd→aTXd→Rd.a∀∈Theorem 5 (Continous Mapping). Let f be a continuous function from Rd to Rl. If Xnthen f (Xn) d→f (X).d→X,Theorem 6 (Slutsky). Let f be a continuous function from Rd+k to Rl andrandom vectors in Rd. If XnX and Ynf (X, c).a sequence ofc, where c is a constant vector, then f (Xn, Yn) d→Yn}p→d→{Theorem 7 (Glivenko - Cantelli). Let ξ1, ξ2, . . . be an i.i.d. sequence of random variables withcumulative distribution function F (z) = P r. Let Fn(z) be the empirical estimate of F (z)ξ1 ≤based on a sample of size n :{}zFn(z) =1nwhere 1 (·) is the indicator function. Then,n1 (ξt ≤z),t=1Xlimn→∞supR |z∈F (z)Fn(z)|−= 0 w.p.1.Theorem 8 (Kolmogorov’s Strong Law of Large Numbers). Let ξ1, ξ2, . . . be a sequence of inde-pendent random variables with ﬁnite second moments, and let Sn =nt=1 ξt. Assume thatthenE[ξt])2]E[(ξt −t2<,∞P∞t=1XE[Sn]Sn −nlimn→∞= 0 w.p.1.The following result is a rewriting of Theorem 35.12 in [5]. See also [50], Theorems 1-4 (ChapterVII, Section 8), for weaker assumptions.40Theorem 9 (Central Limit Theorem for Martingales). For every n, let ξn,1, ξn,2, . . . be a mar-n,k :=tingale diﬀerence with ﬁnite second moments relative to the ﬁltrationE[ξ2andFn,1,Fn,0 is the trivial σ-algebra). Assuming that, for each n,w.p.1, ifFn,2, . . .. Let σ2∞k=1 ξn,k <∞1] (n,k|Fn,k∞k=1 σ2−n,k <∞Pwhere 0 < ¯σ2 <, and∞σ2n,kp−→¯σ2 as n,→ ∞∞Xk=1limn→∞∞Xk=1E[ξ2n,k1 (ξn,k||> ǫ)] = 0P(63)(64)for every ǫ > 0, then, as n,→ ∞with zero mean and variance ¯σ2.P∞k=1 ξn,k converges in distribution to a Gaussian random variableThe following theorem ([50], Theorem 1, VII, §4) is fundamental in the study of convergenceof (sub)martingale, and can be thought of as a stochastic analogue of the monotone convergencetheorem for real sequences.Theorem 10 (Doob). Let (ξn,supn] <E[|ξn|∞. Then with probability 1, the limit limnξn = ξ→∞∞Fn) be a submartingale (i.e., E[ξn+1|Fn]≥exists and E[|ξn w.p.1), withξ] <.∞|∞The following theorem provides a characterisation of almost sure convergence.Theorem 11 (Th. 1, Section 10, Chap. 2, [50]). Let ξ1, ξ2, . . . be a sequence of random variables.A necessary and suﬃcient condition that ξn= 0 forevery ǫ > 0.ξ is that limnw.p.1→ξn −supk> ǫPrn |→∞ξ≥|(cid:8)(cid:9)References[1] Y. Abbasi-Yadkori and Cs. Szepesv´ari, Regret bounds for the adaptive control of linearquadratic systems, in Proceedings of the 24th Annual Conference on Learning Theory, JMLRWorkshop and Conference Proceedings, 2011, pp. 1–26.[2] K. Amelin and O. Granichin, Randomized control strategies under arbitrary external noise,IEEE Transactions on Automatic Control, 61 (2016), pp. 1328–1333.[3] G. Baggio, A. Car`e, and G. Pillonetto, Finite-sample guarantees for state-space systemidentiﬁcation under full state measurements, in 2022 IEEE 61st Conference on Decision andControl (CDC), IEEE, 2022, pp. 2789–2794.41[4] G. Baggio, A. Car`e, A. Scampicchio, and G. Pillonetto, Bayesian frequentistbounds for machine learning and system identiﬁcation, Automatica, 146 (2022), p. 110599,https://doi.org/https://doi.org/10.1016/j.automatica.2022.110599.[5] P. Billingsley, Probability and measure, 3rd edition, Wiley-Interscience, 1995.[6] R. Boczar, N. Matni, and B. Recht, Finite-data performance guarantees for the output-feedback control of an unknown system, in 2018 IEEE Conference on Decision and Control(CDC), IEEE, 2018, pp. 2994–2999.[7] P. Bullen, Dictionary of inequalities, CRC Press, 2nd ed., 2015.[8] A. Calisti, D. Dardari, G. Pasolini, M. Kieffer, and F. Bassi,Infor-mation diﬀusion algorithms over WSNs for non-asymptotic conﬁdence region evalua-tion,in 2017 IEEE International Conference on Communications (ICC), 2017, pp. 1–7,https://doi.org/10.1109/ICC.2017.7997230.[9] M. C. Campi and E. Weyer, Guaranteed non-asymptotic conﬁdence regions in systemidentiﬁcation, Automatica, 41 (2005), pp. 1751–1764.[10] A. Car`e, M. C. Campi, B. Cs. Cs´aji, and E. Weyer, Facing undermodelling in Sign-Perturbed-Sums system identiﬁcation, Systems & Control Letters, 153 (2021), p. 104936.[11] A. Car`e, B. Cs. Cs´aji, M. C. Campi, and E. Weyer, Finite-sample system identiﬁcation:An overview and a new correlation method, IEEE Control Systems Letters, 2 (2018), pp. 61–66.[12] D. F. Coutinho, C. E. de Souza, K. A. Barbosa, and A. Trofino, Robust linear H∞ﬁlter design for a class of uncertain nonlinear systems: An LMI approach, SIAM Journal onControl and Optimization, 48 (2009), pp. 1452–1472, https://doi.org/10.1137/060669504,https://doi.org/10.1137/060669504.[13] B. Cs. Cs´aji, M. C. Campi, and E. Weyer, Non-asymptotic conﬁdence regions for theleast-squares estimate, in Proceedings of the 16th IFAC Symposium on System Identiﬁcation,2012, pp. 227–232.[14] B. Cs. Cs´aji, M. C. Campi, and E. Weyer, Sign-Perturbed Sums: A new sys-tem identiﬁcation approach for constructing exact non-asymptotic conﬁdence regions in lin-ear regression models, IEEE Transactions on Signal Processing, 63 (2015), pp. 169–181,https://doi.org/10.1109/TSP.2014.2369000.[15] B. Cs. Cs´aji and K. B. Kis, Distribution-free uncertainty quantiﬁcation for kernel methodsby gradient perturbations, Machine Learning, 108 (2019), pp. 1677–1699.42[16] F. Dabbene, M. Sznaier, and R. Tempo, Probabilistic optimal estimation with uniformlydistributed noise, IEEE Transasctions on Automatic Control, 59 (2014), pp. 2113–2127.[17] M. Dalai, E. Weyer, and M. C. Campi, Parameter identiﬁcation for non-linear sys-tems: guaranteed conﬁdence regions through LSCR, Automatica, 43 (2007), pp. 1418–1425.[18] S. Dean, H. Mania, N. Matni, B. Recht, and S. Tu, On the sample complexity of thelinear quadratic regulator, Foundations of Computational Mathematics, 20 (2020), pp. 633–679.[19] A. D. Evstifeev, G. A. Volkov, A. A. Chevrychkina, and Y. V. Petrov, Strengthperformance of 1230 aluminum alloy under tension in the quasi-static and dynamic ranges ofloading parameters, Technical Physics, 64 (2019), pp. 620–624.[20] S. Fattahi, N. Matni, and S. Sojoudi, Eﬃcient learning of distributed linear-quadraticcontrol policies, SIAM Journal on Control and Optimization, 58 (2020), pp. 2927–2951,https://doi.org/10.1137/19M1291108, https://doi.org/10.1137/19M1291108.[21] D. Foster, T. Sarkar, and A. Rakhlin, Learning nonlinear dynamical systems froma single trajectory, in Learning for Dynamics and Control, Proceedings of Machine LearningResearch, 2020, pp. 851–861.[22] S. Garatti, M. C. Campi, and S. Bittanti, Assessing the quality of identiﬁed modelsthrough the asymptotic theory – when is the result reliable?, Automatica, 40 (2004), pp. 1319–1332.[23] A. Goldenshluger, Nonparametric estimation of transfer functions: rates of convergenceand adaptation, IEEE Transactions on Information Theory, 44 (1998), pp. 644–658.[24] N. Granichin, G. Volkov, Y. Petrov, and M. Volkova, Randomized approach todetermine dynamic strength of ice, Cybernetics and Physics, 10 (2021), pp. 122–126.[25] O. N. Granichin, The nonasymptotic conﬁdence set for parameters of a linear control objectunder an arbitrary external disturbance, Automation and Remote Control, 73 (2012), pp. 20–30.[26] C.-Y. Han, M. Kieffer, and A. Lambert, Guaranteed conﬁdence region characterizationfor source localization using RSS measurements, Signal Processing, 152 (2018), pp. 104–117.[27] U. D. Hanebeck,andticalhttps://doi.org/https://doi.org/10.1016/S0005-1098(99)00011-4.set-theoretic35J. Horn,estimation,and G.Automatica,Schmidt, On(1999),43combiningstatis-1101–1109,pp.[28] M. Hardt, T. Ma, and B. Recht, Gradient descent learns linear dynamical systems,Journal of Machine Learning Research, 19 (2018), pp. 1–44.[29] M. Karimshoushtari and C. Novara, Design of experiments for nonlinear system identi-ﬁcation: A set membership approach, Automatica, 119 (2020), p. 109036.[30] M. M. Khorasani and E. Weyer, Non-asymptotic conﬁdence regions for the parametersof EIV systems, Automatica, 115 (2020), p. 108873.[31] M. M. Khorasani and E. Weyer, Non-asymptotic conﬁdence regions for the transferfunctions of errors-in-variables systems, IEEE Transactions on Automatic Control, 67 (2021),pp. 2373–2388.[32] M. Kieffer and E. Walter, Guaranteed estimation ofinterval25linearJournalhttps://doi.org/https://doi.org/10.1002/acs.1194.continuous-time models:of Adaptive Controland Signal Processing,Contributionsofthe parameters of non-International191–207,analysis,(2011),pp.[33] M. Kieffer and E. Walter, Guaranteed characterization of exact non-asymptotic conﬁ-dence regions as deﬁned by LSCR and SPS, Automatica, 49 (2013), pp. 507–512.[34] S. Kolumb´an, I. Vajk, and J. Schoukens, Perturbed datasets methods for hypothesistesting and structure of corresponding conﬁdence sets, Automatica, 51 (2015), pp. 326–331.[35] L. Ljung, Characterization of the concept of ’persistently exciting’ in the frequency domain,Tech. Report TFRT-3038, Department of Automatic Control, Lund Institute of Technology(LTH), 1971.[36] L. Ljung, Consistency of the least squares identiﬁcation method, IEEE Transactions on Au-tomatic Control, 21 (1976), pp. 779–781.[37] L. Ljung, System Identiﬁcation: Theory for the User, Prentice-Hall, Upper Saddle River,2nd ed., 1999.[38] H. Mania, M. I. Jordan, and B. Recht, Active learning for nonlinear system identiﬁcationwith guarantees, Journal of Machine Learning Research, 23 (2022), pp. 1–30.[39] M. Milanese, J. Norton, H. Piet-Lahanier, and ´E. Walter, Bounding approaches tosystem identiﬁcation, Springer Science & Business Media, 2013.[40] M. Milanese and C. Novara, Uniﬁed set membership theory for identiﬁcation, predictionand ﬁltering of nonlinear systems, Automatica, 47 (2011), pp. 2141 – 2151.44[41] M. Milanese and M. Taragna, H41 (2005), pp. 2019 – 2032.set membership identiﬁcation: A survey, Automatica,∞[42] S. Oymak and N. Ozay, Non-asymptotic identiﬁcation of LTI systems from a single trajec-tory, in 2019 American control conference (ACC), IEEE, 2019, pp. 5655–5661.[43] J. Pereira, M. Ibrahimi, and A. Montanari, Learning networks of stochastic diﬀerentialequations, Advances in Neural Information Processing Systems, 23 (2010).[44] P. Polterauer, H. Kirchsteiger, and L. del Re, State observation with guaranteedconﬁdence regions through sign perturbed sums, in 2015 54th IEEE Conference on Decision andControl (CDC), 2015, pp. 5660–5665, https://doi.org/10.1109/CDC.2015.7403107.[45] J. G. Proakis, Digital signal processing: principles, algorithms, and application-3/E.,Prentice-Hall, 1996.[46] M. Quincampoix and V. M. Veliov, Optimal control of uncertain systems with incom-plete information for the disturbances, SIAM Journal on Control and Optimization, 43 (2004),pp. 1373–1399, https://doi.org/10.1137/S0363012903420863.[47] T. Sarkar, A. Rakhlin, and M. A. Dahleh, Finite time LTI system identiﬁcation, Jour-nal of Machine Learning Research, 22 (2021), pp. 1186–1246.[48] Y. Sattar and S. Oymak, Non-asymptotic and accurate learning of nonlinear dynamicalsystems, Journal of Machine Learning Research, 23 (2022), pp. 6248–6296.[49] P. Shah, B. N. Bhaskar, G. Tang, and B. Recht, Linear system identiﬁcation via atomicnorm regularization, in 2012 IEEE 51st IEEE conference on decision and control (CDC), IEEE,2012, pp. 6265–6270.[50] A. N. Shiryaev, Probability, Springer, 2 ed., 1995.[51] Y. Sun, S. Oymak, and M. Fazel, Finite sample identiﬁcation of low-order LTI systems vianuclear norm regularization, IEEE Open Journal of Control Systems, 1 (2022), pp. 237–254,https://doi.org/10.1109/OJCSYS.2022.3200015.[52] Sz. Szentp´eteri and B. Cs. Cs´aji, Non-asymptotic state-space identiﬁcation of closed-loopstochastic linear systems using instrumental variables, Systems & Control Letters, 178 (2023),p. 105565, https://doi.org/https://doi.org/10.1016/j.sysconle.2023.105565.45[53] S. Y. Trapitsin, O. A. Granichina, and O. N. Granichin, Social capital of professorsas a factor of increasing the eﬀectiveness of the university, in 2018 IEEE International Con-ference ”Quality Management, Transport and Information Security, Information Technologies”(IT&QM&IS), 2018, pp. 739–742, https://doi.org/10.1109/ITMQIS.2018.8524943.[54] A. Tsiamis and G. J. Pappas, Finite sample analysis of stochastic system identiﬁca-tion, in 2019 IEEE 58th Conference on Decision and Control (CDC), 2019, pp. 3648–3654,https://doi.org/10.1109/CDC40024.2019.9029499.[55] A. Tsiamis and G. J. Pappas, Linearin2021 60th IEEE Conference on Decision and Control (CDC), 2021, pp. 2903–2910,https://doi.org/10.1109/CDC45484.2021.9682778.can be hard to learn,systems[56] A. Tsiamis, I. Ziemann, N. Matni, and G. J. Pappas, Statistical learning theory forcontrol: A ﬁnite-sample perspective, IEEE Control Systems Magazine, 43 (2023), pp. 67–97.[57] A. W. van der Vaart, Asymptotic Statistics, Cambridge University Press, 1998.[58] M. Verhaegen and V. Verdult, Filtering and system identiﬁcation: a least squares ap-proach, Cambridge university press, 2007.[59] M. Vidyasagar and R. L. Karandikar, System identiﬁcation: A learning theory ap-proach,in Control and Modeling of Complex Systems: Cybernetics in the 21st CenturyFestschrift in Honor of Hidenori Kimura on the Occasion of his 60th Birthday, K. Hashimoto,Y. Oishi, and Y. Yamamoto, eds., Birkh¨auser Boston, Boston, MA, 2003, pp. 89–104,https://doi.org/10.1007/978-1-4612-0023-9_6.[60] M. Vidyasagar and R. L. Karandikar, A learning theory approach to system identiﬁca-tion and stochastic adaptive control, in Probabilistic and Randomized Methods for Design underUncertainty, G. Calaﬁore and F. Dabbene, eds., Springer London, London, 2006, pp. 265–302,https://doi.org/10.1007/1-84628-095-8_10.[61] G. A. Volkov, A. A. Gruzdkov, and Y. V. Petrov, A randomized approach to estimateacoustic strength of water, in Mechanics and Control of Solids and Structures, Springer, 2022,pp. 633–640.[62] M. Volkova, O. Granichin, Y. Petrov, and G. Volkov, Dynamic fracture tests dataanalysis based on the randomized approach, Advances in Systems Science and Applications, 17(2017), pp. 34–41.46[63] M. Volkova, O. Granichin, G. Volkov, and Y. V. Petrov, On the possibility ofusing the method of sign-perturbed sums for the processing of dynamic test data, Vestnik St.Petersburg University, Mathematics, 51 (2018), pp. 23–30.[64] V. Volpe, B. C. Cs´aji, A. Car`e, E. Weyer, and M. C. Campi, Sign-perturbed sums(sps) with instrumental variables for the identiﬁcation of arx systems, in 2015 54th IEEEConference on Decision and Control (CDC), IEEE, 2015, pp. 2115–2120.[65] E. Weyer,Finiteunder mixingsampleconditions,modelshttps://doi.org/https://doi.org/10.1016/S0005-1098(00)00039-X.Automatica,36propertiesofsystem identiﬁcationpp.(2000),of ARX1291–1299,[66] E. Weyer, M. C. Campi,Asymptotictiespp.287https://doi.org/https://doi.org/10.1016/j.automatica.2017.04.041.and B. Cs. Cs´aji,(2017),regions, Automatica,conﬁdenceSPS82ofproper-294,–[67] E. Weyer, R. C. Williamson, and I. M. Mareels, Sample complexity ofleastsquares identiﬁcation of FIR models, IFAC Proceedings Volumes, 29 (1996), pp. 4664–4669, https://doi.org/https://doi.org/10.1016/S1474-6670(17)58418-9. 13th WorldCongress of IFAC, 1996, San Francisco USA, 30 June - 5 July.[68] E. Weyer, R. C. Williamson, and I. M. Mareels, Finite sample properties of linearmodel identiﬁcation, IEEE Transactions on Automatic Control, 44 (1999), pp. 1370–1383.[69] V. Zambianchi, F. Bassi, A. Calisti, D. Dardari, M. Kieffer, and G. Pa-solini, Distributed nonasymptotic conﬁdence region computation over sensor networks, IEEETransactions on Signal and Information Processing over Networks, 4 (2018), pp. 308–324,https://doi.org/10.1109/TSIPN.2017.2695403.47', 'pdf/Signed-Perturbed_Sums_Estimation_of_ARX_Systems__Exact_Coverage_and_Strong_Consistency_(Extended_Version).pdf', '[[-0.2934061586856842041015625000000000000000\n  -0.1565750092267990112304687500000000000000\n  0.1907610446214675903320312500000000000000\n  0.0538077726960182189941406250000000000000\n  -0.3253504037857055664062500000000000000000\n  -0.2949724793434143066406250000000000000000\n  0.0558121576905250549316406250000000000000\n  -0.1863768696784973144531250000000000000000\n  0.2130790948867797851562500000000000000000\n  -0.1279128491878509521484375000000000000000\n  -0.2397598475217819213867187500000000000000\n  -0.0084492918103933334350585937500000000000\n  0.1995406001806259155273437500000000000000\n  0.2149348258972167968750000000000000000000\n  -0.0717216730117797851562500000000000000000\n  0.2455947399139404296875000000000000000000\n  0.1156027838587760925292968750000000000000\n  0.1782178282737731933593750000000000000000\n  -0.0507021993398666381835937500000000000000\n  -0.0229379814118146896362304687500000000000\n  0.2592439651489257812500000000000000000000\n  0.1176034584641456604003906250000000000000\n  0.3626840114593505859375000000000000000000\n  -0.0240419730544090270996093750000000000000\n  -0.2587522864341735839843750000000000000000\n  0.0345431230962276458740234375000000000000\n  0.1275754570960998535156250000000000000000\n  0.3024238646030426025390625000000000000000\n  0.1776655018329620361328125000000000000000\n  0.3104554414749145507812500000000000000000\n  -0.0437533743679523468017578125000000000000\n  -0.0264418181031942367553710937500000000000\n  0.1538701504468917846679687500000000000000\n  -0.3087672889232635498046875000000000000000\n  0.0233030915260314941406250000000000000000\n  -0.0244786366820335388183593750000000000000\n  0.0780365914106369018554687500000000000000\n  -0.0274060294032096862792968750000000000000\n  0.3545895516872406005859375000000000000000\n  -0.2136066257953643798828125000000000000000\n  -0.0548396222293376922607421875000000000000\n  0.1017166152596473693847656250000000000000\n  0.0493383593857288360595703125000000000000\n  -0.1101998388767242431640625000000000000000\n  0.0410505682229995727539062500000000000000\n  -0.1643412858247756958007812500000000000000\n  -3.2222332954406738281250000000000000000000\n  0.2391788810491561889648437500000000000000\n  -0.2706562876701354980468750000000000000000\n  -0.2875384986400604248046875000000000000000\n  0.2288343757390975952148437500000000000000\n  0.2047060132026672363281250000000000000000\n  0.0654331818222999572753906250000000000000\n  0.2240585982799530029296875000000000000000\n  -0.0171329677104949951171875000000000000000\n  0.1734534502029418945312500000000000000000\n  0.0893433094024658203125000000000000000000\n  0.0922533199191093444824218750000000000000\n  0.3019408285617828369140625000000000000000\n  0.1127457618713378906250000000000000000000\n  0.2433158606290817260742187500000000000000\n  0.2411933541297912597656250000000000000000\n  -0.0832600817084312438964843750000000000000\n  0.2155690342187881469726562500000000000000\n  -0.1185675039887428283691406250000000000000\n  0.6138991117477416992187500000000000000000\n  -0.4117847979068756103515625000000000000000\n  0.3379296362400054931640625000000000000000\n  -0.7220358848571777343750000000000000000000\n  0.5832050442695617675781250000000000000000\n  -0.2638355195522308349609375000000000000000\n  -0.1082095876336097717285156250000000000000\n  -0.0533505417406558990478515625000000000000\n  0.0094033852219581604003906250000000000000\n  0.2941847145557403564453125000000000000000\n  0.0392913073301315307617187500000000000000\n  0.0513049587607383728027343750000000000000\n  0.2361094057559967041015625000000000000000\n  0.1405227780342102050781250000000000000000\n  0.4531224370002746582031250000000000000000\n  0.0656085088849067687988281250000000000000\n  -0.0557280853390693664550781250000000000000\n  0.2177926898002624511718750000000000000000\n  -0.4779547452926635742187500000000000000000\n  0.1230686306953430175781250000000000000000\n  0.2513189315795898437500000000000000000000\n  0.1570529788732528686523437500000000000000\n  0.0241113565862178802490234375000000000000\n  0.2553687095642089843750000000000000000000\n  0.3266935050487518310546875000000000000000\n  0.0595796182751655578613281250000000000000\n  -0.0736734494566917419433593750000000000000\n  0.2559524774551391601562500000000000000000\n  -0.0533185116946697235107421875000000000000\n  0.1764014661312103271484375000000000000000\n  -0.1574388891458511352539062500000000000000\n  -0.1256382614374160766601562500000000000000\n  -0.1786085069179534912109375000000000000000\n  0.0729895010590553283691406250000000000000\n  0.1249626278877258300781250000000000000000\n  0.1689384430646896362304687500000000000000\n  0.0456247366964817047119140625000000000000\n  0.1305982172489166259765625000000000000000\n  -0.6332718729972839355468750000000000000000\n  0.1622403264045715332031250000000000000000\n  -0.2295344620943069458007812500000000000000\n  -0.4366131722927093505859375000000000000000\n  -0.3152360618114471435546875000000000000000\n  -0.0173067171126604080200195312500000000000\n  -1.5490068197250366210937500000000000000000\n  0.1421568095684051513671875000000000000000\n  0.6967508792877197265625000000000000000000\n  -0.1631939113140106201171875000000000000000\n  -0.1815903931856155395507812500000000000000\n  0.2310698628425598144531250000000000000000\n  -0.1284012496471405029296875000000000000000\n  -0.0442077890038490295410156250000000000000\n  0.3568549752235412597656250000000000000000\n  0.0585987456142902374267578125000000000000\n  -0.0187077373266220092773437500000000000000\n  -0.2841184735298156738281250000000000000000\n  0.1804472506046295166015625000000000000000\n  0.2832200527191162109375000000000000000000\n  -0.1572124361991882324218750000000000000000\n  -0.3158532381057739257812500000000000000000\n  0.2604168057441711425781250000000000000000\n  0.2076552510261535644531250000000000000000\n  -0.5412507653236389160156250000000000000000\n  0.2902546525001525878906250000000000000000\n  0.4833889305591583251953125000000000000000\n  -0.1170418560504913330078125000000000000000\n  0.0743197649717330932617187500000000000000\n  -0.1790146827697753906250000000000000000000\n  -0.0099936276674270629882812500000000000000\n  -0.3649632334709167480468750000000000000000\n  -0.1392660737037658691406250000000000000000\n  0.2993029057979583740234375000000000000000\n  -0.0353890024125576019287109375000000000000\n  0.0501192770898342132568359375000000000000\n  -0.0165917202830314636230468750000000000000\n  -0.6265962719917297363281250000000000000000\n  -0.1103913336992263793945312500000000000000\n  -1.9629248380661010742187500000000000000000\n  0.1260723471641540527343750000000000000000\n  0.2444574981927871704101562500000000000000\n  0.0690466761589050292968750000000000000000\n  -0.0028916818555444478988647460937500000000\n  0.3841621279716491699218750000000000000000\n  -0.4557395577430725097656250000000000000000\n  -0.0670542269945144653320312500000000000000\n  0.1054609939455986022949218750000000000000\n  -0.1457482725381851196289062500000000000000\n  -0.2252511978149414062500000000000000000000\n  0.0469632185995578765869140625000000000000\n  -0.2054241895675659179687500000000000000000\n  0.0939263179898262023925781250000000000000\n  -0.2004200220108032226562500000000000000000\n  -0.0384687222540378570556640625000000000000\n  0.0633025839924812316894531250000000000000\n  0.1058305501937866210937500000000000000000\n  -0.1556236743927001953125000000000000000000\n  0.0735278800129890441894531250000000000000\n  -0.0138181839138269424438476562500000000000\n  -0.1023406833410263061523437500000000000000\n  0.1389446109533309936523437500000000000000\n  -0.1507754623889923095703125000000000000000\n  0.2048596441745758056640625000000000000000\n  0.0864368528127670288085937500000000000000\n  -0.2266565859317779541015625000000000000000\n  -0.1001414507627487182617187500000000000000\n  0.4768297672271728515625000000000000000000\n  -0.2846062481403350830078125000000000000000\n  0.3441043496131896972656250000000000000000\n  -0.2273045331239700317382812500000000000000\n  0.0461636930704116821289062500000000000000\n  -0.1032820343971252441406250000000000000000\n  0.1721338331699371337890625000000000000000\n  0.3009970486164093017578125000000000000000\n  0.1530500501394271850585937500000000000000\n  0.0044348295778036117553710937500000000000\n  -0.2970102429389953613281250000000000000000\n  0.3337465524673461914062500000000000000000\n  0.0266312882304191589355468750000000000000\n  -0.4673352539539337158203125000000000000000\n  -0.3272607326507568359375000000000000000000\n  0.2726016342639923095703125000000000000000\n  0.2833547592163085937500000000000000000000\n  -0.0245063956826925277709960937500000000000\n  0.1481826305389404296875000000000000000000\n  0.0320538021624088287353515625000000000000\n  -0.3646305203437805175781250000000000000000\n  -0.0726597607135772705078125000000000000000\n  0.2108825892210006713867187500000000000000\n  -0.0334853567183017730712890625000000000000\n  0.4215492010116577148437500000000000000000\n  0.3748886585235595703125000000000000000000\n  -0.1422885358333587646484375000000000000000\n  -0.0565985888242721557617187500000000000000\n  0.0981822907924652099609375000000000000000\n  -0.1087094843387603759765625000000000000000\n  0.0814910084009170532226562500000000000000\n  0.2284255772829055786132812500000000000000\n  -0.1826164573431015014648437500000000000000\n  0.2013657987117767333984375000000000000000\n  -0.0381485074758529663085937500000000000000\n  2.3667106628417968750000000000000000000000\n  0.1342399716377258300781250000000000000000\n  0.0632648319005966186523437500000000000000\n  0.0157044120132923126220703125000000000000\n  -0.1560489535331726074218750000000000000000\n  -0.1166585832834243774414062500000000000000\n  -0.2252130657434463500976562500000000000000\n  -0.0862345770001411437988281250000000000000\n  -0.3666256368160247802734375000000000000000\n  -0.0173831209540367126464843750000000000000\n  -0.0725445896387100219726562500000000000000\n  0.1561900824308395385742187500000000000000\n  -0.1504592597484588623046875000000000000000\n  0.1822101920843124389648437500000000000000\n  -0.2131752967834472656250000000000000000000\n  -0.0037001520395278930664062500000000000000\n  0.1713187098503112792968750000000000000000\n  -0.0451890155673027038574218750000000000000\n  0.3429393172264099121093750000000000000000\n  -0.1890543550252914428710937500000000000000\n  -0.0110908374190330505371093750000000000000\n  -0.0914417356252670288085937500000000000000\n  -0.5745021104812622070312500000000000000000\n  0.1128331720829010009765625000000000000000\n  -1.2412101030349731445312500000000000000000\n  0.2343208044767379760742187500000000000000\n  -0.4115609526634216308593750000000000000000\n  0.0825832709670066833496093750000000000000\n  -0.1163950562477111816406250000000000000000\n  -0.4821189045906066894531250000000000000000\n  -0.0056751407682895660400390625000000000000\n  -0.0959870070219039916992187500000000000000\n  -0.3994097113609313964843750000000000000000\n  0.0113279111683368682861328125000000000000\n  -0.3297190368175506591796875000000000000000\n  -0.0655746906995773315429687500000000000000\n  0.3910396099090576171875000000000000000000\n  -0.0623575448989868164062500000000000000000\n  0.0260861609131097793579101562500000000000\n  -0.2533606290817260742187500000000000000000\n  -0.1327949613332748413085937500000000000000\n  0.1436714679002761840820312500000000000000\n  0.0428002402186393737792968750000000000000\n  -0.0288630425930023193359375000000000000000\n  -0.0859023928642272949218750000000000000000\n  0.0314286723732948303222656250000000000000\n  -0.1898514926433563232421875000000000000000\n  -0.0624048784375190734863281250000000000000\n  -0.0103150624781847000122070312500000000000\n  0.1418581604957580566406250000000000000000\n  0.2204307019710540771484375000000000000000\n  0.1182794868946075439453125000000000000000\n  0.0872960239648818969726562500000000000000\n  -0.2990341782569885253906250000000000000000\n  -0.1364689022302627563476562500000000000000\n  -0.0592440739274024963378906250000000000000\n  0.1029181778430938720703125000000000000000\n  0.0656975135207176208496093750000000000000\n  0.1333547234535217285156250000000000000000\n  -0.3485132455825805664062500000000000000000\n  0.0190282315015792846679687500000000000000\n  0.4438534379005432128906250000000000000000\n  -0.3734301924705505371093750000000000000000\n  0.1275956928730010986328125000000000000000\n  -0.0897715538740158081054687500000000000000\n  -0.1067004352807998657226562500000000000000\n  -0.0885441228747367858886718750000000000000\n  -0.3766957521438598632812500000000000000000\n  -1.2784295082092285156250000000000000000000\n  -0.1953098177909851074218750000000000000000\n  0.0595606341958045959472656250000000000000\n  0.5139955282211303710937500000000000000000\n  -0.1347480863332748413085937500000000000000\n  -0.1058718487620353698730468750000000000000\n  -0.0073551461100578308105468750000000000000\n  0.1710183918476104736328125000000000000000\n  0.2401446104049682617187500000000000000000\n  0.1887744218111038208007812500000000000000\n  -0.1707846969366073608398437500000000000000\n  0.0004208106547594070434570312500000000000\n  0.0337990559637546539306640625000000000000\n  0.1629990190267562866210937500000000000000\n  -0.1516131162643432617187500000000000000000\n  0.5932105779647827148437500000000000000000\n  -0.1486533135175704956054687500000000000000\n  -0.0999481454491615295410156250000000000000\n  0.5593409538269042968750000000000000000000\n  -0.0151424929499626159667968750000000000000\n  0.0806772708892822265625000000000000000000\n  0.2693549394607543945312500000000000000000\n  -0.5063232183456420898437500000000000000000\n  0.2025003731250762939453125000000000000000\n  0.0178748555481433868408203125000000000000\n  -0.0720359832048416137695312500000000000000\n  -0.0484034940600395202636718750000000000000\n  -0.5401554107666015625000000000000000000000\n  -0.1387932598590850830078125000000000000000\n  0.1845010071992874145507812500000000000000\n  0.1078059822320938110351562500000000000000\n  -0.0181734357029199600219726562500000000000\n  0.2638301253318786621093750000000000000000\n  -0.1109789460897445678710937500000000000000\n  -0.2586158514022827148437500000000000000000\n  -4.3500723838806152343750000000000000000000\n  -0.0994748622179031372070312500000000000000\n  -0.4334253072738647460937500000000000000000\n  -0.5418526530265808105468750000000000000000\n  0.1432727724313735961914062500000000000000\n  -0.1838594824075698852539062500000000000000\n  0.5139117836952209472656250000000000000000\n  0.0370463840663433074951171875000000000000\n  0.0404101759195327758789062500000000000000\n  -0.0691487938165664672851562500000000000000\n  -0.1082501634955406188964843750000000000000\n  0.1160588935017585754394531250000000000000\n  -0.2580765187740325927734375000000000000000\n  -0.0637400597333908081054687500000000000000\n  -0.2266928702592849731445312500000000000000\n  -0.2129551470279693603515625000000000000000\n  0.5767709016799926757812500000000000000000\n  0.0118845514953136444091796875000000000000\n  -0.0319507606327533721923828125000000000000\n  0.0238342285156250000000000000000000000000\n  -0.3262076973915100097656250000000000000000\n  -0.3201915025711059570312500000000000000000\n  0.1925174891948699951171875000000000000000\n  0.2879995703697204589843750000000000000000\n  0.4454572200775146484375000000000000000000\n  0.2011767774820327758789062500000000000000\n  -0.6957142353057861328125000000000000000000\n  -0.2475127577781677246093750000000000000000\n  0.1510290205478668212890625000000000000000\n  -0.1560458242893218994140625000000000000000\n  0.0745420083403587341308593750000000000000\n  -0.6130894422531127929687500000000000000000\n  -0.0013582222163677215576171875000000000000\n  0.1437765955924987792968750000000000000000\n  -0.1867894679307937622070312500000000000000\n  0.0154115036129951477050781250000000000000\n  -0.0083081452175974845886230468750000000000\n  0.0719191804528236389160156250000000000000\n  0.4793466925621032714843750000000000000000\n  -0.2907027900218963623046875000000000000000\n  0.2000202387571334838867187500000000000000\n  0.3095946609973907470703125000000000000000\n  0.0913080126047134399414062500000000000000\n  -0.1154852882027626037597656250000000000000\n  0.5079692602157592773437500000000000000000\n  -0.0133261680603027343750000000000000000000\n  0.0047796852886676788330078125000000000000\n  0.5002717971801757812500000000000000000000\n  0.2339364290237426757812500000000000000000\n  0.1215767264366149902343750000000000000000\n  -0.0377402268350124359130859375000000000000\n  0.1165375933051109313964843750000000000000\n  0.9088797569274902343750000000000000000000\n  -0.0742594450712203979492187500000000000000\n  0.0121190436184406280517578125000000000000\n  0.0168424509465694427490234375000000000000\n  0.0848335772752761840820312500000000000000\n  -0.2037197351455688476562500000000000000000\n  -0.2483680844306945800781250000000000000000\n  0.1137925758957862854003906250000000000000\n  0.2606122195720672607421875000000000000000\n  -0.2362971156835556030273437500000000000000\n  -0.0383998677134513854980468750000000000000\n  -0.1425153464078903198242187500000000000000\n  0.0702218487858772277832031250000000000000\n  -0.2021154016256332397460937500000000000000\n  0.1563908904790878295898437500000000000000\n  -0.1431407630443572998046875000000000000000\n  -0.1103549376130104064941406250000000000000\n  -0.0770481675863265991210937500000000000000\n  -0.6278939843177795410156250000000000000000\n  0.0566984452307224273681640625000000000000\n  0.1229843422770500183105468750000000000000\n  -0.3045332431793212890625000000000000000000\n  0.1770799756050109863281250000000000000000\n  -0.3587293326854705810546875000000000000000\n  -0.2009001672267913818359375000000000000000\n  0.1984099745750427246093750000000000000000\n  -0.1294496357440948486328125000000000000000\n  0.0984658002853393554687500000000000000000\n  -0.9169858098030090332031250000000000000000\n  0.0194546636193990707397460937500000000000\n  0.0197869539260864257812500000000000000000\n  0.3401126563549041748046875000000000000000\n  -0.2305745631456375122070312500000000000000\n  0.1996783763170242309570312500000000000000\n  -0.0013578832149505615234375000000000000000\n  -0.0075368508696556091308593750000000000000\n  -0.1924181282520294189453125000000000000000\n  0.5333575010299682617187500000000000000000\n  -0.1405692398548126220703125000000000000000\n  0.0356327146291732788085937500000000000000\n  0.3975223302841186523437500000000000000000\n  0.0461767800152301788330078125000000000000\n  -0.3415463864803314208984375000000000000000\n  0.3260746002197265625000000000000000000000\n  0.2402164936065673828125000000000000000000\n  -0.1669434458017349243164062500000000000000\n  0.4183117747306823730468750000000000000000\n  -0.4701632559299468994140625000000000000000\n  0.2649635672569274902343750000000000000000\n  -0.3088948130607604980468750000000000000000\n  -0.1186781078577041625976562500000000000000\n  -0.1954241096973419189453125000000000000000\n  -0.1889546960592269897460937500000000000000\n  0.0150121934711933135986328125000000000000\n  -0.6011517047882080078125000000000000000000\n  -0.1020784378051757812500000000000000000000\n  0.0870815515518188476562500000000000000000\n  -0.0598227344453334808349609375000000000000\n  -0.2532670795917510986328125000000000000000\n  0.3883644044399261474609375000000000000000\n  -0.1282055974006652832031250000000000000000\n  0.0350887477397918701171875000000000000000\n  0.9139811992645263671875000000000000000000\n  -0.4158048629760742187500000000000000000000\n  -0.1893776506185531616210937500000000000000\n  0.0897087752819061279296875000000000000000\n  0.2776124477386474609375000000000000000000\n  -0.1767470538616180419921875000000000000000\n  0.0595274642109870910644531250000000000000\n  -0.1819715946912765502929687500000000000000\n  0.0153335481882095336914062500000000000000\n  0.0450799465179443359375000000000000000000\n  -0.2837345600128173828125000000000000000000\n  0.3140706419944763183593750000000000000000\n  0.2287900149822235107421875000000000000000\n  -0.0519000105559825897216796875000000000000\n  -0.1143570244312286376953125000000000000000\n  -0.1794670820236206054687500000000000000000\n  0.1218583136796951293945312500000000000000\n  -0.0502272062003612518310546875000000000000\n  -0.4020519256591796875000000000000000000000\n  -0.1622607409954071044921875000000000000000\n  0.0873232930898666381835937500000000000000\n  -0.0595057792961597442626953125000000000000\n  -0.2607886493206024169921875000000000000000\n  0.2148086279630661010742187500000000000000\n  0.1471816599369049072265625000000000000000\n  -0.1766166985034942626953125000000000000000\n  0.4439715743064880371093750000000000000000\n  0.1595994681119918823242187500000000000000\n  -0.1490787416696548461914062500000000000000\n  0.4236947000026702880859375000000000000000\n  0.3268038332462310791015625000000000000000\n  0.0559739209711551666259765625000000000000\n  0.1363733112812042236328125000000000000000\n  0.0826121270656585693359375000000000000000\n  -0.2258294224739074707031250000000000000000\n  0.2793144583702087402343750000000000000000\n  -0.0694102123379707336425781250000000000000\n  -0.7102401256561279296875000000000000000000\n  0.1233267411589622497558593750000000000000\n  -0.1348441392183303833007812500000000000000\n  0.4424476623535156250000000000000000000000\n  0.5833576917648315429687500000000000000000\n  -0.1418083310127258300781250000000000000000\n  -0.1958444118499755859375000000000000000000\n  -0.3560102283954620361328125000000000000000\n  -0.1233016625046730041503906250000000000000\n  0.4007589519023895263671875000000000000000\n  0.0810112878680229187011718750000000000000\n  -1.7573636770248413085937500000000000000000\n  0.1439654380083084106445312500000000000000\n  0.2461706697940826416015625000000000000000\n  -0.1293697655200958251953125000000000000000\n  0.1067300438880920410156250000000000000000\n  0.1038248836994171142578125000000000000000\n  0.1614245623350143432617187500000000000000\n  0.0044521242380142211914062500000000000000\n  0.2229002267122268676757812500000000000000\n  0.1828177124261856079101562500000000000000\n  -0.0360289104282855987548828125000000000000\n  -0.0063977064564824104309082031250000000000\n  0.2580588459968566894531250000000000000000\n  0.2386644631624221801757812500000000000000\n  0.4136695861816406250000000000000000000000\n  0.0823965445160865783691406250000000000000\n  -0.0873187631368637084960937500000000000000\n  0.2824037373065948486328125000000000000000\n  -0.0850816667079925537109375000000000000000\n  0.0605354383587837219238281250000000000000\n  0.0417787171900272369384765625000000000000\n  0.8770999908447265625000000000000000000000\n  0.2072447240352630615234375000000000000000\n  0.0739909633994102478027343750000000000000\n  0.1694739609956741333007812500000000000000\n  -0.1318549066781997680664062500000000000000\n  -0.1829321682453155517578125000000000000000\n  0.3388804495334625244140625000000000000000\n  0.1323192566633224487304687500000000000000\n  -0.1990639716386795043945312500000000000000\n  0.2425452172756195068359375000000000000000\n  -0.7130491733551025390625000000000000000000\n  0.1837410181760787963867187500000000000000\n  -0.0926213413476943969726562500000000000000\n  0.3991571962833404541015625000000000000000\n  0.1930661201477050781250000000000000000000\n  -0.1401846110820770263671875000000000000000\n  0.1089766174554824829101562500000000000000\n  0.0136784631758928298950195312500000000000\n  0.4385106563568115234375000000000000000000\n  -0.0508783832192420959472656250000000000000\n  0.2997254431247711181640625000000000000000\n  0.4664380550384521484375000000000000000000\n  0.1438851803541183471679687500000000000000\n  0.1469144821166992187500000000000000000000\n  0.1891068518161773681640625000000000000000\n  -0.1832661032676696777343750000000000000000\n  -0.4175700247287750244140625000000000000000\n  0.1594010591506958007812500000000000000000\n  -0.0959962606430053710937500000000000000000\n  0.3784841895103454589843750000000000000000\n  0.4419749677181243896484375000000000000000\n  0.2165349870920181274414062500000000000000\n  -0.4032098352909088134765625000000000000000\n  -0.1864656656980514526367187500000000000000\n  -0.0387337580323219299316406250000000000000\n  0.0490757115185260772705078125000000000000\n  -0.2035249918699264526367187500000000000000\n  0.2269718199968338012695312500000000000000\n  -0.3063469529151916503906250000000000000000\n  0.2093250155448913574218750000000000000000\n  -0.5112127065658569335937500000000000000000\n  0.2315900623798370361328125000000000000000\n  -0.0285124480724334716796875000000000000000\n  0.1342183351516723632812500000000000000000\n  -0.6425123214721679687500000000000000000000\n  0.3925957977771759033203125000000000000000\n  0.0889689102768898010253906250000000000000\n  -0.0538129284977912902832031250000000000000\n  -0.2643716633319854736328125000000000000000\n  0.2173533439636230468750000000000000000000\n  0.0084498673677444458007812500000000000000\n  -0.6720849275588989257812500000000000000000\n  -0.1949188113212585449218750000000000000000\n  0.2274735271930694580078125000000000000000\n  0.1182791665196418762207031250000000000000\n  0.2319354861974716186523437500000000000000\n  0.4862978756427764892578125000000000000000\n  -0.0879839807748794555664062500000000000000\n  -0.4059097170829772949218750000000000000000\n  0.2038577347993850708007812500000000000000\n  -0.4998533129692077636718750000000000000000\n  -0.3227032423019409179687500000000000000000\n  0.1751172393560409545898437500000000000000\n  -0.0042245816439390182495117187500000000000\n  -0.1772093772888183593750000000000000000000\n  -0.0931968837976455688476562500000000000000\n  0.2231030762195587158203125000000000000000\n  0.2052274942398071289062500000000000000000\n  -0.0489192008972167968750000000000000000000\n  -0.2454221397638320922851562500000000000000\n  -0.3872165977954864501953125000000000000000\n  0.3180216550827026367187500000000000000000\n  0.3455259799957275390625000000000000000000\n  0.1063598319888114929199218750000000000000\n  -0.1718396544456481933593750000000000000000\n  -0.1788376569747924804687500000000000000000\n  -0.1767912805080413818359375000000000000000\n  -0.1155878454446792602539062500000000000000\n  0.0165752265602350234985351562500000000000\n  -0.1688679158687591552734375000000000000000\n  0.1469092667102813720703125000000000000000\n  -0.1536757498979568481445312500000000000000\n  0.0290393568575382232666015625000000000000\n  0.1088391542434692382812500000000000000000\n  0.0388957038521766662597656250000000000000\n  0.5230138301849365234375000000000000000000\n  -0.4484375417232513427734375000000000000000\n  -0.2935919761657714843750000000000000000000\n  0.1770103424787521362304687500000000000000\n  0.0447430498898029327392578125000000000000\n  0.0897012948989868164062500000000000000000\n  0.1318501532077789306640625000000000000000\n  0.1691953539848327636718750000000000000000\n  0.1374961137771606445312500000000000000000\n  0.0620270334184169769287109375000000000000\n  -0.1257056891918182373046875000000000000000\n  -0.1174907833337783813476562500000000000000\n  1.3251178264617919921875000000000000000000\n  0.0364626571536064147949218750000000000000\n  0.3604580760002136230468750000000000000000\n  0.0622257068753242492675781250000000000000\n  0.3493559956550598144531250000000000000000\n  0.1629831492900848388671875000000000000000\n  0.1023522689938545227050781250000000000000\n  -0.2178521007299423217773437500000000000000\n  0.0382916480302810668945312500000000000000\n  -0.2416804879903793334960937500000000000000\n  -0.2170670032501220703125000000000000000000\n  0.1084042415022850036621093750000000000000\n  -0.0911554843187332153320312500000000000000\n  -0.2233427762985229492187500000000000000000\n  0.0438155159354209899902343750000000000000\n  0.2915869355201721191406250000000000000000\n  -0.0202143080532550811767578125000000000000\n  0.0280441697686910629272460937500000000000\n  -0.5605721473693847656250000000000000000000\n  -0.0268341861665248870849609375000000000000\n  -0.2006509602069854736328125000000000000000\n  -0.1511483639478683471679687500000000000000\n  0.2054238766431808471679687500000000000000\n  -0.1088636443018913269042968750000000000000\n  -0.2345945090055465698242187500000000000000\n  0.1423133611679077148437500000000000000000\n  0.2922755181789398193359375000000000000000\n  -0.2477348595857620239257812500000000000000\n  -0.1874857693910598754882812500000000000000\n  -0.0729924887418746948242187500000000000000\n  -0.1957511156797409057617187500000000000000\n  -0.0528825297951698303222656250000000000000\n  0.3020212650299072265625000000000000000000\n  0.0367147177457809448242187500000000000000\n  0.0409560203552246093750000000000000000000\n  -0.1965995281934738159179687500000000000000\n  -0.1434548348188400268554687500000000000000\n  -0.2382099926471710205078125000000000000000\n  -0.0552372485399246215820312500000000000000\n  0.1545324623584747314453125000000000000000\n  -0.0011769174598157405853271484375000000000\n  -0.1003113836050033569335937500000000000000\n  0.5332249402999877929687500000000000000000\n  -0.1690621972084045410156250000000000000000\n  -0.0481878817081451416015625000000000000000\n  -0.0264081209897994995117187500000000000000\n  0.0405530557036399841308593750000000000000\n  -0.4416321218013763427734375000000000000000\n  0.1755643934011459350585937500000000000000\n  0.6995193958282470703125000000000000000000\n  0.0907562598586082458496093750000000000000\n  -0.2908405065536499023437500000000000000000\n  -0.2243019640445709228515625000000000000000\n  0.1978003233671188354492187500000000000000\n  -0.3666392266750335693359375000000000000000\n  0.2430990785360336303710937500000000000000\n  0.1501155793666839599609375000000000000000\n  -0.3303613662719726562500000000000000000000\n  -0.2261280417442321777343750000000000000000\n  0.3132542669773101806640625000000000000000\n  -0.0617804042994976043701171875000000000000\n  0.4481780529022216796875000000000000000000\n  0.4540297091007232666015625000000000000000\n  -0.5602043271064758300781250000000000000000\n  0.0446298755705356597900390625000000000000\n  -0.0566050000488758087158203125000000000000\n  0.1232855096459388732910156250000000000000\n  -0.0615168288350105285644531250000000000000\n  -0.0763227865099906921386718750000000000000\n  0.0071018282324075698852539062500000000000\n  -0.0226109735667705535888671875000000000000\n  0.1259966939687728881835937500000000000000\n  0.3681966066360473632812500000000000000000\n  0.6030867099761962890625000000000000000000\n  0.0209631212055683135986328125000000000000\n  -0.1290528029203414916992187500000000000000\n  0.1811964809894561767578125000000000000000\n  -0.1773415803909301757812500000000000000000\n  -0.0271602831780910491943359375000000000000\n  -1.0987161397933959960937500000000000000000\n  -0.1090307384729385375976562500000000000000\n  0.5171378850936889648437500000000000000000\n  0.7389591932296752929687500000000000000000\n  0.1277110874652862548828125000000000000000\n  -0.0941361337900161743164062500000000000000\n  -0.2473500669002532958984375000000000000000\n  0.0767389833927154541015625000000000000000\n  -0.0777835994958877563476562500000000000000\n  0.3647090792655944824218750000000000000000\n  0.1631785631179809570312500000000000000000\n  0.1779769212007522583007812500000000000000\n  0.3896390199661254882812500000000000000000\n  -0.0452492088079452514648437500000000000000\n  0.0389573723077774047851562500000000000000\n  0.0529729351401329040527343750000000000000\n  -0.0044109327718615531921386718750000000000\n  -0.1333716213703155517578125000000000000000\n  -0.1468580365180969238281250000000000000000\n  -0.0733139663934707641601562500000000000000\n  -0.1348508298397064208984375000000000000000\n  0.2573953270912170410156250000000000000000\n  -0.2511427104473114013671875000000000000000\n  -0.2291310727596282958984375000000000000000\n  -0.1569539904594421386718750000000000000000\n  0.5671482682228088378906250000000000000000\n  0.0007562935352325439453125000000000000000\n  -0.2506576180458068847656250000000000000000\n  0.3950751721858978271484375000000000000000\n  -0.0978212431073188781738281250000000000000\n  0.0085519552230834960937500000000000000000\n  0.1242928802967071533203125000000000000000\n  -0.2797504961490631103515625000000000000000\n  -0.0056432634592056274414062500000000000000\n  0.1585565805435180664062500000000000000000\n  -0.2883628904819488525390625000000000000000\n  0.2814774513244628906250000000000000000000\n  -0.3467355072498321533203125000000000000000\n  0.0624505244195461273193359375000000000000\n  0.3562702536582946777343750000000000000000\n  -0.1756436973810195922851562500000000000000\n  0.3732989132404327392578125000000000000000\n  -0.0796856284141540527343750000000000000000\n  0.2660370469093322753906250000000000000000\n  0.1200120374560356140136718750000000000000\n  0.2349552214145660400390625000000000000000\n  0.4175272583961486816406250000000000000000\n  -0.2411464601755142211914062500000000000000\n  0.5075171589851379394531250000000000000000\n  -0.3852846324443817138671875000000000000000\n  -0.0465667918324470520019531250000000000000\n  -0.0087259896099567413330078125000000000000\n  -0.0927259922027587890625000000000000000000\n  0.0883216783404350280761718750000000000000\n  0.1761537194252014160156250000000000000000\n  0.0301051400601863861083984375000000000000\n  -0.2141435444355010986328125000000000000000\n  -0.0877583026885986328125000000000000000000\n  -0.1394038498401641845703125000000000000000\n  -0.1227853521704673767089843750000000000000\n  -0.5036234855651855468750000000000000000000\n  0.3066844940185546875000000000000000000000\n  -0.0214032977819442749023437500000000000000\n  0.2851337194442749023437500000000000000000\n  0.2487199008464813232421875000000000000000\n  -0.0377925746142864227294921875000000000000\n  -0.3884379565715789794921875000000000000000\n  0.1377667635679244995117187500000000000000\n  -0.0205082967877388000488281250000000000000\n  -0.2412324249744415283203125000000000000000\n  -0.1567543148994445800781250000000000000000\n  -0.0849389433860778808593750000000000000000\n  0.2378081679344177246093750000000000000000\n  -0.1114650294184684753417968750000000000000\n  0.1813269406557083129882812500000000000000\n  -0.3095203936100006103515625000000000000000\n  0.2008163779973983764648437500000000000000\n  0.4106325209140777587890625000000000000000\n  -0.1181976497173309326171875000000000000000\n  0.5294705033302307128906250000000000000000\n  0.1461991071701049804687500000000000000000\n  0.0026929471641778945922851562500000000000\n  0.3589752018451690673828125000000000000000\n  -0.5336542725563049316406250000000000000000\n  0.3860982358455657958984375000000000000000\n  -3.4720132350921630859375000000000000000000\n  -0.1847448050975799560546875000000000000000\n  -0.2506241798400878906250000000000000000000\n  -0.0620379149913787841796875000000000000000\n  -0.4448238909244537353515625000000000000000\n  -0.2363186478614807128906250000000000000000\n  0.2490787059068679809570312500000000000000\n  -0.1106517687439918518066406250000000000000\n  0.0041876211762428283691406250000000000000\n  -0.0912074968218803405761718750000000000000\n  0.2903877794742584228515625000000000000000\n  -0.1365924775600433349609375000000000000000\n  -0.0795269906520843505859375000000000000000\n  0.0802037864923477172851562500000000000000\n  0.7493298649787902832031250000000000000000\n  0.0726670175790786743164062500000000000000]]');
INSERT INTO `ms_file` (`file_id`, `file_name`, `file_content`, `file_url`, `file_content_vector`) VALUES
(4, 'A_hyperbolastic_type-I_diffusion_process__Parameter_estimation_bymeans_of_the_firefly_algorithm', '4202beF5]EM.tats[1v61430.2042:viXraA hyperbolastic type-I diffusion process: parameterestimation by means of the firefly algorithmAntonio Barreraa, Patricia Rom´an-Rom´anb, Francisco Torres-Ruizb,∗aDepartamento de Matem´atica Aplicada, E.T.S.I. Inform´atica, Bulevar Louis Pasteur, 35,Campus de Teatinos, Universidad de M´alaga, 29071 M´alaga, Spain.bDepartamento de Estad´ıstica e Investigaci´on Operativa, Facultad de Ciencias, s/n,Campus de Fuentenueva, Universidad de Granada, 18071 Granada, Spain.AbstractA stochastic diffusion process, whose mean function is a hyperbolastic curve oftype I, is presented. The main characteristics of the process are studied and theproblem of maximum likelihood estimation for the parameters of the processis considered. To this end, the firefly metaheuristic optimization algorithm isapplied after bounding the parametric space by a stagewise procedure. Someexamples based on simulated sample paths and real data illustrate this devel-opment.Keywords: Hyperbolastic curve, Diffusion process, Firefly algorithm.2010 MSC: 00-01, 99-001. IntroductionThe construction of mathematical models to describe growth dynamics hasbeen the subject of several studies in the last decades. The reason is the varietyof situations where these phenomena arise in a natural way. Originally, studiesfocused on population growth, although nowadays they extend to many otherresearch fields. For instance, in recent years, these models have been applied totumor growth and the spread of diseases.One of the main problems associated with the study of growth phenomenais the choice of a suitable model: even though diverse representations by de-terministic models based on sigmoidal curves have been used (Tsoularis andWallace (2002)), the presence of fixed inflection points restricts the adequacy ofthe model to real circumstances.In order to deal with these issues, the hyperbolastic curves of type I, II andIII (H1, H2 and H3, respectively), were developed by Tabatabai et al. (2005),∗Corresponding authorEmail addresses: antonio.barrera@uma.es (Antonio Barrera), proman@ugr.es (PatriciaRom´an-Rom´an), fdeasis@ugr.es (Francisco Torres-Ruiz)Preprint submitted to BioSystems      who introduced hyperbolic functions into known models, thus obtaining mobileinflection points and increasing the capability of the models to fit real data.Recent results have proved the usefulness of these curves in the descriptionand modeling of dynamical phenomena. In this sense, Eby et al. (2010) usedhyperbolastic models to study the growth of the solid Ehrlich carcinoma un-der particular treatments, obtaining a more accurate representation than thoseyielded by other classic curves such as Gompertz or Weibull. Tabatabai et al.(2011) used the H3 model to describe the behavior of embryonic stem cells, im-proving the results of other models such as those in Deasy et al. (2003). Recently,new models also derived from hyperbolastic curves have been introduced, suchas the oscillabolastic model (Tabatabai et al. (2012)) or the T-model (Tabatabaiet al. (2013)).Despite the good results obtained by applying these deterministic models,it is clear that natural dynamical phenomena occur under the influence of un-known (or even immeasurable) factors which can have a significant effect onthe evolution of the process. Therefore, introducing into the models certainelements capable of describing such influences seems necessary and justified. Inthis way, the next step in the mathematical modeling of dynamic phenomenalies in extending deterministic models to stochastic models while taking intoaccount the random influences affecting the dynamics of the phenomenon.To this end, stochastic processes, and in particular diffusion processes, ap-pear as the most appropriate tool. In the context of growth curves, these pro-cesses arise when a random factor is introduced into the differential equationwhose solution is the aforementioned curve, thus becoming a stochastic differ-ential equation whose solution is the final process.This methodology, used by Capocelli and Ricciardi (1974) for obtaining adiffusion process associated with the Gompertz curve, has been applied to awide range of curves. A modification of such model has been recently appliedto tumor growth by Albano et al. (2011, 2013). Diffusion processes based onthe Bertalanffy curve have been applied to the study of population growth inanimals and plants (Rom´an-Rom´an et al. (2010); Rom´an-Rom´an and Torres-Ruiz (2014); Russo et al. (2009)). With respect to the logistic curve, for whichthe first model was proposed by Feller (1940), Kolmogorov equations have noexplicit solution, so the transition probability density function is not known inclosed-form. Therefore, many variants have been proposed, for which a sum-mary can be consulted in Tuckwell and Koziol (1987). Schurz (2007) considereda more general version of the stochastic differential equation associated withlogistic growth and Barrera-Garc´ıa et al. (2013) proposed a Gaussian logisticdiffusion process, whereas Rom´an-Rom´an et al. (2012) established a diffusionprocess based on a reformulation of the logistic curve. Recently, Rom´an-Rom´anand Torres-Ruiz (2015) generalized this last diffusion process to the case of theRichards curve.In this paper we introduce a diffusion process whose mean function is a re-formulation of the H1 curve. This allows us to express the asymptotic behavioraccording to the initial values. Thanks to this, we can consider situations wheredata are available from many individuals, each exhibiting the same growth pat-2tern but with different bounds for the initial value.In this way, we construct a model such that its mean function is an H1 curve,making it suitable for prediction purposes. To this end, data must be used toobtain estimates for the parameters of the process, an estimation that is carriedout by maximum likelihood. This is not problematic as far the parameters ofthe initial distribution are concerned, but the estimation of the rest of param-eters yields a system of equations whose solution is not guaranteed by classicalnumerical procedures. An alternative is the use of metaheuristic optimizationprocedures, to which purpose we propose using the firefly algorithm (FA). In-troduced by Yang (2008), this population-based, nature-inspired metaheuristicalgorithm is undergoing an important development in fields such as engineeringand optimization (e.g. the works of Alb et al. (2016) and Kavousi-Fard et al.(2014) for its efficiency and capacity to deal with NP-hard problems). Someinteresting modifications have also been made (Gandomi et al. (2013); Zhanget al. (2016)). The low computational cost of the algorithm makes it especiallyuseful to address maximum likelihood estimation in diffusion processes.This paper is structured as follows: Section 2 presents a reformulation ofthe H1 curve, including some relevant properties. The scope of this work isrestricted to the case of increasing curves showing at least one inflection point.The hyperbolastic type-I diffusion process (from now on referred to as H1 diffu-sion process) is introduced as a particular case of the lognormal diffusion processwith exogenous factors. Section 3 deals with the estimation of the parametersof the model by means of the maximum likelihood procedure, showing the com-plexity of the system of equations. This question leads to our proposal of usingFA as a valid tool for maximizing the likelihood function. A summary of thealgorithm and its properties is then shown, as well as the modifications requiredin our context. In this sense, a procedure for bounding the parametric space isproposed. Finally, in Section 4, the methodology is applied to two examples:the first over simulated data, and the second over real data from a study abouta molecular biology technique called quantitative polymerase chain reaction.2. The model2.1. Reformulation of the H1 curveThe H1 curve is the solution of the Bernoulli differential equationdx(t)dt= M −1x(t)(cid:16)M − x(t)(cid:17) (cid:18)ρ +√(cid:19)θ1 + t2, t ≥ t0, θ ∈ R,(1)with initial value x(t0) = x0 > 0. Here, x(t) represents the size of a populationat time instant t and M denotes the maximum sustainable population (carryingcapacity), whereas parameters θ and ρ jointly determine growth rate. Note thatwhen θ = 0 this equation is the well-known logistic differential equation.The solution to (1) isx(t) = M (1 + a exp (−ρt − θ arcsinh(t)))−1 ,(2)3wherea = x−10 (M − x0) exp (ρt0 + θ arcsinh(t0)) .Note that limt→∞ x(t) = M if ρ > 0, and limt→∞ x(t) = 0 if ρ < 0. So, ifρ > 0, the curve increases to the asymptote M , whereas ρ < 0 leads to decayprofiles.In the above remark we have shown that the asymptote M is independentfrom the initial value, something that can become an important restriction forsome applications. In practice, there are situations in which the growth phe-nomenon under study shows an H1-type sigmoidal growth and several samplepaths are available, each with the same growth pattern but with different initialvalues and upper bounds (as the weight of individuals in a population). Forthese reasons, in order to model the situations in which the limit value dependson the initial one, we consider a reformulation of curve (2). Setting η = 1/a,λ = e−ρ, and µ = e−θ we obtainx(t) = x0η + ξ(t0)η + ξ(t), t ≥ t0, η, µ > 0,(3)where ξ(t) = λtµarcsinh (t). In the following we will deal with increasing curves.This question leads to conditions 0 < λ < 1 and µ < cλ(t), ∀t ≥ t0, where1+t2 is an increasing function for all t. Therefore, in the followingcλ(t) = λ−we will consider µ < cλ(t0).√Another important characteristic of the H1 curve is the mobility of its in-flections, which gives it a flexibility superior to that of other commonly usedcurves, allowing for more precise modeling of phenomena showing a sigmoidalbehavior. From x′′(t) = 0 it follows that inflections of the curve verify thefollowing equation, which corresponds, after the reparametrization made in thecurve, with that obtained by Tabatabai et al. (2005):2ηη + ξ(t)− 1 =t log µ(1 + t2)3/2(cid:18)log λ +√log µ1 + t2(cid:19)−2.2.2. The H1 diffusion processIn this section we will introduce a diffusion process associated with the curve(3). To this end, we will proceed with the general procedure of obtaining astochastic differential equation from an ordinary differential equation associatedwith the curve. We will then verify that the mean function of the resultingprocess is an H1 curve, which is useful for the purpose of making predictions insituations modeled by this type of growth.From (3) we can confirm that the curve verifies the differential equationwheredx(t)dt= h(t)x(t),x(t0) = x0,h(t) = −dξ(t)/dtη + ξ(t),4which is a generalization of the Malthusian growth model with a time-dependentfertility rate h(t).Replacing this fertility rate with h(t) + σ W (t), where W (t) denotes thestandard Wiener process and σ > 0, we obtain the Langevin equationdX(t)dt= h(t)X(t) + σ X(t) W (t),which, rewritten as a stochastic differential equation leads todX(t) = h(t)X(t)dt + σX(t)dW (t).(4)Taking into account that h is a continuous and bounded function, equation(4) verifies the conditions for the existence and uniqueness of solution (see Ok-sendal (2003)). The solution is a stochastic diffusion process taking values on R+and characterized by infinitesimal moments A1(x, t) = h(t)x and A2(x) = σ2x2.In addition, a closed-form expression for the solution can be provided. In fact,by considering the initial condition X(t0) = X0, independent from W (t) fort ≥ t0, we haveX(t) = X0η + ξ(t0)η + ξ(t)(cid:18)exp−σ22(t − t0) + σ(W (t) − W (t0)).(5)(cid:19)This process is a particular case of the lognormal diffusion process with ex-ogenous factors. Several applied works have been developed around this process.For instance, Guti´errez et al. (1999, 2003), performed an inferential analysis andassessed its usefulness for studies in Economics, including the consideration offirst-passage-time problems, a topic already considered in Guti´errez et al. (1995).As regards the distribution of the process, if X0 is distributed according to(cid:3) or X0 is a degenerate variable (P [X0 =a lognormal distribution Λ1x0] = 1), all the finite-dimensional distributions of the process are lognormal.Concretely, ∀n ∈ N and t1 < . . . < tn, vector (X(t1), . . . , X(tn))T has a n-dimensional lognormal distribution Λn[δ, Σ], where the components of vector δand matrix Σ are(cid:2)µ0; σ20δi = µ0 + log(cid:19)(cid:18) η + ξ(t0)η + ξ(t)−σ22(t − t0),i = 1, . . . , nandσij = σ20 + σ2(min(ti, tj) − t0),i, j = 1, . . . , nrespectively. The transition probability density function can be obtained fromthe distribution of (X(s), X(t))T , beingf (x, t|, y, s) =1x(cid:112)2πσ2(t − s)exp−(cid:16)5log xy − log η+ξ(s)η+ξ(t) + σ22σ2(t − s)2 (t − s)(cid:17)2that is, X(t)|X(s) = y follows a lognormal distributionX(t)|X(s) = y ⇝ Λ1(cid:18)log y + logη + ξ(s)η + ξ(t)−σ22(t − s); σ2(t − s)(cid:19).From the previous distributions the main characteristics of the process canbe found. If we noteGn(t|z, τ ) = zn(cid:19)n(cid:18) η + ξ(τ )η + ξ(t)exp(cid:18) n(n − 1)σ22(cid:19)(t − τ ), n ≥ 0,thenE [X(t)n] = Gn (t|E[X(t0)], t0)and E [X(t)n|X(τ ) = y] = Gn (t|y, τ ) .In particular, the mean function of the process and the one conditioned toan initial value x0 areandE [X(t)] = E[X(t0)]η + ξ(t0)η + ξ(t)E [X(t)|X(t0) = x0] = x0η + ξ(t0)η + ξ(t)respectively, which are H1 curves of the type (3) described previously. Thisjustifies considering the stochastic model herein proposed.3. Estimation of the modelLet us consider a discrete sampling of the process, based on d sample paths,for times tij, (i = 1, . . . , d, j = 1, . . . , ni) with ti1 = t1, i = 1, . . . , d. That is, weobserve variables X(tij), the values of which: x = {xij}i=1,...,d;j=1,...,ni, makeup the sample for the inferential study.By considering the most general case in which the initial distribution islognormal, X(t1) ∼ Λ(µ1, σ21), the log-likelihood function of the sample islog Lx(µ1, σ21, η, λ, µ, σ2) =d(cid:88)i=1log f1(xi1) +d(cid:88)ni(cid:88)i=1j=2log f (xij, tij|xi,j−1, ti,j−1)where f1 is the density function of X(t1).6Denoting N =d(cid:88)i=1ni, we havelog Lx(µ1, σ21, η, λ, µ, σ2) = −N2log(2π) −d2log σ21 −N − d2log σ2 −d(cid:88)i=1log xi1−−12σ2112σ2d(cid:88)i=1[log xi1 − µ1]2 −d(cid:88)ni(cid:88)i=1j=2log xij −12d(cid:88)ni(cid:88)i=1j=2log ∆ijd(cid:88)ni(cid:88)i=1j=2(cid:18)1∆ijRij − T η,λ,µij+(cid:19)2σ22∆ij(6)whereT η,λ,µij= log(cid:18) η + ξ(ti,j−1)η + ξ(tij)(cid:19), Rij = logxijxi,j−1and ∆ij = tij − ti,j−1.From (6) the ML estimates of µ1 and σ21 are(cid:98)µ1 =1dd(cid:88)i=1log xi1 and (cid:98)σ21 =1dd(cid:88)i=1(log xi1 − (cid:98)µ1)2.However, estimating the rest of the parameters poses some difficulties. Con-cretely, the resulting system of equations (see Appendix) is exceedingly complexand does not have an explicit solution. Therefore, numerical procedures mustbe employed to find its approximate solution. Since the likelihood system ofequations depends on sample data, it is impossible to carry out a general studyabout it. For instance, the conditions of convergence of the most widely usednumerical methods are impossible to verify. Such methods (as Newton-Raphsonand its variants) require calculating and inverting the Jacobian and Hessian ma-trices of the vectorial function that determines the system of equations. Thesample data may then lead to singular matrices and, consequently, to a failureof the numerical procedure. Rom´an et al (2012) contains an example of this(for a Gompertz-type diffusion process). Adding to this problem, we must alsoselect an initial solution.For these reasons, we propose the use of the local search metaheuristic fireflyalgorithm in order to maximize the likelihood function. The study and develop-ment of metaheuristic optimization algorithms have grown considerably in thelast years, with applications having been developed to several fields, includingestimations in diffusion processes. For example, Rom´an-Rom´an et al. (2012)used simulated annealing for estimating the parameters of a Gompertz-type dif-fusion process, whereas Rom´an-Rom´an and Torres-Ruiz (2015) suggested thevariable neighborhood search method in combination with simulated annealingin the context of the Richards diffusion process.Once we have found the estimates of µ1 and σ21, the problem becomes max-1, η, λ, µ, σ2). So, from (6), the target function weimizing function log Lx((cid:98)µ1, (cid:98)σ27will consider isfo(λ, µ, η, σ2) = −d(N − 1)2log σ2−12σ2d(cid:88)ni(cid:88)i=1j=2(cid:18)1∆ijRij − T η,λ,µij+(cid:19)2∆ij.σ223.1. Application of the firefly algorithmFA is a stochastic, population-based, and nature-inspired metaheuristic al-gorithm developed by Yang (2008). It has been successfully applied in manyfields such as optimization and engineering (see Kavousi-Fard et al. (2014);Niknam et al. (2012)) because of its efficiency and ability to deal with NP-hardproblems, becoming one of the most important tools of swarm intelligence, aresearch subfield of artificial intelligence focused on the collective behavior ofIn the last years many works havedecentralized and self-organized systems.explored in detail the theory of the firefly algorithm and its applications. See,for instance, Fister et al. (2013) and Yang and He (2013), as well as Fister et al.(2014) and Gandomi et al. (2013) where some modifications of the algorithmhave been introduced.The algorithm works over population subgroups, and thus it can deal effi-ciently with nonlinear and multimodal optimization problems. Indeed, FA canbe viewed as a generalization of well-known algorithms such as particle swarmoptimization (PSO), differential evolution, and simulated annealing (SA).It is inspired by the flashing lights of fireflies, produced by a bioluminescencephenomenon and used for attraction purposes between them. Thus, in basicterms, n fireflies are randomly distributed over the search space, with each ofthem having an associated light intensity that is dependent on its position.Then, at every generation, fireflies are attracted (attractiveness is proportionalto light intensity) by the brighter neighbors according to the distance betweenthem; that is, if a firefly flashes with high intensity, the closer ones will detecthigh attractiveness and fly towards it.With the movement of fireflies, the attraction between them is updated tothe new distances and light intensities, and the generation finishes by rankingthe fireflies from the dimmest to the brightest one, which correspond to thosethat provide a lowest and highest value of the objective function, respectively.This procedure is repeated for a fixed number of generations, with each onereducing the randomness of fireflies’ movements.Looking at the matter in more detail, it is obvious that light intensity, andtherefore attractiveness, varies according to the distance between fireflies andto the medium and its light-absorbing properties. The firefly algorithm usesthis characteristics in order to search for the local and global maxima of a givenfunction, defining the search space as the domain of the objective function andthe light intensity as proportional to the value of the function at every pointoccupied by a firefly. Therefore, measures of the attraction between fireflies mustdecrease when the distance and/or the absorption coefficient increase. Usually,this is formulated asβ(r) = β0 exp (cid:0)−γr2(cid:1) ,8where β0 is the attractiveness at r = 0 and γ is the absorption coefficient of themedium.Thus, if firefly i detects the greater light intensity of firefly j at distance rij,it results on a movement of i towards j according tok → xixik + β(rij)(cid:16)xjk − xik(cid:17)+ αϵk,(7)k and xjwhere xik are the k-th component of the position of the i-th and j-thfireflies, respectively, ϵk is a random value of the form u − 1/2 for u from anuniform distribution (it can be substituted by a Gaussian or other distribution),and α is the randomization parameter, which controls the stochastic influenceon the movement and is reduced recursively at every generation by a factorδ ∈ (0, 1) on form α → δα.It is noteworthy that FA is essentially managed by three parameters: ran-domization parameter (α), attractiveness (β0), and light absorption coefficient(γ). This can be displayed as an asymptotic behavior: for instance, if γ tendsto 0, the resulting constant attractiveness β = β0 leads to a special case of PSO,and when γ tends to ∞, FA becomes a random walk, parallel version of SA.Algorithm 1 Pseudocode of the firefly algorithmDefine objective function f (x) where x = (x1, . . . , xd)TAssign values for γ, β0, α, δ and M axGenerations (maximum number ofgenerations)Generate initial population of fireflies xi for i = 1, 2, . . . , nDetermine light intensity Ii at xi via f (xi)while t < M axGeneration dofor i = 1 : n all n fireflies dofor j = 1 : i all n fireflies doif Ii < Ij thenMove firefly i towards j according (7)end ifVary attractiveness with distance r via exp (cid:0)−γr2(cid:1)Update light intensity by evaluating new solutionsend forend forRank the fireflies (from lowest to highest value of the objective function)and find the current best (i.e., the last one).Update α by applying the reduction factor δend whilePostprocess results and visualization3.1.1. Initial parameters of the algorithm and bounding of the parametric spaceof the processAs regards the initial parameters of the algorithm, and following the com-ments of Yang (2008), the next considerations can be made:9• α: The randomization parameter controls randomness in the movement ofthe fireflies. It usually takes values between 0 and 1, with a value around0.2 being recommended.• δ: This parameter reduces α at every generation. Usually this value ischosen to be between 0.95 and 0.99.• β0: Since the attractiveness at distance r = 0 is usually considered to be1, β0 = 1 is taken in most cases.• γ: Absorption coefficient is a critical parameter that strongly influencesthe velocity of convergence. It simulates the environment conditions inwhich fireflies can detect light. In most cases, suitable values range from1 to 10.• n: The number of fireflies may vary in a wide range depending, for exam-ple, on the number of local maxima. Of course, having a high n ensures abetter coverage of the parametric space.• Generations: Usually a value between 50 and 100 is considered, althougha value below 50 can also provide good results.In any case, the high degree of efficiency of FA allows us to test it withdifferent initial values without incurring excessive computational costs.To determine the initial population of fireflies, n points of the parametricspace of the process must be randomly selected. To facilitate this choice, wepropose bounding such space. To this end, we will use the curve reformulationmade in section 2.1 and the information provided by the sample paths:• λ: It is between 0 and 1, in order to guarantee strictly increasing curves.• µ: It holds 0 < µ < λ−√1+t20 for a previously established λ.• η: This parameter can be bounded taking into account the asymptote ofthe curve verifying k = x0 (1 + ξ(t0)/η)). From that expression, and if wedenote by ki the maximum value of the i-th sample path, the followingexpression holds(cid:91)ξ(t0)(cid:20)maxi=1,...,d(cid:18) kixi0(cid:19)(cid:21)−1− 1< ˆη < (cid:91)ξ(t0)(cid:20)mini=1,...,d(cid:18) kixi0(cid:19)(cid:21)−1− 1,(8)where xi0 is the initial value of the i-th sample path. In practice, whent0 ̸= 0, the algorithm must choose values for λ and µ to finally constructan interval for η. Otherwise, in the case t0 = 0, the interval does notdepend on λ and µ.• σ: Regarding parameter σ, when it has high values it leads to sample pathswith great variability around the mean of the process. Thus, excessivevariability in available paths would make an H1-type modeling inadvisable.10Some simulations performed for several values of σ have led us to considerthat 0 < σ < 0.5, so that we may have paths compatible with an H1-typegrowth.4. Applications4.1. Application to simulated dataWe will now proceed to introduce some simulation studies. These will beaimed at validating the procedures described above as they apply to estimatingthe parameters of the process.The general pattern of simulations is based on generating 30 sample paths ofthe H1 diffusion process in interval [0, 50] from (5). All sample paths have thesame length, and ti = (i − 1) · 0.1, i = 1, . . . , 501 are the time instants at whichobservations are made. Each example has been replicated 50 times, that is, foreach combination of the parameters of the model that we have considered, thesimulation of the trajectories and the estimation procedure has been carried out50 times. The final results are the average of those obtained in each replication.A first simulation example has been performed by taking a degenerate initialstate at x0 = 0.1 and parameters η = 0.5, λ = 0.8, µ = 0.8 and σ = 0.015 forthe process. Figure 1 shows the simulated paths.Figure 1: Some simulated sample paths of the H1-type diffusion process.We have considered n = 40 fireflies, that is 40 4-uples of the parametricspace, and selected each one as follows: firstly, for λ, we choose a random value(uniformly distributed) between 0 and 1 and then, for µ, we take a random valuein the interval (0, 1/λ) since t0 = 0 in this case. The 4-uple is completed byselecting a random value for η in the interval (0.3562706, 0.5612756) (by virtueof (8)) and for σ in the interval (0, 0.5).As regards the parameters for FA, we have made the following choices:• 80 generations.11010203040500.100.150.200.250.300.35• α = 0.2.• δ = 0.97.• β0 = 1.• γ = 1.λ0.8Real valueEstimated value0.7873909Abs. relative error 0.0157614µ0.80.81845370.0230670η0.50.50015650.0003129σ0.0150.01495640.0029034fo55554.1755554.580.0000073Table 1: Results of the firefly algorithm over simulated data.Table 1 contains the real and estimated values of the parameters, as wellas the value of the objective function f0 at the resulting point. As a measureof the estimation error we have also included the absolute relative error, i.e.,the difference in absolute value between real and estimated value, divided bythe real one. From here we can observe very good results in terms of estimatedvalues and absolute relative errors.The behavior of the algorithm over generations is shown with more detail inFigures 2 and 3. For each parameter of the process, that is, for each coordinateof the fireflies, each line of Figure 2 represents a generation (from lighter grayto dark black) showing the estimated values of the parameter for each firefly.Figure 3 shows, for each parameter of the process, the evolution of its estimatedvalue for each firefly over the last 60 generations. Green and red lines representthe evolution of the estimation provided by the best and the worst firefly ineach generation, respectively. Remember that the best firefly is the one thatprovides the highest value of the objective function, whereas the worst producesthe smaller one.Since the parametric space is 4-dimensional, it is impossible to visualize theevolution of the fireflies. For this reason, in Figure 4 we have included graphs ofthe dynamics of the best firefly in each generation, for each pair of parameters.Figure 5 shows the evolution of the objective function f0 for the best fireflyin each generation. Obviously this graph is increasing since the best fireflyof each generation improves the value of the objective function of the best ofthe previous generation. Finally, Figure 6 shows the real and estimated meanfunction of the H1 diffusion process.12Figure 2: First simulation example: evolution (from light gray to black) of 80 generations of40 fireflies for η = 0.5, λ = 0.8, µ = 0.8 and σ = 0.015 (left to right, top to bottom). Originalvalues in red.Figure 3: First simulation example: evolution of fireflies over the last 60 generations for everyparameter (in the same order as in the previous figure). In green, evolution of best firefly; inred, evolution of worst one.131020304010.50.4260.50931020304010.80.24540.78961020304010.80.786814.25381020304010.0150.0010.45340.470.480.490.500.51204060800.7650.7700.7750.7800.7850.790204060800.820.830.840.85204060800.000.010.020.030.040.0520406080Figure 4: First simulation example: for each pair of parameters, bidimensional path projectionrepresenting the evolution of the last firefly (the best) over successive generations (from lightto dark red point).Figure 5: First simulation example: value of objective function at the best firefly. Comparisonbetween observed and fitted mean (red).140.450.460.470.480.490.500.600.650.700.75etalambda0.450.460.470.480.490.500.81.01.21.4etamu0.450.460.470.480.490.500.0160.0180.0200.0220.0240.026etasigma0.600.650.700.750.81.01.21.4lambdamu0.600.650.700.750.0160.0180.0200.0220.0240.026lambdasigma0.81.01.21.40.0160.0180.0200.0220.0240.026musigma02040608050749.4555554.58Figure 6: First simulation example: comparison between observed and fitted mean (red).In order to observe the behavior of the algorithm for different values of theparameters, we have simulated a new dataset taking, η = 0.0003, λ = 0.6, µ =0.8, σ = 0.025 and x0 = 0.000125. Over this data let us apply FA, varyingparameters α, γ, δ and the number of fireflies n, and considering 60 generations.The efficiency of the algorithm allows this type of studies to be carried outwithout incurring in an excessive computational cost. For example, for thissimulation study, with 40 fireflies and 60 generations, the computation timehas been 6 seconds for each replication, that is, 5 minutes for the total 50replications.Next, we analyze the behavior of absolute errors related to each pair ofparameters considered when applying FA. These errors have been calculated byaveraging over all other parameters.Table 2 shows such errors by varying α and γ.It can be seen that, ingeneral, the error increases when γ grows, regardless of the α value, with themost appropriate γ values being those between 1 and 5. Conversely, for eachγ, biggest errors are associated with extreme values of α, being 0.2 and 0.4 thevalues for which the observed errors are smaller.51α0.1 0.0199 0.03110.0089 0.00950.20.0073 0.00480.40.0125 0.01150.60.0112 0.01600.80.0126 0.02090.9γ100.02950.01230.01110.01420.02740.0306200.05020.02200.02700.02980.04790.0459350.04540.03950.03940.04950.06420.0548Table 2: Second simulation example: absolute relative error of ˆfo for α vs. γ.15010203040500.100.150.200.250.30Table 3 compares absolute relative errors as a function of α and δ. These twoparameters are related since they mark the level of randomness with which thefireflies move in the space. A priori, it seems logical to think that when setting asmall α value (low randomness), it should decrease slowly in later stages, whichis associated with higher δ values. The situation should be reversed as a largerα initial value is selected, in which case a smaller δ value should be selected.This intuition is confirmed after observing this table. Specifically, the optimalcombination is given by α = 0.2 (as suggested by Yang (2008)) and δ = 0.99.0.9α0.10.03310.2 0.01670.4 0.01150.6 0.01090.8 0.01350.9 0.0099δ0.950.00900.00750.01140.01510.03370.03510.970.00520.00470.01140.01660.04260.04610.990.00790.00430.02540.04420.07480.0767Table 3: Second simulation example: absolute relative error of ˆfo for α vs. δ.Table 4 considers δ and γ parameters. It can be seen how, in general, forfixed γ errors grow as δ increases. Regardless of the value of δ, the smallesterrors are associated with γ values between 1 and 5.δ0.9γ0.0276150.018610 0.024320 0.033135 0.05160.950.02290.01610.02380.03640.05740.970.02030.02520.03430.04430.04940.990.02280.04350.04890.06140.0635Table 4: Second simulation example: absolute relative error of ˆfo for γ vs. δ.Tables 5, 6 and 7 consider the combination of the number of fireflies, n, withthe others parameters. In all cases, it is observed how the error always decreases(as seems logical) with n, regardless of the other parameter considered.On the other hand, regardless of n, the smallest errors are associated withthe central values of α, specifically for α = 0.2, 0.4, as can be observed in Table5. For fixed n, Table 6 shows that the preferable γ values are those between 1and 10, with γ = 1 being the most recommendable. Finally, Table 7 does notshow large differences in error when δ varies.4.2. Application to real dataIn order to show a practical application of the process herein, we consider areal case in the context of the qPCR (Quantitative Polymerase Chain Reaction)16510α0.1 0.1484 0.03870.0787 0.01670.20.0687 0.01500.40.0788 0.02480.60.0936 0.03310.80.0795 0.03800.9n200.00700.00310.00740.00980.01870.0202400.00250.00140.00210.00540.01160.0126600.00130.00130.00180.00570.00940.0128Table 5: Second simulation example: absolute relative error of ˆfo for α vs. n.510γ0.0820 0.0365150.0831 0.036310 0.0955 0.039920 0.1584 0.054535 0.1672 0.0747n200.01950.02340.01910.02140.0334400.00750.01120.01730.01480.0185600.00590.01030.01260.01440.0113Table 6: Second simulation example: absolute relative error of ˆfo for γ vs. n.5δ0.90.09110.95 0.08430.97 0.07840.99 0.1097100.03620.03860.03810.0568n200.01700.01870.02590.0318400.00710.00800.01620.0217600.00380.00710.01480.0201Table 7: Second simulation example: absolute relative error of ˆfo for δ vs. n.17technique. This is a method used in molecular biology to amplify DNA or RNApieces (nucleic acids) by taking advantage of the polymerase enzyme.Different qPCR techniques allow the simultaneous amplification and quan-tification of the obtained product. In particular, kinetic PCR can quantify theproduct from the cycle at which certain threshold of amplicon DNA is reached(the exponential phase).In order to calculate this cycle, this technique (aswell as other methods) employ fluorescence monitoring, and consider the firstcycle that exceeds a certain level of fluorescence. Rutledge and Cote (2003)established the criteria for the mathematical development of kPCR technique,focusing on the threshold of log-fluorescence. Absolute quantification is achievedusing a standard curve constructed by amplification of known amounts of targetDNA and plotting the values obtained for the threshold cycle, Ct, against targetDNA concentration.As can be seen, the determination of the Ct cycle is vital for this technique.Since we are dealing with a dynamic phenomenon, we consider fitting an H1-typediffusion process in order to model, at each instant of time (cycle in this case),the level of fluorescence. Please note that when considering a stochastic process,at each time instant we have a random variable that models the behavior ofthe variable under study, which allows us to take into account the randomfluctuations that exist in this type of phenomena. In addition, it is possible tointroduce the use of techniques associated with the study of temporal variablessuch as first-passage times, that is, the time at which the process verifies, forthe first time, a certain property.The data (available as supplementary material in the paper mentioned above)corresponds to fluorescence emitted for several replicate amplifications (con-cretely 20) in a given concentration of DNA (as a matter of fact, in the paperthe authors considered six magnitudes of target concentration, the first of whichis being considered in our study) over 45 cycles (Figure 7). We have to notethat, given the characteristics of the process introduced, the values consideredin our study are those of fluorescence, and not its logarithm.18Figure 7: Real application: Fluorescence versus cycleIn order to apply FA to estimating the parameters of the process, we haveconsidered, for the initial parameters of the algorithm, a combination of valuessuggested from the simulation study previously performed. Concretely, α =0.2, 0.4, γ = 1, 5, δ = 0.90.95, 0.97, 0.99. In addition, we have considered n = 20fireflies over 110 generations. The initial time instant for the observed values ist1 = 0, whereas for the initial value we have considered the mean of the initialvalues of the sample paths, being x1 = 0.000125.The results are summarized in Table 8. For each combination of the initialvalues of the algorithm we have calculated the absolute relative errors betweenthe observed data of the fluorescence and the corresponding estimate from themean function of the estimated process, once the parameters of the process havebeen estimated. From this table we conclude that the optimal combination ofthe initial parameters for FA are α = 0.2, γ = 1 and δ = 0.99.190102030400.00.10.20.30.40.5CycleFluorescenceα0.2γ150.415δ0.90.950.970.990.90.950.970.990.90.950.970.990.90.950.970.99ˆη0.000310.000290.000300.000300.000300.000290.000300.000320.000320.000290.000340.000320.000280.000290.000280.00031ˆλ0.468690.468470.462740.445800.449960.460620.475820.431980.469430.442800.442290.451370.484610.427970.465800.45165ˆµ1.896611.885741.947782.279412.052631.935211.803972.457401.889932.269972.334652.211211.664312.536491.864362.17721ˆσ0.024990.025000.025000.025000.025000.025000.025000.024990.025000.024990.024990.025000.024890.025000.025000.02500Error0.225490.220920.232960.174870.265640.265070.227220.206230.233090.220140.220910.192020.295210.189410.298210.19100Table 8: Estimated parameter values and absolute relative errors from the application of FAto real data.From these values, we have estimated the model again by increasing thenumber of fireflies, following the conclusions obtained in the second simulationstudy. Specifically, we have considered 60 fireflies. Table 9 contains the esti-mates of the parameters of the process.Estimated valueˆλ0.483548ˆµ1.6539ˆη0.0002902ˆσ0.025Table 9: Real application: estimates of the parameters after applying FA.In Figure 8 the stabilization of four estimated parameters (for the last repli-cation) is presented.20Figure 8: Real application: evolution of the estimates of the parameters by applying FA.Figure 9: Real application: mean of original fluorescence (grey) vs. simulated fluorescence(red) vs. fitted theoretical (green).Once the estimates of the parameters have been obtained, we simulatedthe H1-type diffusion process under the conditions of the experiment. Figure9 shows the mean of the observed values together with the theoretical mean210500100015002000−2−1012lambda0500100015002000051015mu0500100015002000−1012eta0500100015002000−1012sigma0102030400.00.10.20.30.4CycleMean Fluorescencefunction and the mean of the simulated sample paths of the estimated process.With this method, we can simulate the fluorescence of the amplificationprocedure in different reactions, which enables us to use simulated instead ofreal data. Even when the simulated process differs from the original in thefirsts cycles, the important question is whether we can find the cycles at whichthe fluorescence threshold is reached (see Figure 10, showing log-fluorescencevalues for the original data and the logarithm of the simulated sample paths ofthe estimated process. The horizontal line represents a fluorescence threshold).This issue can be approached using techniques associated with the study oftemporal variables in the context of diffusion processes, as in the case of first-passage times.Figure 10: Real application:simulated sample paths of the simulated process (red).log-fluorescence for original data (grey) and logarithm of the5. ConclusionsThe hyperbolastic type I diffusion process is obtained, showing to be advan-tageous over the deterministic hyperbolastic type-I curve, of wide use in manyresearch fields. This diffusion process allows us to introduce into the model allthe information coming from data, as well as the random factors which mustbe taken into account in order to explain different growth phenomena. Nev-ertheless, improvements resulting from the use of this curve are dependent onthe number of parameters to estimate, and even though in this work we havereduced this quantity in one element, this complicates the development of theinference procedure. Computationally efficient methods are therefore necessary,among which metaheuristic algorithms, such as the firefly algorithm, are ableto reduce computational cost.In this work we have developed the theoretical base for the practical use ofhyperbolastic type-I diffusion processes as a particular case of the lognormalprocess with exogenous factors, and applying the firefly algorithm in order tosolve inference problems.22010203040−4−3−2−1CycleLog FluorescenceSimulations were performed, which show that the strategy used for bound-ing the parametric space behaves well, as does the firefly algorithm for differentchoices of its parameters. From the simulations we have obtained several combi-nations of the parameters of the algorithm that can be considered optimal. Anexample based on real data from a study about quantitative polymerase chainreaction is also developed, showing the capability of the process for fitting thefluorescence levels associated with the amplification of amplicons of DNA.AcknowledgementsThis work was supported in part by the Ministerio de Econom´ıa y Compet-itividad, Spain, under grant MTM2014-58061-P.6. AppendixFrom the log-likelihood function (6), the estimates of η, λ, µ and σ2 followfrom the solution of the system of equations= 0d(cid:88)ni(cid:88)i=1j=2d(cid:88)ni(cid:88)i=1j=2d(cid:88)ni(cid:88)i=1j=2ijRij − T η,λ,µ∆ijSη,λ,µijRij − T η,λ,µ∆ijSη,λ,µijRij − T η,λ,µ∆ijSη,λ,µijijijW η,λ,µij+σ22d(cid:88)i=1ni(cid:88)j=2V η,λ,µij+Z η,λ,µij+σ22σ22d(cid:88)ni(cid:88)i=1j=2d(cid:88)ni(cid:88)i=1j=2W η,λ,µijSη,λ,µijV η,λ,µijSη,λ,µijZ η,λ,µijSη,λ,µij= 0= 0∆inii1 + 4σ2(N − d) − 4d(cid:88)ni(cid:88)i=1j=2(Rij)2∆ij− 4d(cid:88)ni(cid:88)i=1j=2(cid:17)2(cid:16)T η,λ,µij∆ij+ 8d(cid:88)ni(cid:88)i=1j=2RijT η,λ,µij∆ij= 0σ4d(cid:88)i=1whereSη,λ,µijV η,λ,µijW η,λ,µijZ η,λ,µij= (η + ξ(ti,j−1))(η + ξ(tij))= λ−1 [ti,j−1ξ(ti,j−1)(η + ξ(tij)) − tijξ(tij)(η + ξ(ti,j−1))]= ξ(tij) − ξ(ti,j−1)= µ−1 [arcsinh(ti,j−1)ξ(ti,j−1)(η + ξ(tij)) − arcsinh(tij)ξ(tij)(η + ξ(ti,j−1))]Solving the last equation for σ we obtainσ22= U − 12 Γ12η,λ,µ23whereΓη,λ,µ =d(cid:88)ni(cid:88)i=1j=2(cid:16)1∆ijRij − T η,λ,µij(cid:17)2and U = 4(N − d) +d(cid:88)i=1∆inii1 .This expression depends only on η, λ and µ. Substituting in the other threeequations, the following system of equations appearsΛη,λ,µJ+ U − 12 Γwhere12η,λ,µΨη,λ,µJ= 0, J = W, V, Z,Λη,λ,µJ=d(cid:88)ni(cid:88)i=1j=2Rij − T η,λ,µ∆ijSη,λ,µijijJ η,λ,µijand Ψη,λ,µJ=d(cid:88)ni(cid:88)i=1j=2J η,λ,µijSη,λ,µij, J = W, V, Z.The solution of the above system of equations provides the maximum likeli-hood estimates ˆη, ˆλ and ˆµ, from which ˆσ2 = 2U − 12 Γ12ˆηˆλˆµ.ReferencesReferencesAlb, M., Alotto, P., Magele, C., Renhart, W., Preis, K., Trapp, B., 2016. Fireflyalgorithm for finding optimal shapes of electromagnetic devices. IEEE Trans.Magn. 52, 1–4. doi:10.1109/TMAG.2015.2483058.Albano, G., Giorno, V., Rom´an-Rom´an, P., Torres-Ruiz, F., 2011. Inferring theeffect of therapy on tumors showing stochastic gompertzian growth. J. Theor.Biol. 276, 67–77. doi:10.1016/j.jtbi.2011.01.040.Albano, G., Giorno, V., Rom´an-Rom´an, P., Torres-Ruiz, F., 2013. On the effectof a therapy able to modify both the growth rates in a gompertz stochasticmodel. Math. Biosci. 245, 12–21. doi:10.1016/j.mbs.2013.01.001.Barrera-Garc´ıa, A.J., Rom´an-Rom´an, P., Torres-Ruiz, F., 2013. Fitting dy-namic growth models of biological phenomena from sample observationsthrough gaussian diffusion processes. Biosyst. 112, 284–291. doi:10.1016/j.biosystems.2012.12.007.Capocelli, R., Ricciardi, L., 1974. A diffusion model for population growthdoi:10.1016/in random environment. Theor. Popul. Biol. 5, 28–41.0040-5809(74)90050-1.Deasy, B., Jankowski, R., Payne, T., Cao, B., Goff, J., Greenberger, J., Huard,J., 2003. Modeling stem cell population growth:incorporating terms forproliferative heterogeneity. Stem Cells 21, 536–545. doi:10.1634/stemcells.21-5-536.24Eby, W.M., Tabatabai, M.A., Bursac, Z., 2010. Hyperbolastic modeling of tu-mor growth with a combined treatment of iodoacetate and dimethylsulphox-ide. BMC Cancer 10, 509. doi:10.1186/1471-2407-10-509.Feller, W., 1940. On the logistic law of growth and its empirical verifications inbiology. Acta Biotheor. 5, 51–66. doi:10.1007/BF01602862.Fister, I., Yang, X.S., Brest, J., 2013. A comprehensive review of firefly algo-rithms. Swarm Evol. Comput. 13, 34–46. doi:doi.org/10.1016/j.swevo.2013.06.001.Fister, I., Yang, X.S., Brest, J., Fister Jr, I., 2014. On the randomized fireflyalgorithm, in: Cuckoo Search and Firefly Algorithm. Springer, pp. 27–48.Gandomi, A., Yang, X.S., Talatahari, S., Alavi, A., 2013. Firefly algorithmwith chaos. Comm. Nonlinear Sci. Numer. Simul. 18, 89–98. doi:10.1016/j.cnsns.2012.06.009.Guti´errez, R., Rom´an, P., Romero, D., Torres, F., 2003. Forecasting for theunivariate lognormal diffusion process with exogenous factors. Cybern. Syst.34, 709–724. doi:10.1080/716100279.Guti´errez, R., Rom´an, P., Torres, F., 1995. A note on the Volterra integralequation for the first-passage-time density. J. Appl. Prob. 53, 635–648. doi:10.1017/S0021900200103092.Guti´errez, R., Rom´an, P., Torres, F., 1999. Inference and first-passage-times forthe lognormal diffusion process with exogenous factors: application to mod-elling in economics. Appl. Stoch. Models Bus. Ind. 15, 325–332. doi:10.1002/(SICI)1526-4025(199910/12)15:4<325::AID-ASMB397>3.0.CO;2-F.Kavousi-Fard, A., Samet, H., Marzbani, F., 2014. A new hybrid modified fireflyalgorithm and support vector regression model for accurate short term loadforecasting. Expert Syst. Appl. 41, 6047–6056. doi:10.1016/j.eswa.2014.03.053.Niknam, T., Azizipanah-Abarghooee, R., Roosta, A., 2012. Reserve constraineddynamic economic dispatch: a new fast self-adaptive modified firefly algo-rithm. IEEE Syst. J. 6, 635–646. doi:10.1109/JSYST.2012.2189976.Oksendal, B., 2003. Stochastic differential equations: an introduction withapplications. 6th ed., Springer-Verlag, New York.Rom´an-Rom´an, P., Romero, D., Rubio, M., Torres-Ruiz, F., 2012. Estimatingthe parameters of a Gompertz-type diffusion process by means of simulatedannealing. App. Math. Comput. 218, 5121–5131. doi:10.1016/j.amc.2011.10.077.Rom´an-Rom´an, P., Romero, D., Torres-Ruiz, F., 2010. A diffusion process tomodel generalized von Bertalanffy growth patterns: Fitting to real data. J.Theor. Biol. 263, 59–69. doi:10.1016/j.jtbi.2009.12.009.25Rom´an-Rom´an, P., Torres-Ruiz, F., 2014. Forecasting fruit size and caliber bymeans of diffusion processes. application to “valencia late” oranges. J. Agric.Biol. Envir. S. 19, 292–313. doi:10.1007/s13253-014-0172-3.Rom´an-Rom´an, P., Torres-Ruiz, F., 2015. A stochastic model related to theRichards-type growth curve. estimation by means of simulated annealing andvariable neighborhood search. App. Math. Comput. 266, 579 – 598. doi:10.1016/j.amc.2015.05.096.Russo, T., Baldi, P., Parisi, A., Magnifico, G., Mariani, S., Cataudella, S.,2009. Levy processes and stochastic von Bertalanffy models of growth, withapplication to fish population analysis. J. Theor. Biol. 258, 521–529. doi:10.1016/j.jtbi.2009.01.033.Rutledge, R., Cote, C., 2003. Mathematics of quantitative kinetic pcr and theapplication of standard curves. Nucleic Acids Res. 31, e93. doi:10.1093/nar/gng093.Schurz, H., 2007. Modeling, analysis and discretizations of stochastic logisticequations. Int. J. Numer. Anal. Mod. 4, 178–197.Tabatabai, M., Eby, W., Bursac, Z., 2012. Oscillabolastic model, a new modelfor oscillatory dynamics, applied to the analysis of Hes1 gene expression andEhrlich ascites tumor growth. J. Biomed. Inform. 45, 401–407. doi:10.1016/j.jbi.2011.11.016.Tabatabai, M., Williams, D., Bursac, Z., 2005. Hyperbolastic growth mod-els: theory and application. Theor. Biol. Med. Model. 2, 14. doi:10.1186/1742-4682-2-14.Tabatabai, M.A., Bursac, Z., Eby, W.M., Singh, K.P., 2011. Mathematicalmodeling of stem cell proliferation. Med. Biol. Eng. Comput. 49, 253–262.doi:10.1007/s11517-010-0686-y.Tabatabai, M.A., Eby, W.M., Singh, K.P., Bae, S., 2013. T model of growth andits application in systems of tumor-immune dynamics. Math. Biosci. Eng. 10,925–938. doi:10.3934/mbe.2013.10.925.Tsoularis, A., Wallace, J., 2002. Analysis of logistic growth models. Math.Biosci. 179, 21–55. doi:10.1016/S0025-5564(02)00096-2.Tuckwell, H.C., Koziol, J.A., 1987. Logistic population growth under ran-dom dispersal. B. Math. Biol. 49, 495 – 506. doi:10.1016/S0092-8240(87)80010-1.Yang, X.S., 2008. Nature-Inspired Metaheuristic Algorithms. Luniver Press.Yang, X.S., He, X., 2013. Firefly algorithm: recent advances and applications.Internat. J. Swarm Intell. 1, 36–50. doi:10.1504/IJSI.2013.055801.26Zhang, L., Liu, L., Yang, X.S., Dai, Y., 2016. A novel hybrid firefly algorithmfor global optimization. PLoS ONE 11, 1–17. doi:10.1371/journal.pone.0163230.27', 'pdf/A_hyperbolastic_type-I_diffusion_process__Parameter_estimation_bymeans_of_the_firefly_algorithm.pdf', '[[-0.1612255871295928955078125000000000000000\n  -0.0886243209242820739746093750000000000000\n  0.1074959114193916320800781250000000000000\n  0.0236895326524972915649414062500000000000\n  -0.2972993850708007812500000000000000000000\n  -0.2061421424150466918945312500000000000000\n  0.0888904482126235961914062500000000000000\n  -0.1224339976906776428222656250000000000000\n  0.1630727797746658325195312500000000000000\n  -0.2665418982505798339843750000000000000000\n  -0.1581099033355712890625000000000000000000\n  -0.1719136238098144531250000000000000000000\n  0.0780718624591827392578125000000000000000\n  0.2440482974052429199218750000000000000000\n  -0.0488493889570236206054687500000000000000\n  0.2066210508346557617187500000000000000000\n  0.0077845510095357894897460937500000000000\n  0.3476415276527404785156250000000000000000\n  0.0253052078187465667724609375000000000000\n  0.0384394228458404541015625000000000000000\n  0.1182145848870277404785156250000000000000\n  -0.0731802657246589660644531250000000000000\n  0.5236259698867797851562500000000000000000\n  0.0795333683490753173828125000000000000000\n  -0.2559164166450500488281250000000000000000\n  -0.0117948856204748153686523437500000000000\n  0.0069514773786067962646484375000000000000\n  0.2521011233329772949218750000000000000000\n  0.0947565361857414245605468750000000000000\n  0.2511770129203796386718750000000000000000\n  0.1165489777922630310058593750000000000000\n  -0.0186868868768215179443359375000000000000\n  0.0472799949347972869873046875000000000000\n  -0.2494320273399353027343750000000000000000\n  0.1478020250797271728515625000000000000000\n  -0.0730144530534744262695312500000000000000\n  0.0451142676174640655517578125000000000000\n  -0.1417047977447509765625000000000000000000\n  0.2943670451641082763671875000000000000000\n  -0.2362232208251953125000000000000000000000\n  0.0455055199563503265380859375000000000000\n  0.0610606744885444641113281250000000000000\n  0.0581724531948566436767578125000000000000\n  -0.0554864332079887390136718750000000000000\n  -0.0578273013234138488769531250000000000000\n  -0.0501928329467773437500000000000000000000\n  -3.2330820560455322265625000000000000000000\n  0.2427424788475036621093750000000000000000\n  -0.1419471651315689086914062500000000000000\n  -0.3309541940689086914062500000000000000000\n  0.2668609023094177246093750000000000000000\n  0.1512883156538009643554687500000000000000\n  0.1211005449295043945312500000000000000000\n  0.1297413110733032226562500000000000000000\n  0.0552828907966613769531250000000000000000\n  0.3406477272510528564453125000000000000000\n  -0.0595421791076660156250000000000000000000\n  0.1669198870658874511718750000000000000000\n  0.4093655943870544433593750000000000000000\n  0.0592412799596786499023437500000000000000\n  0.2247483581304550170898437500000000000000\n  0.1821022927761077880859375000000000000000\n  -0.3363526761531829833984375000000000000000\n  0.0992271304130554199218750000000000000000\n  -0.1391220688819885253906250000000000000000\n  0.6023374795913696289062500000000000000000\n  -0.3130668103694915771484375000000000000000\n  0.3712861835956573486328125000000000000000\n  -0.6071367263793945312500000000000000000000\n  0.4542999565601348876953125000000000000000\n  -0.3375037908554077148437500000000000000000\n  -0.1421477496623992919921875000000000000000\n  0.2204603850841522216796875000000000000000\n  0.0167496241629123687744140625000000000000\n  0.1032005548477172851562500000000000000000\n  -0.0087857544422149658203125000000000000000\n  0.1234048455953598022460937500000000000000\n  0.2637997567653656005859375000000000000000\n  0.0079277157783508300781250000000000000000\n  0.3260363340377807617187500000000000000000\n  0.0906576365232467651367187500000000000000\n  -0.0825777649879455566406250000000000000000\n  0.3337194323539733886718750000000000000000\n  -0.3354757726192474365234375000000000000000\n  0.1106875687837600708007812500000000000000\n  0.3422860801219940185546875000000000000000\n  -0.0424212887883186340332031250000000000000\n  -0.1695710867643356323242187500000000000000\n  0.2548843324184417724609375000000000000000\n  0.3629091382026672363281250000000000000000\n  0.0242966115474700927734375000000000000000\n  -0.2095155566930770874023437500000000000000\n  0.3262518942356109619140625000000000000000\n  0.1394374966621398925781250000000000000000\n  0.1078847944736480712890625000000000000000\n  -0.0644474104046821594238281250000000000000\n  -0.1343504786491394042968750000000000000000\n  -0.0615953095257282257080078125000000000000\n  0.0908223986625671386718750000000000000000\n  0.0351107977330684661865234375000000000000\n  0.1321808099746704101562500000000000000000\n  0.1763236969709396362304687500000000000000\n  0.1417381465435028076171875000000000000000\n  -0.7911539673805236816406250000000000000000\n  0.2835164368152618408203125000000000000000\n  -0.2614226043224334716796875000000000000000\n  -0.5665138363838195800781250000000000000000\n  -0.3666145205497741699218750000000000000000\n  0.0202532298862934112548828125000000000000\n  -1.8780077695846557617187500000000000000000\n  0.0748375132679939270019531250000000000000\n  0.5393229722976684570312500000000000000000\n  -0.2369425445795059204101562500000000000000\n  -0.3721524775028228759765625000000000000000\n  0.3983582854270935058593750000000000000000\n  0.0021006483584642410278320312500000000000\n  0.0293898116797208786010742187500000000000\n  0.3167266547679901123046875000000000000000\n  -0.0027848444879055023193359375000000000000\n  -0.1200774163007736206054687500000000000000\n  -0.1088712960481643676757812500000000000000\n  0.1290715783834457397460937500000000000000\n  0.1643775850534439086914062500000000000000\n  -0.2433386445045471191406250000000000000000\n  -0.3317252397537231445312500000000000000000\n  0.2819197773933410644531250000000000000000\n  0.1735888123512268066406250000000000000000\n  -0.6299476027488708496093750000000000000000\n  0.2999357879161834716796875000000000000000\n  0.3722549378871917724609375000000000000000\n  -0.1681490242481231689453125000000000000000\n  0.0696974545717239379882812500000000000000\n  -0.1659006029367446899414062500000000000000\n  0.0464117787778377532958984375000000000000\n  -0.3293678760528564453125000000000000000000\n  -0.0609405525028705596923828125000000000000\n  0.3311168253421783447265625000000000000000\n  0.0597548857331275939941406250000000000000\n  0.1342005580663681030273437500000000000000\n  -0.0433374382555484771728515625000000000000\n  -0.4928839504718780517578125000000000000000\n  0.0538544766604900360107421875000000000000\n  -2.2905039787292480468750000000000000000000\n  0.1220090985298156738281250000000000000000\n  0.5199539065361022949218750000000000000000\n  -0.0423890538513660430908203125000000000000\n  0.0920760557055473327636718750000000000000\n  0.4914633035659790039062500000000000000000\n  -0.1510061621665954589843750000000000000000\n  -0.2103646546602249145507812500000000000000\n  0.1111749783158302307128906250000000000000\n  -0.4348272383213043212890625000000000000000\n  -0.1615608632564544677734375000000000000000\n  -0.0303647927939891815185546875000000000000\n  -0.1930437237024307250976562500000000000000\n  0.0339163132011890411376953125000000000000\n  -0.2051064819097518920898437500000000000000\n  -0.0479183048009872436523437500000000000000\n  0.0978098586201667785644531250000000000000\n  0.0343325398862361907958984375000000000000\n  -0.1559278070926666259765625000000000000000\n  0.0631545931100845336914062500000000000000\n  0.1911847442388534545898437500000000000000\n  -0.1210204660892486572265625000000000000000\n  0.0370721518993377685546875000000000000000\n  -0.1178959608078002929687500000000000000000\n  0.1763707697391510009765625000000000000000\n  0.1204760521650314331054687500000000000000\n  -0.1639112234115600585937500000000000000000\n  -0.0980339497327804565429687500000000000000\n  0.2812804281711578369140625000000000000000\n  -0.1261329054832458496093750000000000000000\n  0.3316839337348937988281250000000000000000\n  -0.2480910271406173706054687500000000000000\n  0.1802016794681549072265625000000000000000\n  0.0287388693541288375854492187500000000000\n  0.1369128376245498657226562500000000000000\n  0.2909400761127471923828125000000000000000\n  0.0980554223060607910156250000000000000000\n  0.0116474907845258712768554687500000000000\n  -0.4376222491264343261718750000000000000000\n  0.3884854912757873535156250000000000000000\n  0.0585498996078968048095703125000000000000\n  -0.2914129197597503662109375000000000000000\n  -0.2958504557609558105468750000000000000000\n  0.2338722944259643554687500000000000000000\n  0.2245005369186401367187500000000000000000\n  -0.0087762568145990371704101562500000000000\n  0.1004113256931304931640625000000000000000\n  0.0942689403891563415527343750000000000000\n  -0.3695594370365142822265625000000000000000\n  -0.1522558629512786865234375000000000000000\n  0.1649028807878494262695312500000000000000\n  0.0767482221126556396484375000000000000000\n  0.3716746270656585693359375000000000000000\n  0.3443663418292999267578125000000000000000\n  -0.1359272152185440063476562500000000000000\n  -0.1876343488693237304687500000000000000000\n  0.2340205162763595581054687500000000000000\n  -0.2324854433536529541015625000000000000000\n  -0.1114889532327651977539062500000000000000\n  0.1867937147617340087890625000000000000000\n  -0.1157815307378768920898437500000000000000\n  0.2143053561449050903320312500000000000000\n  -0.0503807589411735534667968750000000000000\n  2.8299667835235595703125000000000000000000\n  0.1931485086679458618164062500000000000000\n  0.0197764113545417785644531250000000000000\n  0.1636547893285751342773437500000000000000\n  -0.0611205250024795532226562500000000000000\n  -0.0699935704469680786132812500000000000000\n  -0.2877841591835021972656250000000000000000\n  -0.0611667856574058532714843750000000000000\n  -0.3153498768806457519531250000000000000000\n  -0.0885297060012817382812500000000000000000\n  -0.1083305329084396362304687500000000000000\n  0.1889168620109558105468750000000000000000\n  -0.0905717536807060241699218750000000000000\n  -0.0397341959178447723388671875000000000000\n  -0.1428988873958587646484375000000000000000\n  -0.0503624826669692993164062500000000000000\n  0.1204090714454650878906250000000000000000\n  -0.0435086302459239959716796875000000000000\n  0.3180448412895202636718750000000000000000\n  -0.1842274665832519531250000000000000000000\n  0.0264257490634918212890625000000000000000\n  -0.1494389176368713378906250000000000000000\n  -0.4999824166297912597656250000000000000000\n  0.0441295541822910308837890625000000000000\n  -1.2105264663696289062500000000000000000000\n  0.3138801455497741699218750000000000000000\n  -0.3215926289558410644531250000000000000000\n  -0.1664963364601135253906250000000000000000\n  0.1060291230678558349609375000000000000000\n  -0.4454671740531921386718750000000000000000\n  -0.1396495848894119262695312500000000000000\n  -0.1237048953771591186523437500000000000000\n  -0.3294052183628082275390625000000000000000\n  0.0312922783195972442626953125000000000000\n  -0.2315267622470855712890625000000000000000\n  -0.0554199665784835815429687500000000000000\n  0.2790841162204742431640625000000000000000\n  0.0248106475919485092163085937500000000000\n  -0.0485207736492156982421875000000000000000\n  -0.3033912181854248046875000000000000000000\n  0.0747380107641220092773437500000000000000\n  0.1851936280727386474609375000000000000000\n  -0.0238611251115798950195312500000000000000\n  0.0401033610105514526367187500000000000000\n  0.0318896658718585968017578125000000000000\n  0.0803017541766166687011718750000000000000\n  -0.1906387805938720703125000000000000000000\n  -0.1030133292078971862792968750000000000000\n  -0.0111450217664241790771484375000000000000\n  0.1490245014429092407226562500000000000000\n  0.1258638799190521240234375000000000000000\n  0.1109465658664703369140625000000000000000\n  0.0570634566247463226318359375000000000000\n  -0.3881892561912536621093750000000000000000\n  -0.2134307622909545898437500000000000000000\n  -0.0437496043741703033447265625000000000000\n  0.1021756380796432495117187500000000000000\n  0.1844915449619293212890625000000000000000\n  0.2200553417205810546875000000000000000000\n  -0.2500122487545013427734375000000000000000\n  -0.0456166788935661315917968750000000000000\n  0.2639001011848449707031250000000000000000\n  -0.5536926388740539550781250000000000000000\n  0.1944777816534042358398437500000000000000\n  -0.1417385935783386230468750000000000000000\n  -0.0543786548078060150146484375000000000000\n  -0.0861224308609962463378906250000000000000\n  -0.3381765782833099365234375000000000000000\n  -1.8141101598739624023437500000000000000000\n  -0.1948805004358291625976562500000000000000\n  0.0568972527980804443359375000000000000000\n  0.5957250595092773437500000000000000000000\n  -0.2196479290723800659179687500000000000000\n  -0.0235826335847377777099609375000000000000\n  0.0122683942317962646484375000000000000000\n  0.1259320825338363647460937500000000000000\n  0.3178702294826507568359375000000000000000\n  0.0387944914400577545166015625000000000000\n  -0.0893718600273132324218750000000000000000\n  0.0614664554595947265625000000000000000000\n  0.1208274364471435546875000000000000000000\n  0.2069464325904846191406250000000000000000\n  -0.1558156907558441162109375000000000000000\n  0.6279892921447753906250000000000000000000\n  -0.0798642411828041076660156250000000000000\n  0.0253428556025028228759765625000000000000\n  0.2360073328018188476562500000000000000000\n  0.0962903648614883422851562500000000000000\n  0.0023586079478263854980468750000000000000\n  0.1376390904188156127929687500000000000000\n  -0.4615380764007568359375000000000000000000\n  0.3116810619831085205078125000000000000000\n  0.0445803143084049224853515625000000000000\n  0.0180421508848667144775390625000000000000\n  -0.0104834241792559623718261718750000000000\n  -0.5063576698303222656250000000000000000000\n  -0.1285248696804046630859375000000000000000\n  0.1488107740879058837890625000000000000000\n  0.0062906928360462188720703125000000000000\n  -0.0796267315745353698730468750000000000000\n  0.2655480504035949707031250000000000000000\n  -0.2381063103675842285156250000000000000000\n  -0.2346877604722976684570312500000000000000\n  -4.0225687026977539062500000000000000000000\n  0.1550216078758239746093750000000000000000\n  -0.2795819640159606933593750000000000000000\n  -0.5101599693298339843750000000000000000000\n  0.2493618428707122802734375000000000000000\n  -0.2720109522342681884765625000000000000000\n  0.6180023550987243652343750000000000000000\n  -0.0283792596310377120971679687500000000000\n  0.0197594836354255676269531250000000000000\n  0.0448741838335990905761718750000000000000\n  -0.1361175924539566040039062500000000000000\n  0.2182110100984573364257812500000000000000\n  -0.2050072997808456420898437500000000000000\n  -0.1586818993091583251953125000000000000000\n  -0.2654087543487548828125000000000000000000\n  -0.0562274679541587829589843750000000000000\n  0.6149771809577941894531250000000000000000\n  -0.0530276745557785034179687500000000000000\n  0.0502445995807647705078125000000000000000\n  0.0204158462584018707275390625000000000000\n  -0.2419121265411376953125000000000000000000\n  -0.2392247319221496582031250000000000000000\n  0.1815099716186523437500000000000000000000\n  0.2480162829160690307617187500000000000000\n  0.4010596275329589843750000000000000000000\n  0.0992467924952507019042968750000000000000\n  -0.6305630803108215332031250000000000000000\n  -0.2834658324718475341796875000000000000000\n  -0.0154599845409393310546875000000000000000\n  -0.0679127424955368041992187500000000000000\n  0.1259053349494934082031250000000000000000\n  -0.4687770307064056396484375000000000000000\n  -0.1195694655179977416992187500000000000000\n  0.2290054261684417724609375000000000000000\n  -0.1007030457258224487304687500000000000000\n  -0.0128645002841949462890625000000000000000\n  -0.0177517328411340713500976562500000000000\n  0.0347099527716636657714843750000000000000\n  0.3138544261455535888671875000000000000000\n  -0.4781226515769958496093750000000000000000\n  0.2516773641109466552734375000000000000000\n  0.5059696435928344726562500000000000000000\n  0.2395726740360260009765625000000000000000\n  -0.0191350188106298446655273437500000000000\n  0.4381106197834014892578125000000000000000\n  -0.0336080454289913177490234375000000000000\n  0.0219935439527034759521484375000000000000\n  0.4081116020679473876953125000000000000000\n  0.2027953565120697021484375000000000000000\n  0.1195852756500244140625000000000000000000\n  -0.0242141243070363998413085937500000000000\n  0.1403254419565200805664062500000000000000\n  0.9402859210968017578125000000000000000000\n  0.0164909437298774719238281250000000000000\n  0.1962469518184661865234375000000000000000\n  -0.0384700223803520202636718750000000000000\n  0.1902872174978256225585937500000000000000\n  -0.2207993865013122558593750000000000000000\n  -0.0721439421176910400390625000000000000000\n  0.0818929150700569152832031250000000000000\n  0.4540398418903350830078125000000000000000\n  -0.2169896215200424194335937500000000000000\n  0.2238811999559402465820312500000000000000\n  -0.0606808550655841827392578125000000000000\n  0.0951020196080207824707031250000000000000\n  -0.3208038210868835449218750000000000000000\n  0.2830214798450469970703125000000000000000\n  -0.1718855649232864379882812500000000000000\n  -0.1915347725152969360351562500000000000000\n  -0.0004353262484073638916015625000000000000\n  -0.4523738622665405273437500000000000000000\n  0.1046383678913116455078125000000000000000\n  0.1109635606408119201660156250000000000000\n  -0.5801369547843933105468750000000000000000\n  0.1468945294618606567382812500000000000000\n  -0.3988269269466400146484375000000000000000\n  -0.1383200585842132568359375000000000000000\n  0.2581778168678283691406250000000000000000\n  -0.1632029414176940917968750000000000000000\n  0.0825500488281250000000000000000000000000\n  -0.6948050260543823242187500000000000000000\n  0.0902899205684661865234375000000000000000\n  0.0195451974868774414062500000000000000000\n  0.3969180285930633544921875000000000000000\n  -0.2374185174703598022460937500000000000000\n  0.3373193740844726562500000000000000000000\n  -0.0385904684662818908691406250000000000000\n  0.0811361670494079589843750000000000000000\n  -0.2484533786773681640625000000000000000000\n  0.4316158294677734375000000000000000000000\n  -0.0581123530864715576171875000000000000000\n  0.1378033459186553955078125000000000000000\n  0.2746504247188568115234375000000000000000\n  0.1827860027551651000976562500000000000000\n  -0.4071970283985137939453125000000000000000\n  0.1484789550304412841796875000000000000000\n  0.2802324891090393066406250000000000000000\n  -0.2814048230648040771484375000000000000000\n  0.3470863699913024902343750000000000000000\n  -0.3585481941699981689453125000000000000000\n  0.2920519113540649414062500000000000000000\n  -0.4220661520957946777343750000000000000000\n  -0.2130400985479354858398437500000000000000\n  -0.0682796686887741088867187500000000000000\n  -0.0921852365136146545410156250000000000000\n  -0.0526989847421646118164062500000000000000\n  -0.5957971215248107910156250000000000000000\n  0.0189025253057479858398437500000000000000\n  0.0872076228260993957519531250000000000000\n  -0.0396125242114067077636718750000000000000\n  -0.1730719953775405883789062500000000000000\n  0.3455224335193634033203125000000000000000\n  -0.2666912078857421875000000000000000000000\n  0.1304198503494262695312500000000000000000\n  0.9724323153495788574218750000000000000000\n  -0.3981490433216094970703125000000000000000\n  -0.3131867349147796630859375000000000000000\n  0.2191239595413208007812500000000000000000\n  0.3306702375411987304687500000000000000000\n  0.0391867384314537048339843750000000000000\n  0.1044848337769508361816406250000000000000\n  -0.0825614556670188903808593750000000000000\n  0.0670375898480415344238281250000000000000\n  0.1857926547527313232421875000000000000000\n  -0.0746410712599754333496093750000000000000\n  0.2795280218124389648437500000000000000000\n  0.2246182262897491455078125000000000000000\n  -0.1496575623750686645507812500000000000000\n  -0.2707142829895019531250000000000000000000\n  -0.2651285827159881591796875000000000000000\n  0.1118899583816528320312500000000000000000\n  -0.2252609729766845703125000000000000000000\n  -0.4456525743007659912109375000000000000000\n  -0.3954851925373077392578125000000000000000\n  0.0834182649850845336914062500000000000000\n  -0.1924120336771011352539062500000000000000\n  -0.1605880856513977050781250000000000000000\n  0.1734081804752349853515625000000000000000\n  0.1808884143829345703125000000000000000000\n  -0.0126494169235229492187500000000000000000\n  0.4903608858585357666015625000000000000000\n  0.0644223242998123168945312500000000000000\n  -0.1003524065017700195312500000000000000000\n  0.4599490463733673095703125000000000000000\n  0.3453973233699798583984375000000000000000\n  0.2042080312967300415039062500000000000000\n  0.1134516224265098571777343750000000000000\n  0.0833615660667419433593750000000000000000\n  -0.2564638257026672363281250000000000000000\n  0.3943030536174774169921875000000000000000\n  0.0461210422217845916748046875000000000000\n  -0.5457835197448730468750000000000000000000\n  -0.0091056162491440773010253906250000000000\n  -0.2139520794153213500976562500000000000000\n  0.3821421861648559570312500000000000000000\n  0.6070846319198608398437500000000000000000\n  -0.1252755224704742431640625000000000000000\n  -0.2468324899673461914062500000000000000000\n  -0.3752884864807128906250000000000000000000\n  -0.2225593924522399902343750000000000000000\n  0.5149139761924743652343750000000000000000\n  0.2002686560153961181640625000000000000000\n  -1.8304973840713500976562500000000000000000\n  0.1937912404537200927734375000000000000000\n  0.3067590296268463134765625000000000000000\n  -0.1715855598449707031250000000000000000000\n  0.2889835238456726074218750000000000000000\n  0.0196146257221698760986328125000000000000\n  -0.0900838673114776611328125000000000000000\n  -0.0341891199350357055664062500000000000000\n  0.1779085993766784667968750000000000000000\n  0.0998954772949218750000000000000000000000\n  -0.1203276291489601135253906250000000000000\n  -0.0663873553276062011718750000000000000000\n  0.2088625431060791015625000000000000000000\n  -0.0117890387773513793945312500000000000000\n  0.3295350670814514160156250000000000000000\n  0.1747308075428009033203125000000000000000\n  0.0477313473820686340332031250000000000000\n  0.1357067525386810302734375000000000000000\n  -0.1197784245014190673828125000000000000000\n  0.2644523978233337402343750000000000000000\n  -0.0185669735074043273925781250000000000000\n  0.7069437503814697265625000000000000000000\n  0.2551424801349639892578125000000000000000\n  0.1677446663379669189453125000000000000000\n  0.2259395420551300048828125000000000000000\n  -0.1098010838031768798828125000000000000000\n  -0.2836391925811767578125000000000000000000\n  0.4090534448623657226562500000000000000000\n  0.1753546595573425292968750000000000000000\n  -0.0894893854856491088867187500000000000000\n  0.0835848227143287658691406250000000000000\n  -0.7839426994323730468750000000000000000000\n  -0.0539495237171649932861328125000000000000\n  -0.0684932768344879150390625000000000000000\n  0.3460498750209808349609375000000000000000\n  0.2916064858436584472656250000000000000000\n  -0.0565768145024776458740234375000000000000\n  0.1716307997703552246093750000000000000000\n  0.1485068649053573608398437500000000000000\n  0.3516146242618560791015625000000000000000\n  -0.3150306046009063720703125000000000000000\n  0.3549548089504241943359375000000000000000\n  0.3337800502777099609375000000000000000000\n  0.1908558160066604614257812500000000000000\n  0.2041965723037719726562500000000000000000\n  0.2007670402526855468750000000000000000000\n  -0.2527462542057037353515625000000000000000\n  -0.5249834060668945312500000000000000000000\n  0.1853910386562347412109375000000000000000\n  -0.0259669274091720581054687500000000000000\n  0.2996138930320739746093750000000000000000\n  0.3778246641159057617187500000000000000000\n  0.2132288813591003417968750000000000000000\n  -0.3464865386486053466796875000000000000000\n  -0.3192611336708068847656250000000000000000\n  0.0339563526213169097900390625000000000000\n  -0.1064903959631919860839843750000000000000\n  -0.1587283462285995483398437500000000000000\n  0.1060464456677436828613281250000000000000\n  -0.3203383684158325195312500000000000000000\n  0.1796468496322631835937500000000000000000\n  -0.4581542313098907470703125000000000000000\n  0.2560085952281951904296875000000000000000\n  0.1954589933156967163085937500000000000000\n  -0.0191737841814756393432617187500000000000\n  -0.7284654378890991210937500000000000000000\n  0.3405342102050781250000000000000000000000\n  0.0723110064864158630371093750000000000000\n  -0.0545800365507602691650390625000000000000\n  -0.2525702714920043945312500000000000000000\n  0.3089988827705383300781250000000000000000\n  -0.0661127343773841857910156250000000000000\n  -0.5530314445495605468750000000000000000000\n  0.0174045339226722717285156250000000000000\n  0.0983052551746368408203125000000000000000\n  0.0837043374776840209960937500000000000000\n  0.2008765935897827148437500000000000000000\n  0.4940925240516662597656250000000000000000\n  -0.0892623066902160644531250000000000000000\n  -0.4070002734661102294921875000000000000000\n  0.1945792287588119506835937500000000000000\n  -0.6032828092575073242187500000000000000000\n  -0.2224391251802444458007812500000000000000\n  0.4005252420902252197265625000000000000000\n  -0.0228614993393421173095703125000000000000\n  -0.0804782435297966003417968750000000000000\n  0.0088103730231523513793945312500000000000\n  0.2278631031513214111328125000000000000000\n  0.2101539820432662963867187500000000000000\n  0.0612324178218841552734375000000000000000\n  -0.2613401412963867187500000000000000000000\n  -0.3925894200801849365234375000000000000000\n  0.2962611913681030273437500000000000000000\n  0.2114235609769821166992187500000000000000\n  0.2915764451026916503906250000000000000000\n  -0.2312765419483184814453125000000000000000\n  -0.0425117611885070800781250000000000000000\n  -0.1053224429488182067871093750000000000000\n  -0.2425258606672286987304687500000000000000\n  0.0600274056196212768554687500000000000000\n  -0.1823718249797821044921875000000000000000\n  0.1289860457181930541992187500000000000000\n  -0.0785445719957351684570312500000000000000\n  -0.0004370734095573425292968750000000000000\n  0.1153641939163208007812500000000000000000\n  0.1369608044624328613281250000000000000000\n  0.4928187727928161621093750000000000000000\n  -0.3548105061054229736328125000000000000000\n  -0.4070906639099121093750000000000000000000\n  0.1436492949724197387695312500000000000000\n  -0.0005390271544456481933593750000000000000\n  0.1706836670637130737304687500000000000000\n  0.0175886582583189010620117187500000000000\n  0.0745415166020393371582031250000000000000\n  0.0769264027476310729980468750000000000000\n  -0.1236746609210968017578125000000000000000\n  -0.4433776140213012695312500000000000000000\n  -0.2223702371120452880859375000000000000000\n  1.4944913387298583984375000000000000000000\n  0.1796656250953674316406250000000000000000\n  0.2647941112518310546875000000000000000000\n  0.0584772452712059020996093750000000000000\n  0.4397549629211425781250000000000000000000\n  0.1333072036504745483398437500000000000000\n  0.0136699788272380828857421875000000000000\n  -0.2635672092437744140625000000000000000000\n  -0.0294947735965251922607421875000000000000\n  -0.0157805755734443664550781250000000000000\n  -0.1967912465333938598632812500000000000000\n  0.0838828682899475097656250000000000000000\n  -0.1808511018753051757812500000000000000000\n  -0.2097346037626266479492187500000000000000\n  0.1258245706558227539062500000000000000000\n  0.4100631773471832275390625000000000000000\n  -0.0765558481216430664062500000000000000000\n  0.0018648300319910049438476562500000000000\n  -0.4111917018890380859375000000000000000000\n  -0.1582978516817092895507812500000000000000\n  -0.1571654379367828369140625000000000000000\n  -0.0442710705101490020751953125000000000000\n  0.1801277101039886474609375000000000000000\n  -0.0379635579884052276611328125000000000000\n  -0.3397905826568603515625000000000000000000\n  -0.0551042072474956512451171875000000000000\n  0.2694746255874633789062500000000000000000\n  -0.3701204061508178710937500000000000000000\n  -0.1440439671277999877929687500000000000000\n  -0.1110343858599662780761718750000000000000\n  -0.2083767205476760864257812500000000000000\n  0.0558053515851497650146484375000000000000\n  0.3244479894638061523437500000000000000000\n  0.0339851230382919311523437500000000000000\n  -0.1025798767805099487304687500000000000000\n  -0.1587015688419342041015625000000000000000\n  -0.0559125319123268127441406250000000000000\n  -0.1196292787790298461914062500000000000000\n  -0.1375107765197753906250000000000000000000\n  0.2521515488624572753906250000000000000000\n  0.0081503875553607940673828125000000000000\n  -0.0817147344350814819335937500000000000000\n  0.5187364816665649414062500000000000000000\n  0.0192114152014255523681640625000000000000\n  0.0351702161133289337158203125000000000000\n  0.0308043882250785827636718750000000000000\n  0.0849840566515922546386718750000000000000\n  -0.5475702881813049316406250000000000000000\n  0.1629963964223861694335937500000000000000\n  0.6392979025840759277343750000000000000000\n  0.0297306347638368606567382812500000000000\n  -0.3069167435169219970703125000000000000000\n  -0.3782160282135009765625000000000000000000\n  0.1545276492834091186523437500000000000000\n  -0.2343596220016479492187500000000000000000\n  0.1546582430601119995117187500000000000000\n  -0.0265807751566171646118164062500000000000\n  -0.3750148117542266845703125000000000000000\n  -0.1871166527271270751953125000000000000000\n  0.3216272294521331787109375000000000000000\n  -0.0451802909374237060546875000000000000000\n  0.4775571525096893310546875000000000000000\n  0.4529955685138702392578125000000000000000\n  -0.5110341310501098632812500000000000000000\n  0.0463488399982452392578125000000000000000\n  -0.0710215047001838684082031250000000000000\n  -0.0216862428933382034301757812500000000000\n  -0.0613699927926063537597656250000000000000\n  -0.0601677596569061279296875000000000000000\n  -0.2007377594709396362304687500000000000000\n  0.0864606499671936035156250000000000000000\n  0.0613626427948474884033203125000000000000\n  0.4488885700702667236328125000000000000000\n  0.5349248647689819335937500000000000000000\n  -0.0305566005408763885498046875000000000000\n  -0.0256629828363656997680664062500000000000\n  0.2848588228225708007812500000000000000000\n  -0.0022094019223004579544067382812500000000\n  -0.1475551277399063110351562500000000000000\n  -1.5750963687896728515625000000000000000000\n  -0.1576858311891555786132812500000000000000\n  0.3455810248851776123046875000000000000000\n  0.6216902136802673339843750000000000000000\n  0.0416193008422851562500000000000000000000\n  -0.0875504389405250549316406250000000000000\n  -0.1846969276666641235351562500000000000000\n  -0.0172814745455980300903320312500000000000\n  -0.0727644786238670349121093750000000000000\n  0.3020963072776794433593750000000000000000\n  0.1093506589531898498535156250000000000000\n  0.3666940033435821533203125000000000000000\n  0.3246493637561798095703125000000000000000\n  0.1556366086006164550781250000000000000000\n  0.0860142484307289123535156250000000000000\n  0.2658248543739318847656250000000000000000\n  0.1497662365436553955078125000000000000000\n  -0.0956973880529403686523437500000000000000\n  -0.1908606290817260742187500000000000000000\n  -0.1614024043083190917968750000000000000000\n  -0.0932007580995559692382812500000000000000\n  0.1253612190485000610351562500000000000000\n  -0.2715030908584594726562500000000000000000\n  -0.2058297991752624511718750000000000000000\n  -0.1233465522527694702148437500000000000000\n  0.4571524858474731445312500000000000000000\n  0.0540538877248764038085937500000000000000\n  -0.4112908542156219482421875000000000000000\n  0.2232905477285385131835937500000000000000\n  0.0525711923837661743164062500000000000000\n  0.0276793055236339569091796875000000000000\n  0.2458618730306625366210937500000000000000\n  -0.0748695284128189086914062500000000000000\n  0.0655638799071311950683593750000000000000\n  0.0644927844405174255371093750000000000000\n  -0.2840280830860137939453125000000000000000\n  0.3124719262123107910156250000000000000000\n  -0.2233127057552337646484375000000000000000\n  -0.0276475697755813598632812500000000000000\n  0.2736888229846954345703125000000000000000\n  -0.2394964843988418579101562500000000000000\n  0.2888382077217102050781250000000000000000\n  0.0476279519498348236083984375000000000000\n  0.3719787001609802246093750000000000000000\n  -0.0247384775429964065551757812500000000000\n  0.3267970979213714599609375000000000000000\n  0.2250697016716003417968750000000000000000\n  -0.1625260710716247558593750000000000000000\n  0.6405866742134094238281250000000000000000\n  -0.3017705678939819335937500000000000000000\n  -0.0846613720059394836425781250000000000000\n  -0.0546317659318447113037109375000000000000\n  -0.1060398817062377929687500000000000000000\n  0.1349414587020874023437500000000000000000\n  0.0380891636013984680175781250000000000000\n  -0.0345413796603679656982421875000000000000\n  -0.1512525677680969238281250000000000000000\n  -0.0658848136663436889648437500000000000000\n  -0.1355030983686447143554687500000000000000\n  -0.1400355547666549682617187500000000000000\n  -0.4521144926548004150390625000000000000000\n  0.3259736001491546630859375000000000000000\n  -0.0208822898566722869873046875000000000000\n  0.3326666653156280517578125000000000000000\n  0.2348407357931137084960937500000000000000\n  -0.2666296064853668212890625000000000000000\n  -0.4798229336738586425781250000000000000000\n  0.2864567041397094726562500000000000000000\n  0.1038343757390975952148437500000000000000\n  -0.3005046248435974121093750000000000000000\n  -0.1050831153988838195800781250000000000000\n  -0.1732683032751083374023437500000000000000\n  0.2095506787300109863281250000000000000000\n  0.0537731535732746124267578125000000000000\n  0.0843820869922637939453125000000000000000\n  -0.1842034906148910522460937500000000000000\n  0.3002288937568664550781250000000000000000\n  0.5088806152343750000000000000000000000000\n  -0.0020541250705718994140625000000000000000\n  0.3818906545639038085937500000000000000000\n  0.1097332760691642761230468750000000000000\n  0.0508306771516799926757812500000000000000\n  0.2636719346046447753906250000000000000000\n  -0.3118977546691894531250000000000000000000\n  0.3305926918983459472656250000000000000000\n  -4.3749451637268066406250000000000000000000\n  -0.4248710572719573974609375000000000000000\n  -0.3150911927223205566406250000000000000000\n  -0.0760410279035568237304687500000000000000\n  -0.4033440947532653808593750000000000000000\n  -0.1376413106918334960937500000000000000000\n  0.3623409569263458251953125000000000000000\n  -0.2613637447357177734375000000000000000000\n  0.0157706364989280700683593750000000000000\n  -0.0317413620650768280029296875000000000000\n  0.3822115957736968994140625000000000000000\n  -0.1167531535029411315917968750000000000000\n  -0.1088832616806030273437500000000000000000\n  0.0568295307457447052001953125000000000000\n  0.7807974219322204589843750000000000000000\n  0.1389092803001403808593750000000000000000]]');
INSERT INTO `ms_file` (`file_id`, `file_name`, `file_content`, `file_url`, `file_content_vector`) VALUES
(5, 'YOLOv9_for_Fracture_Detection_in_Pediatric_Wrist_Trauma_X-ray_Images', 'YOLOv9 for Fracture Detection in PediatricWrist Trauma X-ray ImagesChun-Tse Chien,1 Rui-Yang Ju,2 Kuang-Yi Chou,3 andJen-Shiun Chiang(cid:66), 11Department of Electrical and Computer Engineering, TamkangUniversity, New Taipei City, 251301, Taiwan2Graduate Institute of Networking and Multimedia, National TaiwanUniversity, Taipei City, 106335, Taiwan3School of Nursing, National Taipei University of Nursing and HealthSciences, Taipei City, 112303, Taiwan(cid:66)Email: jsken.chiang@gmail.comThe introduction of YOLOv9, the latest version of the You Only LookOnce (YOLO) series, has led to its widespread adoption across variousscenarios. This paper is the first to apply the YOLOv9 algorithm modelto the fracture detection task as computer-assisted diagnosis (CAD) tohelp radiologists and surgeons to interpret X-ray images. Specifically,this paper trained the model on the GRAZPEDWRI-DX dataset andextended the training set using data augmentation techniques to improvethe model performance. Experimental results demonstrate that com-pared to the mAP 50-95 of the current state-of-the-art (SOTA) model,the YOLOv9 model increased the value from 42.16% to 43.73%, withan improvement of 3.7%. The implementation code is publicly availableat https://github.com/RuiyangJu/YOLOv9-Fracture-Detection.4202raM71]VI.ssee[1v94211.3042:viXraIntroduction: Computer-assisted diagnosis (CAD) helps specialistssuch as radiologists and surgeons to interpret medical images, includ-ing magnetic resonance imaging (MRI), computed tomography (CT),and X-ray images. The application of deep learning techniques to medi-cal images [1–4] has yielded increasingly satisfactory results, making ita popular research focus, especially in fracture detection [5–7].You Only Look Once (YOLO) series [8–16] are the main neural net-works for real-time object detection task, widely employed in fracturedetection [17–19]. Wrist fractures in children are more common casesand the GRAZPEDWRI-DX dataset [20] provides 20,327 X-ray imagesof pediatric wrist trauma that can be used in fracture detection tasks.Research [21] first used the YOLOv8 [16] model for fracture detec-tion on this dataset. Since attention mechanisms [22–25] have excellentresults in enhancing the performance of neural network models, Chienet al. achieved the state-of-the-art (SOTA) performance by incorporatingdifferent attention mechanisms into the YOLOv8 model.With the presentation of YOLOv9 [26], which achieved remarkablemodel performance on the MS COCO 2017 [27] benchmark dataset,this paper first trained the YOLOv9 model on the GRAZPEDWRI-DXdataset and obtained the SOTA performance, as shown in Fig 1.The main contributions of this paper are as follows:1. This paper is the first to apply YOLOv9 to the fracture detectiontask, demonstrating that the model not only has the excellent per-formance in real-time object detection across real-life scenarios, butalso has good results in medical image recognition.2. This paper addresses the issue of information loss in fracture detec-tion on X-ray images by employing YOLOv9 algorithm, and aimsto preserve more information during model training on low-featuresX-ray images, enhancing the performance of the model.3. The mAP 50-95 of YOLOv9 model trained on the GRAZPEDWRI-DX dataset is significantly improved, achieving the SOTA level.Related Works: In the field of object detection task, detectors typicallyemploy either one-stage or two-stage algorithms. Compared to two-stage object detectors, the models of YOLO series offer a more balancedcombination of accuracy and inference speed, making them suitable fordeployment on mobile computing platforms for medical image recog-nition. Son et al. [28] utilized YOLOv4 [9] and U-Net [29] as auxil-iary diagnostic tools to assist dentists in identifying mandibular frac-tures without resorting to cone beam computed tomography (CBCT).Jeon et al. [30] employed YOLOv4 [9] to aid surgeons in diagnosingtrauma by detecting the fracture and mapping it onto a 3D reconstructedFig 1 Comparisons of fracture detection models on the GRAZPEDWRI-DXdataset. In terms of accuracy, our models outperform all previous models atthe state-of-the-art level.bone image, providing a clear display of the fracture region through thered mask overlaid on the 3D bone image. Hrži´c et al. [18] employedthe YOLOv4 [9] model for fracture detection on the GRAZPEDWRI-DX dataset [20], which was the first to demonstrate that the models ofYOLO series can assist radiologists in more accurately predicting wristinjuries in children on X-ray images. Ahmed et al. [31] demonstratedthe potential of one-stage algorithm models in enhancing the accuracyof diagnosis on pediatric wrist X-ray images by employing YOLOv5[12], YOLOv6 [13], YOLOv7 [15], and YOLOv8 [16] models for wristanomaly detection, respectively. Warin et al. [32] utilized the YOLOv5[12] model to detect mandibular fractures in panoramic X-ray images,demonstrating the ability of the YOLOv5 model to recognize mandibu-lar fractures at an expert level. Gaikwad et al. [33] applied the YOLOv5[12] model to detect major and minor fractures of the C1 to C7 ver-tebrae, achieving an accuracy rate of 89%. Zou et al. [34] investigatedvarious fracture morphologies throughout the body, including angle frac-tures, normal fractures, line fractures, and disoriented angle fractures.They integrated the YOLOv7 [15] model with the attention mechanism[35], achieving the superior performance on the FracAtlas [36] dataset.Samothai et al. [17] demonstrated that the YOLOX [10] model exhibitsfaster convergence speed and higher accuracy than YOLOR [11] bydetecting the fracture region through the methods such as detector headdecoupling, anchor-free, and augmentation strategies. They also showedthat YOLOX can locate fractures even in low-featured X-ray images.Moon et al. [37] proposed a computer-aided facial bone fracture diagno-sis (CA-FBFD) system based on the YOLOX model, effectively reduc-ing the workload of doctors in diagnosing facial fractures in facial CTscans. While the application of the models of YOLO series to medicalimage recognition is a hot research topic, to date, no one has utilizedYOLOv9 [26] for fracture detection.Method:YOLOv9: Neural networks often have the challenge of information losssince the input data undergoes multiple layers of feature extraction andspatial transformation, resulting in the loss of the original information.This issue is particularly pronounced in X-ray images, where the low-features present significant difficult in fracture detection tasks. Specif-ically, models trained on such low-featured images tend to performpoorly, and addressing the problem of information loss could substan-tially enhance the accuracy of model predictions. To address this, weutilizing the YOLOv9 algorithm, which leverages the ProgrammableGradient Information (PGI) and the Generalized Efficient Layer Aggre-gation Network (GELAN) to more effectively extract key features.Programmable Gradient Information: Programmable Gradient Infor-mation (PGI) is an auxiliary supervision framework designed to man-age the propagation of gradient information across various semantic lev-ELECTRONICS LETTERS wileyonlinelibrary.com/iet-el1012345678910Inference Time (ms)60616263646566mAP 50 (%)Performance on GRAZPEDWRI-DX DatasetYOLOv8YOLOv8+SAYOLOv8+ECAYOLOv8+GAMYOLOv8+ResGAMYOLOv8+ResCBAMYOLOv9 (Ours)mAP50 SOTASMLCE2.3% HigherSMLMLSSMLSMLSML      Fig 2 An overall flowchart of the YOLOv9 algorithm model applied to the fracture detection task.Table 1. Quantitative comparison with other state-of-the-art modelsfor fracture detection on the GRAZPEDWRI-DX datasets when theinput image size is 640.Table 2. Quantitative comparison with other state-of-the-art modelsfor fracture detection on the GRAZPEDWRI-DX datasets when theinput image size is 1024.ModelParamsFLOPsF1mAP 50mAP 50-95SpeedaModelParamsFLOPsF1mAP 50mAP 50-95Speeda(M)(G)(%)(%)(%)(ms)(M)(G)(%)(%)(%)(ms)YOLOv8[16]43.61164.9YOLOv8+SA[22]43.64165.4YOLOv8+ECA[23]43.64165.5YOLOv8+GAM[24]49.29183.5YOLOv8+ResGAM[38]49.29183.5YOLOv8+ResCBAM[25]53.87196.2YOLOv9-C (Ours)51.02239.0YOLOv9-E (Ours)69.42244.9596261606262646462.4463.9962.6463.3263.9762.9565.3165.4640.3241.4940.2140.7441.1840.1042.6643.323.63.93.68.79.44.15.26.4YOLOv8[16]43.61164.9YOLOv8+SA[22]43.64165.4YOLOv8+ECA[23]43.64165.5YOLOv8+GAM[24]49.29183.5YOLOv8+ResGAM[38]49.29183.5YOLOv8+ResCBAM[25]53.87196.2YOLOv9-C (Ours)51.02239.0YOLOv9-E (Ours)69.42244.9626365656464666663.6364.2564.2664.2664.9865.7865.5765.6240.4141.6441.9441.0041.7542.1643.7043.737.78.07.712.718.18.712.716.1Note: The model size of all YOLOv8 and its variants listed in the table is large.a Speed is the total time for preprocessing, inference, and post-processing.Note: The model size of all YOLOv8 and its variants listed in the table is large.a Speed is the total time for preprocessing, inference, and post-processing.els, to improve the detection capability of the model. PGI comprisesthree main components: main branch, auxiliary reversible branch, andmulti-level auxiliary information. During the inference process, it exclu-sively employs the main branch, which handles both forward and backpropagation. As the network becomes deeper, an information bottleneckmay occur, leading to loss functions that fail to produce useful gradients.In such cases, auxiliary reversible branch employs reversible functionsto preserve information integrity and mitigate information loss in themain branch. Additionally, multi-level auxiliary information addressesthe issue of error accumulation from the deep supervision mechanism,improving the learning capacity of the model through the introductionof supplementary information at different levels. Notably, research [26]highlighted the efficacy of PGI in preserving information during train-ing, particularly in scenarios with limited features. This provides the the-oretical basis for the YOLOv9 model to have excellent performance infracture detection tasks.Generalized Efficient Layer Aggregation Network: To enhance informa-tion integration and propagation efficiency in model training, YOLOv9introduced a novel lightweight network architecture named GeneralizedEfficient Layer Aggregation Network (GELAN). GELAN integratesCSPNet [39] and ELAN [40] to efficiently aggregate network infor-mation, reducing information loss in propagation and enhancing inter-layer information interaction. This architecture is particularly suitablefor fracture detection in environments with limited computing resourcesdue to its lower parameters and computational complexity.Data Processing and Augmentation: Fig 2 illustrates the flowchart ofthe experiments conducted in this study. Since the publisher of theGRAZPEDWRI-DX [20] dataset did not provide predefined training,validation, and test sets, we randomly assigned 70% to training set,20% to validation set, and 10% to test set during the data processing.Moreover, due to the limited brightness diversity of low-featured X-rayimages, models trained only on these images may not generalize well toX-ray images in other environments. To enhance the robustness of themodel, we employed data augmentation techniques to extend the train-ing set. Specifically, we fine-tuned the contrast and luminance of theX-ray images using the addWeighted function from the OpenCV library.Experiment:Dataset: GRAZPEDWRI-DX [20] is a public dataset provided by theMedical University of Graz, which contains 20,327 X-ray images ofpediatric wrist trauma. These X-ray images were collected by a teamof pediatric radiologists at the University Hospital Graz from 2008 to2018. The dataset comprises 6,091 patients and 10,643 studies, with atotal of 74,459 labeled images, representing 67,771 labeled objects.Experiment setup: The experiments in this paper utilized one singleNVIDIA GeForce RTX 3090 GPU, employing Python with the PyTorchframework. Before training our model, we employed the YOLOv9model weights pretrained on the MS COCO 2017 [27] dataset. In thetraining process, we trained the model using the SGD [41] optimizer,with a weight decay rate set to 5e-4 and a momentum of 0.937. We fol-lowed the research [21] to set the initial learning rate to 1e-2, the numberof epochs to 100. Due to resource limitations (24GB memory) imposedby a single GPU, a batch size of 16 was employed for training the model.Experimental Results: To evaluate the performance of YOLOv9 andother SOTA models in real diagnostic scenarios, this study comparesmodel size (parameters and floating-point operations per second), accu-racy (F1 score, mean average precision at 50% (mAP 50), and meanaverage precision from 50% to 95% (mAP 50-95)), and inference time. Itis widely recognized that using larger input image sizes improves predic-tion accuracy but also requires more computational resources. Therefore,we conducted two experiments with input image sizes of 640 and 1024for various scenarios, and the results are presented in Tables 1 and 2.With the input size of 640, both YOLOv9-C (Compact) and YOLOv9-E(Extended) demonstrate significantly improved mAP, while maintaininga reasonable inference speed. Specifically, YOLOv9-E achieves mAP2ELECTRONICS LETTERS wileyonlinelibrary.com/iet-elX-ray Image DatasetRandom SplitTraining Set (70%)Validation Set (20%)Testing Set (10%)Data AugmentationExtended Training SetYOLOv9 AlgorithmOur ModelValidateEvaluatePredictionPredict50-95 of 43.32%, which is 4.4% higher than 41.49% achieved by thecurrent SOTA model YOLOv8+SA. When the input image size is 1024,the mAP 50-95 of YOLOv9-E reaches 43.73%, which also obtains theSOTA performance. However, due to the increased inference time, it ismore suitable for deployment on devices with high computing resources.Conclusion: The models of YOLO series can serve as CAD to assistradiologists and surgeons in interpreting X-ray images. However, thepredictions from the previous models are often unsatisfactory due to thelow features of X-ray images. This paper first introduces the applica-tion of YOLOv9 to fracture detection, addressing the issue of infor-mation loss during model training by employing the newly proposedPGI and GELAN. Experimental results indicate that the YOLOv9 modelachieves SOTA performance on the GRAZPEDWRI-DX dataset, prov-ing the effectiveness of this method.Acknowledgments: This research is supported by National Science andTechnology Council of Taiwan, under Grant Number: NSTC 112-2221-E-032-037-MY2.© 2024 The Authors. Electronics Letters published by John Wiley &Sons Ltd on behalf of The Institution of Engineering and TechnologyReferences1. Chung, S.W., et al.: Automated detection and classification of theproximal humerus fracture by using deep learning algorithm. Actaorthopaedica 89(4), 468–473 (2018)2. Choi, J.W., et al.: Using a dual-input convolutional neural networkfor automated detection of pediatric supracondylar fracture on conven-tional radiography. Investigative radiology 55(2), 101–110 (2020)3. Tanzi, L., et al.: Hierarchical fracture classification of proximal femurx-ray images using a multistage deep learning approach. Europeanjournal of radiology 133, 109373 (2020)4. Adams, S.J., et al.: Artificial intelligence solutions for analysis of x-rayimages. Canadian Association of Radiologists Journal 72(1), 60–72(2021)5. Gan, K., et al.: Artificial intelligence detection of distal radius frac-tures: a comparison between the convolutional neural network and pro-fessional assessments. Acta orthopaedica 90(4), 394–400 (2019)6. Yahalomi, E., Chernofsky, M., Werman, M.: Detection of distal radiusfractures trained by a small set of x-ray images and faster r-cnn. In:Intelligent Computing: Proceedings of the 2019 Computing Confer-ence, Volume 1, pp. 971–981. Springer (2019)7. Blüthgen, C., et al.: Detection and localization of distal radius frac-tures: Deep learning system versus radiologists. European journal ofradiology 126, 108925 (2020)8. Redmon, J., et al.: You only look once: Unified, real-time object detec-tion. In: Proceedings of the IEEE conference on computer vision andpattern recognition, pp. 779–788. (2016)9. Bochkovskiy, A., Wang, C.Y., Liao, H.Y.M.: Yolov4: Optimal speedand accuracy of object detection. arXiv preprint arXiv:2004.10934(2020)10. Ge, Z., et al.: Yolox: Exceeding yolo series in 2021. arXiv preprintarXiv:2107.08430 (2021)11. Wang, C.Y., Yeh, I.H., Liao, H.Y.M.: You only learn one representation:Unified network for multiple tasks. arXiv preprint arXiv:2105.04206(2021)12. Glenn, J.: Ultralytics yolov5. GitHub. https://github.com/ultralytics/yolov5 (2022)14.13. Li, C., et al.: Yolov6: A single-stage object detection framework forindustrial applications. arXiv preprint arXiv:2209.02976 (2022)Ju, R.Y., et al.: Resolution enhancement processing on low qualityimages using swin transformer based on interval dense connectionstrategy. Multimedia Tools and Applications pp. 1–17. (2023)15. Wang, C.Y., Bochkovskiy, A., Liao, H.Y.M.: Yolov7: Trainable bag-of-freebies sets new state-of-the-art for real-time object detectors. In: Pro-ceedings of the IEEE/CVF Conference on Computer Vision and PatternRecognition, pp. 7464–7475. (2023)16. Glenn, J.: Ultralytics yolov8. GitHub. https://github.com/ultralytics/ultralytics (2023)17. Samothai, P., et al.: The evaluation of bone fracture detection ofyolo series.In: 2022 37th International Technical Conferenceon Circuits/Systems, Computers and Communications (ITC-CSCC),pp. 1054–1057. IEEE (2022)18. Hrži´c, F., et al.: Fracture recognition in paediatric wrist radiographs:An object detection approach. Mathematics 10(16), 2939 (2022)19. Su, Z., et al.: Skeletal fracture detection with deep learning: A compre-hensive review. Diagnostics 13(20), 3245 (2023)20. Nagy, E., et al.: A pediatric wrist trauma x-ray dataset (grazpedwri-dx)21.for machine learning. Scientific Data 9(1), 222 (2022)Ju, R.Y., Cai, W.: Fracture detection in pediatric wrist trauma x-rayimages using yolov8 algorithm.arXiv preprint arXiv:2304.05071(2023)22. Zhang, Q.L., Yang, Y.B.: Sa-net: Shuffle attention for deep convolu-tional neural networks.In: ICASSP 2021-2021 IEEE InternationalConference on Acoustics, Speech and Signal Processing (ICASSP),pp. 2235–2239. IEEE (2021)23. Wang, Q., et al.: Eca-net: Efficient channel attention for deep convolu-tional neural networks. In: Proceedings of the IEEE/CVF conferenceon computer vision and pattern recognition, pp. 11534–11542. (2020)24. Liu, Y., Shao, Z., Hoffmann, N.: Global attention mechanism: RetainarXiv preprintinformation to enhance channel-spatial interactions.arXiv:2112.05561 (2021)25. Woo, S., et al.: Cbam: Convolutional block attention module.In:Proceedings of the European conference on computer vision (ECCV),pp. 3–19. (2018)26. Wang, C.Y., Yeh, I.H., Liao, H.Y.M.: Yolov9: Learning what you wantarXiv preprintto learn using programmable gradient information.arXiv:2402.13616 (2024)27. Lin, T.Y., et al.: Microsoft coco: Common objects in context. In: Com-puter Vision–ECCV 2014: 13th European Conference, Zurich, Switzer-land, September 6-12, 2014, Proceedings, Part V 13, pp. 740–755.Springer (2014)28. Son, D.M., et al.: Combined deep learning techniques for mandibularfracture diagnosis assistance. Life 12(11), 1711 (2022)29. Ronneberger, O., Fischer, P., Brox, T.: U-net: Convolutional networksfor biomedical image segmentation.In: Medical Image Computingand Computer-Assisted Intervention–MICCAI 2015: 18th InternationalConference, Munich, Germany, October 5-9, 2015, Proceedings, PartIII 18, pp. 234–241. Springer (2015)Jeon, Y.D., et al.: Deep learning model based on you only look oncealgorithm for detection and visualization of fracture areas in three-dimensional skeletal images. Diagnostics 14(1), 11 (2023)30.31. Ahmed, A., et al.: Enhancing wrist abnormality detection with yolo:Analysis of state-of-the-art single-stage detection models. BiomedicalSignal Processing and Control 93, 106144 (2024)32. Warin, K., et al.: Assessment of deep convolutional neural networkmodels for mandibular fracture detection in panoramic radiographs.International Journal of Oral and Maxillofacial Surgery 51(11), 1488–1494 (2022)33. Gaikwad, D., et al.: Identification of cervical spine fracture using deeplearning. Australian Journal of Multi-Disciplinary Engineering pp. 1–9. (2024)34. Zou, J., Arshad, M.R.: Detection of whole body bone fractures basedon improved yolov7. Biomedical Signal Processing and Control 91,105995 (2024)35. Hu, J., Shen, L., Sun, G.: Squeeze-and-excitation networks. In: Pro-ceedings of the IEEE conference on computer vision and pattern recog-nition, pp. 7132–7141. (2018)36. Abedeen, I., et al.: Fracatlas: A dataset for fracture classification, local-ization and segmentation of musculoskeletal radiographs. ScientificData 10(1), 521 (2023)37. Moon, G., et al.: Computer aided facial bone fracture diagnosis (ca-fbfd) system based on object detection model. IEEE Access 10, 79061–79070 (2022)38. Chien, C.T., et al.: Yolov8-am: Yolov8 with attention mechanisms forarXiv preprint arXiv:2402.09329pediatric wrist fracture detection.(2024)39. Wang, C.Y., et al.: Cspnet: A new backbone that can enhance learn-ing capability of cnn.In: Proceedings of the IEEE/CVF conferenceon computer vision and pattern recognition workshops, pp. 390–391.(2020)40. Wang, C.Y., Liao, H.Y.M., Yeh, I.H.: Designing network design strate-gies through gradient path analysis. arXiv preprint arXiv:2211.04800(2022)41. Ruder, S.: An overview of gradient descent optimization algorithms.arXiv preprint arXiv:1609.04747 (2016)ELECTRONICS LETTERS wileyonlinelibrary.com/iet-el3', 'pdf/YOLOv9_for_Fracture_Detection_in_Pediatric_Wrist_Trauma_X-ray_Images.pdf', '[[-0.2472720295190811157226562500000000000000\n  0.0364129766821861267089843750000000000000\n  0.1494421958923339843750000000000000000000\n  0.0607616677880287170410156250000000000000\n  -0.0209843516349792480468750000000000000000\n  -0.1164777278900146484375000000000000000000\n  0.0525716394186019897460937500000000000000\n  -0.0187160745263099670410156250000000000000\n  0.0879080295562744140625000000000000000000\n  -0.1466029584407806396484375000000000000000\n  -0.2422543764114379882812500000000000000000\n  -0.1151341944932937622070312500000000000000\n  0.1967711299657821655273437500000000000000\n  0.1736439019441604614257812500000000000000\n  -0.1576851308345794677734375000000000000000\n  0.0481917411088943481445312500000000000000\n  0.0591426789760589599609375000000000000000\n  0.2619606852531433105468750000000000000000\n  0.0359058342874050140380859375000000000000\n  0.0336162857711315155029296875000000000000\n  0.0340852960944175720214843750000000000000\n  -0.2455977499485015869140625000000000000000\n  0.5168278813362121582031250000000000000000\n  0.0977833420038223266601562500000000000000\n  -0.1989562064409255981445312500000000000000\n  0.0687516927719116210937500000000000000000\n  0.0700583234429359436035156250000000000000\n  0.1954622268676757812500000000000000000000\n  0.0291600227355957031250000000000000000000\n  0.1483705192804336547851562500000000000000\n  0.0340022817254066467285156250000000000000\n  -0.0687571391463279724121093750000000000000\n  0.1465258300304412841796875000000000000000\n  -0.1173872798681259155273437500000000000000\n  0.0283919312059879302978515625000000000000\n  -0.0492001138627529144287109375000000000000\n  0.0308023057878017425537109375000000000000\n  -0.0029294192790985107421875000000000000000\n  0.3105700910091400146484375000000000000000\n  -0.0452512763440608978271484375000000000000\n  0.0488022230565547943115234375000000000000\n  -0.0220676735043525695800781250000000000000\n  -0.1118313819169998168945312500000000000000\n  -0.0061013381928205490112304687500000000000\n  0.0168982893228530883789062500000000000000\n  -0.1536213159561157226562500000000000000000\n  -3.0622305870056152343750000000000000000000\n  0.1817391365766525268554687500000000000000\n  -0.1741504371166229248046875000000000000000\n  -0.2024996280670166015625000000000000000000\n  0.2552624344825744628906250000000000000000\n  0.2150121927261352539062500000000000000000\n  0.1492541134357452392578125000000000000000\n  0.1937345266342163085937500000000000000000\n  0.1558694839477539062500000000000000000000\n  0.1694414168596267700195312500000000000000\n  -0.0790396779775619506835937500000000000000\n  0.0612936541438102722167968750000000000000\n  0.1389965862035751342773437500000000000000\n  0.0291521996259689331054687500000000000000\n  0.3512175679206848144531250000000000000000\n  0.2464901208877563476562500000000000000000\n  -0.2063876241445541381835937500000000000000\n  0.2125547230243682861328125000000000000000\n  -0.0644317567348480224609375000000000000000\n  0.2248877584934234619140625000000000000000\n  -0.2748226225376129150390625000000000000000\n  0.2461139261722564697265625000000000000000\n  -0.4852201044559478759765625000000000000000\n  0.5104228258132934570312500000000000000000\n  -0.2263766527175903320312500000000000000000\n  -0.0804853811860084533691406250000000000000\n  0.1387754082679748535156250000000000000000\n  0.0524502731859683990478515625000000000000\n  0.2190751880407333374023437500000000000000\n  -0.1175814345479011535644531250000000000000\n  -0.0058161737397313117980957031250000000000\n  0.1410678476095199584960937500000000000000\n  0.1632666289806365966796875000000000000000\n  0.3473671972751617431640625000000000000000\n  0.2403890490531921386718750000000000000000\n  -0.0966564565896987915039062500000000000000\n  0.3043349385261535644531250000000000000000\n  -0.3122731149196624755859375000000000000000\n  0.0513124093413352966308593750000000000000\n  0.2500241696834564208984375000000000000000\n  0.0287626441568136215209960937500000000000\n  0.0001392439007759094238281250000000000000\n  0.1610935926437377929687500000000000000000\n  0.1905403286218643188476562500000000000000\n  0.1453438252210617065429687500000000000000\n  -0.0245148614048957824707031250000000000000\n  0.2192616462707519531250000000000000000000\n  0.0612853243947029113769531250000000000000\n  0.0376033633947372436523437500000000000000\n  -0.1447951346635818481445312500000000000000\n  -0.1963403820991516113281250000000000000000\n  -0.0950965657830238342285156250000000000000\n  0.1975300759077072143554687500000000000000\n  0.0528901964426040649414062500000000000000\n  0.1648199111223220825195312500000000000000\n  -0.0225073415786027908325195312500000000000\n  0.1428186297416687011718750000000000000000\n  -0.5683908462524414062500000000000000000000\n  0.1397426724433898925781250000000000000000\n  -0.2127584069967269897460937500000000000000\n  -0.2969382405281066894531250000000000000000\n  -0.1804187595844268798828125000000000000000\n  -0.0513451509177684783935546875000000000000\n  -2.1090042591094970703125000000000000000000\n  0.0578296780586242675781250000000000000000\n  0.4507547020912170410156250000000000000000\n  -0.1668273508548736572265625000000000000000\n  -0.4279779195785522460937500000000000000000\n  0.1564603447914123535156250000000000000000\n  0.1154951080679893493652343750000000000000\n  0.0910853669047355651855468750000000000000\n  0.2461594790220260620117187500000000000000\n  -0.0238078292459249496459960937500000000000\n  -0.0660580694675445556640625000000000000000\n  -0.0692191570997238159179687500000000000000\n  0.3444389104843139648437500000000000000000\n  0.1370861381292343139648437500000000000000\n  -0.2662808895111083984375000000000000000000\n  -0.0474384203553199768066406250000000000000\n  0.2481715530157089233398437500000000000000\n  0.0695848315954208374023437500000000000000\n  -0.3371189236640930175781250000000000000000\n  0.3451504409313201904296875000000000000000\n  0.3303797543048858642578125000000000000000\n  -0.2035432010889053344726562500000000000000\n  0.0821987986564636230468750000000000000000\n  -0.2760174870491027832031250000000000000000\n  0.0955655872821807861328125000000000000000\n  -0.0709048360586166381835937500000000000000\n  -0.1577297598123550415039062500000000000000\n  0.1525574475526809692382812500000000000000\n  -0.1491569876670837402343750000000000000000\n  0.0781922340393066406250000000000000000000\n  0.0329219326376914978027343750000000000000\n  -0.3559638261795043945312500000000000000000\n  0.0837195217609405517578125000000000000000\n  -2.3815906047821044921875000000000000000000\n  0.0825094729661941528320312500000000000000\n  0.3210685253143310546875000000000000000000\n  -0.0336487367749214172363281250000000000000\n  -0.0356115996837615966796875000000000000000\n  0.3192809224128723144531250000000000000000\n  -0.3872491121292114257812500000000000000000\n  -0.0654220879077911376953125000000000000000\n  0.0253780521452426910400390625000000000000\n  -0.2216758579015731811523437500000000000000\n  -0.2453576624393463134765625000000000000000\n  0.0985448360443115234375000000000000000000\n  -0.1476788669824600219726562500000000000000\n  0.1539402455091476440429687500000000000000\n  0.0359259434044361114501953125000000000000\n  -0.0992926508188247680664062500000000000000\n  0.1759878695011138916015625000000000000000\n  0.1639342755079269409179687500000000000000\n  -0.1473347544670104980468750000000000000000\n  -0.0381556339561939239501953125000000000000\n  -0.2403791695833206176757812500000000000000\n  -0.0276330374181270599365234375000000000000\n  -0.0404206328094005584716796875000000000000\n  -0.1413435637950897216796875000000000000000\n  0.1885788291692733764648437500000000000000\n  0.0908127576112747192382812500000000000000\n  -0.1267680525779724121093750000000000000000\n  -0.2015516310930252075195312500000000000000\n  0.2211744785308837890625000000000000000000\n  -0.1038844585418701171875000000000000000000\n  0.2192855179309844970703125000000000000000\n  -0.2237774431705474853515625000000000000000\n  0.3024950027465820312500000000000000000000\n  0.1685290187597274780273437500000000000000\n  0.0138278640806674957275390625000000000000\n  0.0115488283336162567138671875000000000000\n  -0.0500155314803123474121093750000000000000\n  0.0310265403240919113159179687500000000000\n  -0.3417122364044189453125000000000000000000\n  0.3933438658714294433593750000000000000000\n  0.0002345629036426544189453125000000000000\n  -0.1218004450201988220214843750000000000000\n  -0.2140896618366241455078125000000000000000\n  0.0985682308673858642578125000000000000000\n  0.2328722178936004638671875000000000000000\n  -0.0273962896317243576049804687500000000000\n  0.0966226011514663696289062500000000000000\n  0.3484168648719787597656250000000000000000\n  -0.2724332809448242187500000000000000000000\n  -0.1852111518383026123046875000000000000000\n  0.3814311623573303222656250000000000000000\n  0.0420675724744796752929687500000000000000\n  0.3524355590343475341796875000000000000000\n  0.3444938957691192626953125000000000000000\n  -0.0106769604608416557312011718750000000000\n  0.0172049775719642639160156250000000000000\n  0.1524702310562133789062500000000000000000\n  -0.2245393097400665283203125000000000000000\n  -0.0880812406539916992187500000000000000000\n  0.0370292104780673980712890625000000000000\n  -0.2073345035314559936523437500000000000000\n  0.1425569206476211547851562500000000000000\n  -0.1797641068696975708007812500000000000000\n  2.8669764995574951171875000000000000000000\n  0.2349436730146408081054687500000000000000\n  0.0471095219254493713378906250000000000000\n  0.0550506487488746643066406250000000000000\n  -0.0779394283890724182128906250000000000000\n  0.0514679551124572753906250000000000000000\n  -0.2911300957202911376953125000000000000000\n  0.0684323608875274658203125000000000000000\n  -0.3406119048595428466796875000000000000000\n  0.0082818660885095596313476562500000000000\n  -0.1237611174583435058593750000000000000000\n  0.1743977814912796020507812500000000000000\n  -0.2121240049600601196289062500000000000000\n  -0.0755132213234901428222656250000000000000\n  -0.0992159843444824218750000000000000000000\n  0.0278026629239320755004882812500000000000\n  0.1194203644990921020507812500000000000000\n  0.0384699255228042602539062500000000000000\n  0.3679295480251312255859375000000000000000\n  -0.0547824800014495849609375000000000000000\n  0.0920867398381233215332031250000000000000\n  -0.0398491658270359039306640625000000000000\n  -0.2831836938858032226562500000000000000000\n  0.0414117686450481414794921875000000000000\n  -1.0782839059829711914062500000000000000000\n  0.1803774386644363403320312500000000000000\n  -0.1733969748020172119140625000000000000000\n  0.0371257588267326354980468750000000000000\n  0.0160350538790225982666015625000000000000\n  -0.1645502597093582153320312500000000000000\n  -0.0975515693426132202148437500000000000000\n  -0.0833932906389236450195312500000000000000\n  -0.2543256580829620361328125000000000000000\n  0.2394871413707733154296875000000000000000\n  -0.1699513792991638183593750000000000000000\n  -0.0741059035062789916992187500000000000000\n  0.2566265463829040527343750000000000000000\n  -0.0226761661469936370849609375000000000000\n  0.0000001601874828338623046875000000000000\n  -0.3396089076995849609375000000000000000000\n  0.1311526298522949218750000000000000000000\n  0.0513100773096084594726562500000000000000\n  -0.0795691087841987609863281250000000000000\n  0.0459576100111007690429687500000000000000\n  -0.1282244026660919189453125000000000000000\n  0.3246573507785797119140625000000000000000\n  -0.0705384612083435058593750000000000000000\n  -0.0468430146574974060058593750000000000000\n  -0.0743533670902252197265625000000000000000\n  0.2939557433128356933593750000000000000000\n  0.0475156679749488830566406250000000000000\n  0.0779730975627899169921875000000000000000\n  -0.0034676212817430496215820312500000000000\n  -0.2665706574916839599609375000000000000000\n  -0.1764355450868606567382812500000000000000\n  -0.1350113600492477416992187500000000000000\n  -0.0526813790202140808105468750000000000000\n  0.1633401215076446533203125000000000000000\n  0.2425238490104675292968750000000000000000\n  -0.1748025119304656982421875000000000000000\n  0.0575263947248458862304687500000000000000\n  0.3986152410507202148437500000000000000000\n  -0.3845043778419494628906250000000000000000\n  0.2266369909048080444335937500000000000000\n  -0.1357069015502929687500000000000000000000\n  -0.1134672760963439941406250000000000000000\n  0.0793222337961196899414062500000000000000\n  -0.3684534430503845214843750000000000000000\n  -2.5517024993896484375000000000000000000000\n  -0.1876790821552276611328125000000000000000\n  0.0268291085958480834960937500000000000000\n  0.3709735274314880371093750000000000000000\n  -0.1004799753427505493164062500000000000000\n  -0.0015616677701473236083984375000000000000\n  0.0451832264661788940429687500000000000000\n  0.0476804450154304504394531250000000000000\n  0.2938563823699951171875000000000000000000\n  0.1084204092621803283691406250000000000000\n  -0.0952485799789428710937500000000000000000\n  0.1335937976837158203125000000000000000000\n  0.1499277949333190917968750000000000000000\n  0.0202903114259243011474609375000000000000\n  -0.1766919195652008056640625000000000000000\n  0.5979405641555786132812500000000000000000\n  -0.0643655732274055480957031250000000000000\n  -0.0522403754293918609619140625000000000000\n  0.1378012299537658691406250000000000000000\n  -0.0108505785465240478515625000000000000000\n  0.1064752638339996337890625000000000000000\n  0.0896374061703681945800781250000000000000\n  -0.2927702367305755615234375000000000000000\n  0.2406519204378128051757812500000000000000\n  0.0626429021358489990234375000000000000000\n  0.0631295666098594665527343750000000000000\n  -0.0272324122488498687744140625000000000000\n  -0.3124234080314636230468750000000000000000\n  -0.0058208629488945007324218750000000000000\n  0.0838831663131713867187500000000000000000\n  0.0858991518616676330566406250000000000000\n  -0.0588551983237266540527343750000000000000\n  0.0769425258040428161621093750000000000000\n  -0.2101571261882781982421875000000000000000\n  -0.1798554956912994384765625000000000000000\n  -3.8991343975067138671875000000000000000000\n  -0.0841243118047714233398437500000000000000\n  -0.3109065890312194824218750000000000000000\n  -0.3363238871097564697265625000000000000000\n  0.1374778747558593750000000000000000000000\n  -0.0757333636283874511718750000000000000000\n  0.6187845468521118164062500000000000000000\n  0.0946351736783981323242187500000000000000\n  -0.0044778473675251007080078125000000000000\n  0.0582370124757289886474609375000000000000\n  -0.1444973051548004150390625000000000000000\n  0.1644820421934127807617187500000000000000\n  -0.0836491286754608154296875000000000000000\n  -0.2016696780920028686523437500000000000000\n  -0.1253939270973205566406250000000000000000\n  0.1316413283348083496093750000000000000000\n  0.4500063061714172363281250000000000000000\n  -0.0237902067601680755615234375000000000000\n  0.1079125478863716125488281250000000000000\n  -0.0543914958834648132324218750000000000000\n  -0.2520788908004760742187500000000000000000\n  -0.1798981577157974243164062500000000000000\n  0.0463824272155761718750000000000000000000\n  0.3042268455028533935546875000000000000000\n  0.1902787238359451293945312500000000000000\n  0.1638420075178146362304687500000000000000\n  -0.5458615422248840332031250000000000000000\n  -0.0997942835092544555664062500000000000000\n  0.1134223714470863342285156250000000000000\n  -0.1457127928733825683593750000000000000000\n  0.1705539524555206298828125000000000000000\n  -0.4915557205677032470703125000000000000000\n  -0.2335881441831588745117187500000000000000\n  0.2703528106212615966796875000000000000000\n  -0.2081412225961685180664062500000000000000\n  -0.3163961768150329589843750000000000000000\n  0.0882139578461647033691406250000000000000\n  0.0928660407662391662597656250000000000000\n  0.3915141522884368896484375000000000000000\n  -0.2892025709152221679687500000000000000000\n  0.2065812945365905761718750000000000000000\n  0.3153192400932312011718750000000000000000\n  0.0897682011127471923828125000000000000000\n  0.0121184047311544418334960937500000000000\n  0.3482420146465301513671875000000000000000\n  0.0015491172671318054199218750000000000000\n  0.0766030400991439819335937500000000000000\n  0.3271803557872772216796875000000000000000\n  0.0488697327673435211181640625000000000000\n  0.1207644194364547729492187500000000000000\n  0.0244928151369094848632812500000000000000\n  0.1483182460069656372070312500000000000000\n  1.0175069570541381835937500000000000000000\n  -0.0644881576299667358398437500000000000000\n  0.1171315461397171020507812500000000000000\n  -0.0495755597949028015136718750000000000000\n  0.1144762784242630004882812500000000000000\n  -0.0027465000748634338378906250000000000000\n  -0.1055185347795486450195312500000000000000\n  0.2108231484889984130859375000000000000000\n  0.3655604422092437744140625000000000000000\n  -0.1616371572017669677734375000000000000000\n  0.2074591964483261108398437500000000000000\n  -0.0784822925925254821777343750000000000000\n  0.0419579967856407165527343750000000000000\n  -0.1216160580515861511230468750000000000000\n  0.3067493438720703125000000000000000000000\n  -0.0848599150776863098144531250000000000000\n  -0.0723632425069808959960937500000000000000\n  -0.1142441034317016601562500000000000000000\n  -0.4956518411636352539062500000000000000000\n  0.2294187247753143310546875000000000000000\n  0.1755967438220977783203125000000000000000\n  -0.4226806759834289550781250000000000000000\n  0.1973193287849426269531250000000000000000\n  -0.0957972779870033264160156250000000000000\n  -0.0947855189442634582519531250000000000000\n  0.1950935423374176025390625000000000000000\n  -0.0835177004337310791015625000000000000000\n  0.0650192871689796447753906250000000000000\n  -0.6205918788909912109375000000000000000000\n  0.0465734824538230895996093750000000000000\n  0.1397493630647659301757812500000000000000\n  0.3404798209667205810546875000000000000000\n  -0.0973578914999961853027343750000000000000\n  0.2439913302659988403320312500000000000000\n  0.0326647907495498657226562500000000000000\n  -0.1573407500982284545898437500000000000000\n  -0.1531492918729782104492187500000000000000\n  0.4753258824348449707031250000000000000000\n  -0.1557648032903671264648437500000000000000\n  0.0583147108554840087890625000000000000000\n  0.1667239964008331298828125000000000000000\n  0.0769977718591690063476562500000000000000\n  -0.0679289400577545166015625000000000000000\n  0.0363315492868423461914062500000000000000\n  0.1676276773214340209960937500000000000000\n  -0.2963781058788299560546875000000000000000\n  0.0742815434932708740234375000000000000000\n  -0.3312257826328277587890625000000000000000\n  0.3613545298576354980468750000000000000000\n  -0.3169350326061248779296875000000000000000\n  -0.2414239495992660522460937500000000000000\n  -0.1949927061796188354492187500000000000000\n  -0.1603540778160095214843750000000000000000\n  -0.0796177536249160766601562500000000000000\n  -0.3265589475631713867187500000000000000000\n  0.0690781921148300170898437500000000000000\n  0.0295884665101766586303710937500000000000\n  0.0318698845803737640380859375000000000000\n  -0.1074092015624046325683593750000000000000\n  0.4287797212600708007812500000000000000000\n  -0.2435192465782165527343750000000000000000\n  0.1299955844879150390625000000000000000000\n  1.0238146781921386718750000000000000000000\n  -0.2143792361021041870117187500000000000000\n  -0.2169280350208282470703125000000000000000\n  0.1156790778040885925292968750000000000000\n  0.1776789575815200805664062500000000000000\n  -0.0246520154178142547607421875000000000000\n  -0.0370741710066795349121093750000000000000\n  -0.0464306846261024475097656250000000000000\n  0.1259000003337860107421875000000000000000\n  0.0676974728703498840332031250000000000000\n  -0.1603874564170837402343750000000000000000\n  0.0488508976995944976806640625000000000000\n  0.1837034821510314941406250000000000000000\n  -0.0995243266224861145019531250000000000000\n  -0.2326496243476867675781250000000000000000\n  -0.1940098106861114501953125000000000000000\n  0.1903454214334487915039062500000000000000\n  -0.1117085218429565429687500000000000000000\n  -0.4303445518016815185546875000000000000000\n  -0.2931552529335021972656250000000000000000\n  0.0338390693068504333496093750000000000000\n  -0.0706735998392105102539062500000000000000\n  -0.1745567619800567626953125000000000000000\n  0.1491177678108215332031250000000000000000\n  0.2078863829374313354492187500000000000000\n  0.0110707022249698638916015625000000000000\n  0.2502206265926361083984375000000000000000\n  0.1351188719272613525390625000000000000000\n  0.0088176494464278221130371093750000000000\n  0.3738533556461334228515625000000000000000\n  0.3419964611530303955078125000000000000000\n  0.1289701759815216064453125000000000000000\n  -0.0356254167854785919189453125000000000000\n  0.1762538552284240722656250000000000000000\n  -0.0709580481052398681640625000000000000000\n  0.0885581597685813903808593750000000000000\n  0.0785668641328811645507812500000000000000\n  -0.6704692840576171875000000000000000000000\n  -0.0350090526044368743896484375000000000000\n  -0.0213848799467086791992187500000000000000\n  0.1505078226327896118164062500000000000000\n  0.4524377584457397460937500000000000000000\n  -0.0280276648700237274169921875000000000000\n  -0.0512338802218437194824218750000000000000\n  -0.2739193141460418701171875000000000000000\n  -0.0638110935688018798828125000000000000000\n  0.2777951359748840332031250000000000000000\n  0.0734342560172080993652343750000000000000\n  -1.7045531272888183593750000000000000000000\n  0.1665517240762710571289062500000000000000\n  0.1095723211765289306640625000000000000000\n  -0.0595071837306022644042968750000000000000\n  0.1869225949048995971679687500000000000000\n  -0.0399566292762756347656250000000000000000\n  -0.0547261945903301239013671875000000000000\n  -0.0236073210835456848144531250000000000000\n  0.2145759314298629760742187500000000000000\n  0.1190520524978637695312500000000000000000\n  -0.2314395308494567871093750000000000000000\n  -0.0546537712216377258300781250000000000000\n  0.1413490474224090576171875000000000000000\n  0.0441215522587299346923828125000000000000\n  0.2312214821577072143554687500000000000000\n  0.0307961199432611465454101562500000000000\n  0.0741991922259330749511718750000000000000\n  0.2407593727111816406250000000000000000000\n  -0.0247895941138267517089843750000000000000\n  0.1821978986263275146484375000000000000000\n  -0.1129403710365295410156250000000000000000\n  0.7340980172157287597656250000000000000000\n  0.1650781631469726562500000000000000000000\n  0.1882013231515884399414062500000000000000\n  0.2606922090053558349609375000000000000000\n  -0.2415369451045989990234375000000000000000\n  -0.2262332439422607421875000000000000000000\n  0.4316331148147583007812500000000000000000\n  0.1970994025468826293945312500000000000000\n  -0.0369030907750129699707031250000000000000\n  0.1752468347549438476562500000000000000000\n  -0.3313068151473999023437500000000000000000\n  0.0598050765693187713623046875000000000000\n  -0.0753872543573379516601562500000000000000\n  0.4316236078739166259765625000000000000000\n  0.2221853286027908325195312500000000000000\n  -0.1482722312211990356445312500000000000000\n  0.1987995058298110961914062500000000000000\n  0.0123978871852159500122070312500000000000\n  0.3613434433937072753906250000000000000000\n  -0.3099353015422821044921875000000000000000\n  0.2799298167228698730468750000000000000000\n  0.3859719634056091308593750000000000000000\n  0.1060370653867721557617187500000000000000\n  0.0314808338880538940429687500000000000000\n  0.0807936266064643859863281250000000000000\n  -0.1345767676830291748046875000000000000000\n  -0.3798382580280303955078125000000000000000\n  0.0546457171440124511718750000000000000000\n  0.0594653189182281494140625000000000000000\n  0.1833425313234329223632812500000000000000\n  0.1452979594469070434570312500000000000000\n  0.1014784872531890869140625000000000000000\n  -0.2840036749839782714843750000000000000000\n  -0.2823695540428161621093750000000000000000\n  -0.0585252232849597930908203125000000000000\n  -0.1248067319393157958984375000000000000000\n  -0.1155293658375740051269531250000000000000\n  -0.0530451089143753051757812500000000000000\n  -0.0095920842140913009643554687500000000000\n  0.1202828064560890197753906250000000000000\n  -0.4342721104621887207031250000000000000000\n  0.2034921646118164062500000000000000000000\n  0.1344493776559829711914062500000000000000\n  -0.1161876767873764038085937500000000000000\n  -0.7117428779602050781250000000000000000000\n  0.2850411236286163330078125000000000000000\n  -0.0807340294122695922851562500000000000000\n  -0.1530199795961380004882812500000000000000\n  -0.3078878223896026611328125000000000000000\n  0.3098181486129760742187500000000000000000\n  -0.0241770148277282714843750000000000000000\n  -0.5026336908340454101562500000000000000000\n  0.0674710571765899658203125000000000000000\n  0.0963512808084487915039062500000000000000\n  0.1632209271192550659179687500000000000000\n  0.2733746170997619628906250000000000000000\n  0.3414404690265655517578125000000000000000\n  -0.1693148314952850341796875000000000000000\n  -0.3236549198627471923828125000000000000000\n  0.1368647962808609008789062500000000000000\n  -0.4417271912097930908203125000000000000000\n  -0.2681506276130676269531250000000000000000\n  0.2874839603900909423828125000000000000000\n  0.0925559997558593750000000000000000000000\n  0.0758728981018066406250000000000000000000\n  0.0345133021473884582519531250000000000000\n  0.2277661114931106567382812500000000000000\n  0.0660802945494651794433593750000000000000\n  0.0017738919705152511596679687500000000000\n  -0.2605628371238708496093750000000000000000\n  -0.3189359903335571289062500000000000000000\n  0.0257332660257816314697265625000000000000\n  0.2865821123123168945312500000000000000000\n  0.2743372917175292968750000000000000000000\n  -0.1967374384403228759765625000000000000000\n  -0.0147842951118946075439453125000000000000\n  -0.1402860581874847412109375000000000000000\n  -0.2366952747106552124023437500000000000000\n  0.0012277355417609214782714843750000000000\n  -0.1328446567058563232421875000000000000000\n  -0.0197183787822723388671875000000000000000\n  -0.0201799012720584869384765625000000000000\n  0.1312263309955596923828125000000000000000\n  -0.0821386799216270446777343750000000000000\n  -0.0227123871445655822753906250000000000000\n  0.2959114015102386474609375000000000000000\n  -0.1756496876478195190429687500000000000000\n  -0.2436986565589904785156250000000000000000\n  0.1038982272148132324218750000000000000000\n  -0.1141758337616920471191406250000000000000\n  0.1366646140813827514648437500000000000000\n  0.0778284817934036254882812500000000000000\n  -0.0660053044557571411132812500000000000000\n  0.1666167378425598144531250000000000000000\n  0.0710818916559219360351562500000000000000\n  -0.3563314080238342285156250000000000000000\n  -0.0780594646930694580078125000000000000000\n  1.6740522384643554687500000000000000000000\n  0.1569082736968994140625000000000000000000\n  0.2507076561450958251953125000000000000000\n  0.0579352453351020812988281250000000000000\n  0.2746475636959075927734375000000000000000\n  0.0800773799419403076171875000000000000000\n  0.2047102004289627075195312500000000000000\n  -0.1499496996402740478515625000000000000000\n  -0.1264898031949996948242187500000000000000\n  0.0218252390623092651367187500000000000000\n  -0.0559111088514328002929687500000000000000\n  0.0427498221397399902343750000000000000000\n  -0.1367022097110748291015625000000000000000\n  -0.0323213040828704833984375000000000000000\n  0.1357490420341491699218750000000000000000\n  0.4567792117595672607421875000000000000000\n  -0.0223855413496494293212890625000000000000\n  -0.0871778279542922973632812500000000000000\n  -0.3668156862258911132812500000000000000000\n  -0.1368442326784133911132812500000000000000\n  -0.1175258234143257141113281250000000000000\n  0.0212057419121265411376953125000000000000\n  0.3798660933971405029296875000000000000000\n  -0.0221196003258228302001953125000000000000\n  -0.3780778050422668457031250000000000000000\n  0.0297306533902883529663085937500000000000\n  0.1656568497419357299804687500000000000000\n  -0.1816940307617187500000000000000000000000\n  -0.1889413446187973022460937500000000000000\n  -0.0851093083620071411132812500000000000000\n  -0.0038571795448660850524902343750000000000\n  0.0763948559761047363281250000000000000000\n  0.1589496582746505737304687500000000000000\n  0.0736963078379631042480468750000000000000\n  0.1110440641641616821289062500000000000000\n  -0.0711160898208618164062500000000000000000\n  -0.1549802720546722412109375000000000000000\n  -0.1251936107873916625976562500000000000000\n  -0.1197829842567443847656250000000000000000\n  0.0257638394832611083984375000000000000000\n  -0.0305985137820243835449218750000000000000\n  -0.2210496515035629272460937500000000000000\n  0.2468763142824172973632812500000000000000\n  -0.1778702437877655029296875000000000000000\n  0.0330792218446731567382812500000000000000\n  0.0836480259895324707031250000000000000000\n  0.0884579047560691833496093750000000000000\n  -0.3071962296962738037109375000000000000000\n  0.2750171720981597900390625000000000000000\n  0.5097516775131225585937500000000000000000\n  0.0010456796735525131225585937500000000000\n  -0.0959096625447273254394531250000000000000\n  0.0125840976834297180175781250000000000000\n  0.0593517124652862548828125000000000000000\n  -0.1140958517789840698242187500000000000000\n  0.1298242062330245971679687500000000000000\n  -0.0343137755990028381347656250000000000000\n  -0.3779603242874145507812500000000000000000\n  -0.1619412302970886230468750000000000000000\n  0.3356157243251800537109375000000000000000\n  -0.1488896459341049194335937500000000000000\n  0.5072698593139648437500000000000000000000\n  0.4824952483177185058593750000000000000000\n  -0.3794581294059753417968750000000000000000\n  0.0201909635215997695922851562500000000000\n  0.0185716673731803894042968750000000000000\n  -0.0629366040229797363281250000000000000000\n  0.0520991086959838867187500000000000000000\n  -0.1072030737996101379394531250000000000000\n  -0.0554358512163162231445312500000000000000\n  0.1599606722593307495117187500000000000000\n  0.2296994477510452270507812500000000000000\n  0.2384491413831710815429687500000000000000\n  0.3091536164283752441406250000000000000000\n  0.0114821307361125946044921875000000000000\n  -0.1974284648895263671875000000000000000000\n  -0.0039968416094779968261718750000000000000\n  -0.2103561609983444213867187500000000000000\n  -0.1221612095832824707031250000000000000000\n  -1.9764742851257324218750000000000000000000\n  -0.0272091552615165710449218750000000000000\n  0.2411813586950302124023437500000000000000\n  0.4853417575359344482421875000000000000000\n  0.2131534367799758911132812500000000000000\n  -0.0708608999848365783691406250000000000000\n  -0.0433964431285858154296875000000000000000\n  0.0233258828520774841308593750000000000000\n  -0.0541302636265754699707031250000000000000\n  0.2254400551319122314453125000000000000000\n  0.2183672040700912475585937500000000000000\n  0.3220705389976501464843750000000000000000\n  0.2558275163173675537109375000000000000000\n  0.1151167228817939758300781250000000000000\n  0.0052603753283619880676269531250000000000\n  0.0708802714943885803222656250000000000000\n  -0.0895232409238815307617187500000000000000\n  0.0549466758966445922851562500000000000000\n  -0.2082659900188446044921875000000000000000\n  -0.1637495309114456176757812500000000000000\n  -0.1185555905103683471679687500000000000000\n  0.2642731070518493652343750000000000000000\n  -0.1610702276229858398437500000000000000000\n  -0.0922030732035636901855468750000000000000\n  -0.2039327770471572875976562500000000000000\n  0.4200322031974792480468750000000000000000\n  0.0752066820859909057617187500000000000000\n  -0.3898092806339263916015625000000000000000\n  0.2106011658906936645507812500000000000000\n  0.1533819586038589477539062500000000000000\n  0.0953367352485656738281250000000000000000\n  0.2861562967300415039062500000000000000000\n  -0.0481946170330047607421875000000000000000\n  0.1282767057418823242187500000000000000000\n  0.1374807655811309814453125000000000000000\n  -0.3923578858375549316406250000000000000000\n  0.2805750370025634765625000000000000000000\n  -0.2916547656059265136718750000000000000000\n  -0.1566838920116424560546875000000000000000\n  0.2496568858623504638671875000000000000000\n  -0.0824895799160003662109375000000000000000\n  0.2281562685966491699218750000000000000000\n  0.0221429169178009033203125000000000000000\n  0.2155333310365676879882812500000000000000\n  -0.1149425804615020751953125000000000000000\n  0.1297246515750885009765625000000000000000\n  0.2009250819683074951171875000000000000000\n  -0.2187256515026092529296875000000000000000\n  0.6525444984436035156250000000000000000000\n  -0.2934409379959106445312500000000000000000\n  -0.1280640065670013427734375000000000000000\n  0.0793702602386474609375000000000000000000\n  0.0726855397224426269531250000000000000000\n  0.0679880902171134948730468750000000000000\n  0.1092821732163429260253906250000000000000\n  0.0823530703783035278320312500000000000000\n  0.0733584240078926086425781250000000000000\n  -0.0580668896436691284179687500000000000000\n  -0.0648048371076583862304687500000000000000\n  -0.2628716230392456054687500000000000000000\n  -0.2046963721513748168945312500000000000000\n  0.2010259330272674560546875000000000000000\n  -0.0521624013781547546386718750000000000000\n  0.2473520934581756591796875000000000000000\n  0.0610916316509246826171875000000000000000\n  -0.1580230742692947387695312500000000000000\n  -0.2937031090259552001953125000000000000000\n  0.1559340357780456542968750000000000000000\n  -0.0924995690584182739257812500000000000000\n  -0.1514770686626434326171875000000000000000\n  -0.0448849499225616455078125000000000000000\n  -0.3187240064144134521484375000000000000000\n  0.1261052638292312622070312500000000000000\n  -0.0339522883296012878417968750000000000000\n  0.1215923652052879333496093750000000000000\n  -0.1306513994932174682617187500000000000000\n  0.2115012258291244506835937500000000000000\n  0.3559450209140777587890625000000000000000\n  0.1881848871707916259765625000000000000000\n  0.2887943089008331298828125000000000000000\n  -0.0469298474490642547607421875000000000000\n  -0.0634924918413162231445312500000000000000\n  0.1650983393192291259765625000000000000000\n  -0.1808279156684875488281250000000000000000\n  0.2846289277076721191406250000000000000000\n  -5.3298339843750000000000000000000000000000\n  -0.1683974564075469970703125000000000000000\n  -0.2919130325317382812500000000000000000000\n  0.0027267080731689929962158203125000000000\n  -0.3573650419712066650390625000000000000000\n  -0.3407913446426391601562500000000000000000\n  0.3727463185787200927734375000000000000000\n  -0.1759054660797119140625000000000000000000\n  0.0433866158127784729003906250000000000000\n  -0.1666479855775833129882812500000000000000\n  0.1665399372577667236328125000000000000000\n  -0.0802728533744812011718750000000000000000\n  -0.2272039055824279785156250000000000000000\n  0.1691700518131256103515625000000000000000\n  0.6345713138580322265625000000000000000000\n  0.2318912148475646972656250000000000000000]]');
INSERT INTO `ms_file` (`file_id`, `file_name`, `file_content`, `file_url`, `file_content_vector`) VALUES
(6, 'YOLOv9__Learning_What_You_Want_to_Learn_Using_Programmable_Gradient_Information', '4202beF92]VC.sc[2v61631.2042:viXraYOLOv9: Learning What You Want to LearnUsing Programmable Gradient InformationChien-Yao Wang1,2, I-Hau Yeh2, and Hong-Yuan Mark Liao1,2,31Institute of Information Science, Academia Sinica, Taiwan2National Taipei University of Technology, Taiwan3Department of Information and Computer Engineering, Chung Yuan Christian University, Taiwankinyiu@iis.sinica.edu.tw, ihyeh@emc.com.tw, and liao@iis.sinica.edu.twAbstractToday’s deep learning methods focus on how to designthe most appropriate objective functions so that the pre-diction results of the model can be closest to the groundtruth. Meanwhile, an appropriate architecture that canfacilitate acquisition of enough information for predictionhas to be designed. Existing methods ignore a fact thatwhen input data undergoes layer-by-layer feature extrac-tion and spatial transformation, large amount of informa-tion will be lost. This paper will delve into the important is-sues of data loss when data is transmitted through deep net-works, namely information bottleneck and reversible func-tions. We proposed the concept of programmable gradi-ent information (PGI) to cope with the various changesrequired by deep networks to achieve multiple objectives.PGI can provide complete input information for the tar-get task to calculate objective function, so that reliablegradient information can be obtained to update networkweights. In addition, a new lightweight network architec-ture – Generalized Efficient Layer Aggregation Network(GELAN), based on gradient path planning is designed.GELAN’s architecture confirms that PGI has gained su-perior results on lightweight models. We verified the pro-posed GELAN and PGI on MS COCO dataset based ob-ject detection. The results show that GELAN only usesconventional convolution operators to achieve better pa-rameter utilization than the state-of-the-art methods devel-oped based on depth-wise convolution. PGI can be usedfor variety of models from lightweight to large. It can beused to obtain complete information, so that train-from-scratch models can achieve better results than state-of-the-art models pre-trained using large datasets, the compari-son results are shown in Figure 1. The source codes are at:https://github.com/WongKinYiu/yolov9.1. IntroductionDeep learning-based models have demonstrated far bet-ter performance than past artificial intelligence systems invarious fields, such as computer vision, language process-In recent years, researchersing, and speech recognition.Figure 1. Comparisons of the real-time object detecors on MSCOCO dataset. The GELAN and PGI-based object detectionmethod surpassed all previous train-from-scratch methods in termsof object detection performance. In terms of accuracy, the newmethod outperforms RT DETR [43] pre-trained with a largedataset, and it also outperforms depth-wise convolution-based de-sign YOLO MS [7] in terms of parameters utilization.in the field of deep learning have mainly focused on howto develop more powerful system architectures and learn-ing methods, such as CNNs [21–23, 42, 55, 71, 72], Trans-formers [8, 9, 40, 41, 60, 69, 70], Perceivers [26, 26, 32, 52,56, 81, 81], and Mambas [17, 38, 80].In addition, someresearchers have tried to develop more general objectivefunctions, such as loss function [5, 45, 46, 50, 77, 78], la-bel assignment [10, 12, 33, 67, 79] and auxiliary supervi-sion [18, 20, 24, 28, 29, 51, 54, 68, 76]. The above studiesall try to precisely find the mapping between input and tar-get tasks. However, most past approaches have ignored thatinput data may have a non-negligible amount of informa-tion loss during the feedforward process. This loss of in-formation can lead to biased gradient flows, which are sub-sequently used to update the model. The above problemscan result in deep networks to establish incorrect associa-tions between targets and inputs, causing the trained modelto produce incorrect predictions.1      Figure 2. Visualization results of random initial weight output feature maps for different network architectures: (a) input image, (b)PlainNet, (c) ResNet, (d) CSPNet, and (e) proposed GELAN. From the figure, we can see that in different architectures, the informationprovided to the objective function to calculate the loss is lost to varying degrees, and our architecture can retain the most completeinformation and provide the most reliable gradient information for calculating the objective function.In deep networks, the phenomenon of input data losinginformation during the feedforward process is commonlyknown as information bottleneck [59], and its schematic di-agram is as shown in Figure 2. At present, the main meth-ods that can alleviate this phenomenon are as follows: (1)The use of reversible architectures [3, 16, 19]: this methodmainly uses repeated input data and maintains the informa-tion of the input data in an explicit way; (2) The use ofmasked modeling [1, 6, 9, 27, 71, 73]: it mainly uses recon-struction loss and adopts an implicit way to maximize theextracted features and retain the input information; and (3)Introduction of the deep supervision concept [28,51,54,68]:it uses shallow features that have not lost too much impor-tant information to pre-establish a mapping from featuresto targets to ensure that important information can be trans-ferred to deeper layers. However, the above methods havedifferent drawbacks in the training process and inferenceprocess. For example, a reversible architecture requires ad-ditional layers to combine repeatedly fed input data, whichwill significantly increase the inference cost. In addition,since the input data layer to the output layer cannot have atoo deep path, this limitation will make it difficult to modelhigh-order semantic information during the training pro-cess. As for masked modeling, its reconstruction loss some-times conflicts with the target loss. In addition, most maskmechanisms also produce incorrect associations with data.For the deep supervision mechanism, it will produce erroraccumulation, and if the shallow supervision loses informa-tion during the training process, the subsequent layers willnot be able to retrieve the required information. The abovephenomenon will be more significant on difficult tasks andsmall models.To address the above-mentioned issues, we propose anew concept, which is programmable gradient information(PGI). The concept is to generate reliable gradients throughauxiliary reversible branch, so that the deep features canstill maintain key characteristics for executing target task.The design of auxiliary reversible branch can avoid the se-mantic loss that may be caused by a traditional deep super-vision process that integrates multi-path features. In otherwords, we are programming gradient information propaga-tion at different semantic levels, and thereby achieving thebest training results. The reversible architecture of PGI isbuilt on auxiliary branch, so there is no additional cost.Since PGI can freely select loss function suitable for thetarget task, it also overcomes the problems encountered bymask modeling. The proposed PGI mechanism can be ap-plied to deep neural networks of various sizes and is moregeneral than the deep supervision mechanism, which is onlysuitable for very deep neural networks.In this paper, we also designed generalized ELAN(GELAN) based on ELAN [65], the design of GELAN si-multaneously takes into account the number of parameters,computational complexity, accuracy and inference speed.This design allows users to arbitrarily choose appropriatecomputational blocks for different inference devices. Wecombined the proposed PGI and GELAN, and then de-signed a new generation of YOLO series object detectionsystem, which we call YOLOv9. We used the MS COCOdataset to conduct experiments, and the experimental resultsverified that our proposed YOLOv9 achieved the top perfor-mance in all comparisons.We summarize the contributions of this paper as follows:1. We theoretically analyzed the existing deep neural net-work architecture from the perspective of reversiblefunction, and through this process we successfully ex-plained many phenomena that were difficult to explainin the past. We also designed PGI and auxiliary re-versible branch based on this analysis and achieved ex-cellent results.2. The PGI we designed solves the problem that deep su-pervision can only be used for extremely deep neu-ral network architectures, and therefore allows newlightweight architectures to be truly applied in dailylife.3. The GELAN we designed only uses conventional con-volution to achieve a higher parameter usage than thedepth-wise convolution design that based on the mostadvanced technology, while showing great advantagesof being light, fast, and accurate.4. Combining the proposed PGI and GELAN, the objectdetection performance of the YOLOv9 on MS COCOdataset greatly surpasses the existing real-time objectdetectors in all aspects.22. Related work2.1. Real-time Object DetectorsThe current mainstream real-time object detectors are theYOLO series [2, 7, 13–15, 25, 30, 31, 47–49, 61–63, 74, 75],and most of these models use CSPNet [64] or ELAN [65]and their variants as the main computing units. In terms offeature integration, improved PAN [37] or FPN [35] is of-ten used as a tool, and then improved YOLOv3 head [49] orFCOS head [57, 58] is used as prediction head. Recentlysome real-time object detectors, such as RT DETR [43],which puts its fundation on DETR [4], have also been pro-posed. However, since it is extremely difficult for DETRseries object detector to be applied to new domains withouta corresponding domain pre-trained model, the most widelyused real-time object detector at present is still YOLO se-ries. This paper chooses YOLOv7 [63], which has beenproven effective in a variety of computer vision tasks andvarious scenarios, as a base to develop the proposed method.We use GELAN to improve the architecture and the trainingprocess with the proposed PGI. The above novel approachmakes the proposed YOLOv9 the top real-time object de-tector of the new generation.2.2. Reversible ArchitecturesThe operation unit of reversible architectures [3, 16, 19]must maintain the characteristics of reversible conversion,so it can be ensured that the output feature map of eachlayer of operation unit can retain complete original informa-tion. Before, RevCol [3] generalizes traditional reversibleunit to multiple levels, and in doing so can expand the se-mantic levels expressed by different layer units. Througha literature review of various neural network architectures,we found that there are many high-performing architectureswith varying degree of reversible properties. For exam-ple, Res2Net module [11] combines different input parti-tions with the next partition in a hierarchical manner, andconcatenates all converted partitions before passing thembackwards. CBNet [34, 39] re-introduces the original in-put data through composite backbone to obtain completeoriginal information, and obtains different levels of multi-level reversible information through various compositionmethods. These network architectures generally have ex-cellent parameter utilization, but the extra composite layerscause slow inference speeds. DynamicDet [36] combinesCBNet [34] and the high-efficiency real-time object detec-tor YOLOv7 [63] to achieve a very good trade-off amongspeed, number of parameters, and accuracy. This paper in-troduces the DynamicDet architecture as the basis for de-signing reversible branches.In addition, reversible infor-mation is further introduced into the proposed PGI. Theproposed new architecture does not require additional con-nections during the inference process, so it can fully retainthe advantages of speed, parameter amount, and accuracy.2.3. Auxiliary SupervisionDeep supervision [28, 54, 68] is the most common auxil-iary supervision method, which performs training by insert-ing additional prediction layers in the middle layers. Es-pecially the application of multi-layer decoders introducedin the transformer-based methods is the most common one.Another common auxiliary supervision method is to utilizethe relevant meta information to guide the feature maps pro-duced by the intermediate layers and make them have theproperties required by the target tasks [18, 20, 24, 29, 76].Examples of this type include using segmentation loss ordepth loss to enhance the accuracy of object detectors. Re-cently, there are many reports in the literature [53, 67, 82]that use different label assignment methods to generate dif-ferent auxiliary supervision mechanisms to speed up theconvergence speed of the model and improve the robustnessat the same time. However, the auxiliary supervision mech-anism is usually only applicable to large models, so whenit is applied to lightweight models, it is easy to cause anunder parameterization phenomenon, which makes the per-formance worse. The PGI we proposed designed a way toreprogram multi-level semantic information, and this designallows lightweight models to also benefit from the auxiliarysupervision mechanism.3. Problem StatementUsually, people attribute the difficulty of deep neural net-work convergence problem due to factors such as gradientvanish or gradient saturation, and these phenomena do ex-ist in traditional deep neural networks. However, moderndeep neural networks have already fundamentally solvedthe above problem by designing various normalization andactivation functions. Nevertheless, deep neural networksstill have the problem of slow convergence or poor conver-gence results.In this paper, we explore the nature of the above issuefurther. Through in-depth analysis of information bottle-neck, we deduced that the root cause of this problem is thatthe initial gradient originally coming from a very deep net-work has lost a lot of information needed to achieve thegoal soon after it is transmitted.In order to confirm thisinference, we feedforward deep networks of different archi-tectures with initial weights, and then visualize and illus-trate them in Figure 2. Obviously, PlainNet has lost a lot ofimportant information required for object detection in deeplayers. As for the proportion of important information thatResNet, CSPNet, and GELAN can retain, it is indeed posi-tively related to the accuracy that can be obtained after train-ing. We further design reversible network-based methods tosolve the causes of the above problems. In this section weshall elaborate our analysis of information bottleneck prin-ciple and reversible functions.33.1. Information Bottleneck PrincipleAccording to information bottleneck principle, we knowthat data X may cause information loss when going throughtransformation, as shown in Eq. 1 below:I(X, X) ≥ I(X, fθ(X)) ≥ I(X, gϕ(fθ(X))),(1)where I indicates mutual information, f and g are transfor-mation functions, and θ and ϕ are parameters of f and g,respectively.In deep neural networks, fθ(·) and gϕ(·) respectivelyrepresent the operations of two consecutive layers in deepneural network. From Eq. 1, we can predict that as the num-ber of network layer becomes deeper, the original data willbe more likely to be lost. However, the parameters of thedeep neural network are based on the output of the networkas well as the given target, and then update the network aftergenerating new gradients by calculating the loss function.As one can imagine, the output of a deeper neural networkis less able to retain complete information about the pre-diction target. This will make it possible to use incompleteinformation during network training, resulting in unreliablegradients and poor convergence.One way to solve the above problem is to directly in-crease the size of the model. When we use a large numberof parameters to construct a model, it is more capable ofperforming a more complete transformation of the data. Theabove approach allows even if information is lost during thedata feedforward process, there is still a chance to retainenough information to perform the mapping to the target.The above phenomenon explains why the width is more im-portant than the depth in most modern models. However,the above conclusion cannot fundamentally solve the prob-lem of unreliable gradients in very deep neural network.Below, we will introduce how to use reversible functionsto solve problems and conduct relative analysis.3.2. Reversible FunctionsWhen a function r has an inverse transformation func-tion v, we call this function reversible function, as shown inEq. 2.X = vζ(rψ(X)),(2)where ψ and ζ are parameters of r and v, respectively. DataX is converted by reversible function without losing infor-mation, as shown in Eq. 3.I(X, X) = I(X, rψ(X)) = I(X, vζ(rψ(X))).(3)When the network’s transformation function is composedof reversible functions, more reliable gradients can be ob-tained to update the model. Almost all of today’s populardeep learning methods are architectures that conform to thereversible property, such as Eq. 4.X l+1 = X l + f l+1θ(X l),(4)where l indicates the l-th layer of a PreAct ResNet andf is the transformation function of the l-th layer. PreActResNet [22] repeatedly passes the original data X to sub-sequent layers in an explicit way. Although such a designcan make a deep neural network with more than a thousandlayers converge very well, it destroys an important reasonwhy we need deep neural networks. That is, for difficultproblems, it is difficult for us to directly find simple map-ping functions to map data to targets. This also explainswhy PreAct ResNet performs worse than ResNet [21] whenthe number of layers is small.In addition, we tried to use masked modeling that al-lowed the transformer model to achieve significant break-throughs. We use approximation methods, such as Eq. 5,to try to find the inverse transformation v of r, so that thetransformed features can retain enough information usingsparse features. The form of Eq. 5 is as follows:X = vζ(rψ(X) · M ),(5)where M is a dynamic binary mask. Other methods thatare commonly used to perform the above tasks are diffusionmodel and variational autoencoder, and they both have thefunction of finding the inverse function. However, whenwe apply the above approach to a lightweight model, therewill be defects because the lightweight model will be underparameterized to a large amount of raw data. Because ofthe above reason, important information I(Y, X) that mapsdata X to target Y will also face the same problem. For thisissue, we will explore it using the concept of informationbottleneck [59]. The formula for information bottleneck isas follows:I(X, X) ≥ I(Y, X) ≥ I(Y, fθ(X)) ≥ ... ≥ I(Y, ˆY ). (6)Generally speaking, I(Y, X) will only occupy a very smallpart of I(X, X). However, it is critical to the target mis-sion. Therefore, even if the amount of information lost inthe feedforward stage is not significant, as long as I(Y, X)is covered, the training effect will be greatly affected. Thelightweight model itself is in an under parameterized state,so it is easy to lose a lot of important information in thefeedforward stage. Therefore, our goal for the lightweightmodel is how to accurately filter I(Y, X) from I(X, X). Asfor fully preserving the information of X, that is difficult toachieve. Based on the above analysis, we hope to propose anew deep neural network training method that can not onlygenerate reliable gradients to update the model, but also besuitable for shallow and lightweight neural networks.4(a) Path Aggregation Network (PAN)) [37], (b) Reversible ColumnsFigure 3. PGI and related network architectures and methods.(RevCol) [3], (c) conventional deep supervision, and (d) our proposed Programmable Gradient Information (PGI). PGI is mainly composedof three components: (1) main branch: architecture used for inference, (2) auxiliary reversible branch: generate reliable gradients to supplymain branch for backward transmission, and (3) multi-level auxiliary information: control main branch learning plannable multi-level ofsemantic information.4. Methodology4.1. Programmable Gradient InformationIn order to solve the aforementioned problems, we pro-pose a new auxiliary supervision framework called Pro-grammable Gradient Information (PGI), as shown in Fig-ure 3 (d). PGI mainly includes three components, namely(1) main branch, (2) auxiliary reversible branch, and (3)multi-level auxiliary information. From Figure 3 (d) wesee that the inference process of PGI only uses main branchand therefore does not require any additional inference cost.As for the other two components, they are used to solve orslow down several important issues in deep learning meth-ods. Among them, auxiliary reversible branch is designedto deal with the problems caused by the deepening of neuralnetworks. Network deepening will cause information bot-tleneck, which will make the loss function unable to gener-ate reliable gradients. As for multi-level auxiliary informa-tion, it is designed to handle the error accumulation problemcaused by deep supervision, especially for the architectureand lightweight model of multiple prediction branch. Next,we will introduce these two components step by step.4.1.1 Auxiliary Reversible BranchIn PGI, we propose auxiliary reversible branch to gener-ate reliable gradients and update network parameters. Byproviding information that maps from data to targets, theloss function can provide guidance and avoid the possibil-ity of finding false correlations from incomplete feedfor-ward features that are less relevant to the target. We pro-pose the maintenance of complete information by introduc-ing reversible architecture, but adding main branch to re-versible architecture will consume a lot of inference costs.We analyzed the architecture of Figure 3 (b) and found thatwhen additional connections from deep to shallow layersare added, the inference time will increase by 20%. Whenwe repeatedly add the input data to the high-resolution com-puting layer of the network (yellow box), the inference timeeven exceeds twice the time.Since our goal is to use reversible architecture to ob-tain reliable gradients, “reversible” is not the only neces-sary condition in the inference stage. In view of this, weregard reversible branch as an expansion of deep supervi-sion branch, and then design auxiliary reversible branch, asshown in Figure 3 (d). As for the main branch deep fea-tures that would have lost important information due to in-formation bottleneck, they will be able to receive reliablegradient information from the auxiliary reversible branch.These gradient information will drive parameter learning toassist in extracting correct and important information, andthe above actions can enable the main branch to obtain fea-tures that are more effective for the target task. Moreover,the reversible architecture performs worse on shallow net-works than on general networks because complex tasks re-quire conversion in deeper networks. Our proposed methoddoes not force the main branch to retain complete origi-nal information but updates it by generating useful gradientthrough the auxiliary supervision mechanism. The advan-tage of this design is that the proposed method can also beapplied to shallower networks.5Figure 4. The architecture of GELAN: (a) CSPNet [64], (b) ELAN [65], and (c) proposed GELAN. We imitate CSPNet and extend ELANinto GELAN that can support any computational blocks.Finally, since auxiliary reversible branch can be removedduring the inference phase, the inference capabilities of theoriginal network can be retained. We can also choose anyreversible architectures in PGI to play the role of auxiliaryreversible branch.4.1.2 Multi-level Auxiliary InformationIn this section we will discuss how multi-level auxiliary in-formation works. The deep supervision architecture includ-ing multiple prediction branch is shown in Figure 3 (c). Forobject detection, different feature pyramids can be used toperform different tasks, for example together they can de-tect objects of different sizes. Therefore, after connectingto the deep supervision branch, the shallow features will beguided to learn the features required for small object detec-tion, and at this time the system will regard the positionsof objects of other sizes as the background. However, theabove deed will cause the deep feature pyramids to lose a lotof information needed to predict the target object. Regard-ing this issue, we believe that each feature pyramid needsto receive information about all target objects so that subse-quent main branch can retain complete information to learnpredictions for various targets.The concept of multi-level auxiliary information is to in-sert an integration network between the feature pyramid hi-erarchy layers of auxiliary supervision and the main branch,and then uses it to combine returned gradients from differ-ent prediction heads, as shown in Figure 3 (d). Multi-levelauxiliary information is then to aggregate the gradient infor-mation containing all target objects, and pass it to the mainbranch and then update parameters. At this time, the charac-teristics of the main branch’s feature pyramid hierarchy willnot be dominated by some specific object’s information. Asa result, our method can alleviate the broken informationproblem in deep supervision.In addition, any integratednetwork can be used in multi-level auxiliary information.Therefore, we can plan the required semantic levels to guidethe learning of network architectures of different sizes.4.2. Generalized ELANIn this Section we describe the proposed new networkarchitecture – GELAN. By combining two neural networkarchitectures, CSPNet [64] and ELAN [65], which are de-signed with gradient path planning, we designed gener-alized efficient layer aggregation network (GELAN) thattakes into account lighweight, inference speed, and accu-racy. Its overall architecture is shown in Figure 4. We gen-eralized the capability of ELAN [65], which originally onlyused stacking of convolutional layers, to a new architecturethat can use any computational blocks.5. Experiments5.1. Experimental SetupWe verify the proposed method with MS COCO dataset.All experimental setups follow YOLOv7 AF [63], while thedataset is MS COCO 2017 splitting. All models we men-tioned are trained using the train-from-scratch strategy, andthe total number of training times is 500 epochs. In settingthe learning rate, we use linear warm-up in the first threeepochs, and the subsequent epochs set the correspondingdecay manner according to the model scale. As for the last15 epochs, we turn mosaic data augmentation off. For moresettings, please refer to Appendix.5.2. Implimentation DetailsWe built general and extended version of YOLOv9 basedon YOLOv7 [63] and Dynamic YOLOv7 [36] respectively.In the design of the network architecture, we replacedELAN [65] with GELAN using CSPNet blocks [64] withplanned RepConv [63] as computational blocks. We alsosimplified downsampling module and optimized anchor-free prediction head. As for the auxiliary loss part of PGI,we completely follow YOLOv7’s auxiliary head setting.Please see Appendix for more details.6ModelYOLOv5-N r7.0 [14]YOLOv5-S r7.0 [14]YOLOv5-M r7.0 [14]YOLOv5-L r7.0 [14]YOLOv5-X r7.0 [14]YOLOv6-N v3.0 [30]YOLOv6-S v3.0 [30]YOLOv6-M v3.0 [30]YOLOv6-L v3.0 [30]YOLOv7 [63]YOLOv7-X [63]YOLOv7-N AF [63]YOLOv7-S AF [63]YOLOv7 AF [63]YOLOv8-N [15]YOLOv8-S [15]YOLOv8-M [15]YOLOv8-L [15]YOLOv8-X [15]DAMO YOLO-T [75]DAMO YOLO-S [75]DAMO YOLO-M [75]DAMO YOLO-L [75]Gold YOLO-N [61]Gold YOLO-S [61]Gold YOLO-M [61]Gold YOLO-L [61]YOLO MS-N [7]YOLO MS-S [7]YOLO MS [7]GELAN-S (Ours)GELAN-M (Ours)GELAN-C (Ours)GELAN-E (Ours)YOLOv9-S (Ours)YOLOv9-M (Ours)YOLOv9-C (Ours)YOLOv9-E (Ours)#Param. (M) FLOPs (G) APvalTable 1. Comparison of state-of-the-art real-time object detectors.75 (%) APval50 (%) APval50:95 (%) APval––45.728.0––56.837.4––64.145.4––67.349.0––68.950.74.516.549.0109.1205.71.97.221.246.586.7S (%) APval–––––M (%) APval–––––L (%)4.718.534.959.636.971.33.111.043.63.211.225.943.768.28.512.328.242.15.621.541.375.14.58.122.27.120.025.357.37.120.025.357.311.445.385.8150.7104.7189.98.728.1130.58.728.678.9165.2257.818.137.861.897.312.146.087.5151.717.431.280.226.476.3102.1189.026.476.3102.1189.037.044.349.151.851.252.937.645.153.037.344.950.252.953.942.046.049.250.839.645.449.851.843.446.251.046.751.152.555.046.851.453.055.652.761.266.169.269.771.153.361.870.252.661.867.269.871.058.061.965.567.555.762.567.068.960.463.768.663.067.969.571.963.468.170.272.8––––55.951.440.648.957.5–––57.558.745.249.553.055.5––––47.650.555.750.755.757.360.050.756.157.860.6––––31.836.918.725.735.8–––35.335.723.025.929.733.219.725.332.334.123.726.933.125.933.635.838.026.633.636.240.2––––55.557.741.750.258.7–––58.359.346.150.653.155.744.150.255.357.448.350.556.151.556.457.660.656.057.058.561.0––––65.068.652.861.268.9–––69.870.758.562.566.166.657.062.666.368.260.363.066.564.067.369.470.964.568.069.371.45.3. Comparison with state-of-the-artsTable 1 lists comparison of our proposed YOLOv9 withother train-from-scratch real-time object detectors. Over-all, the best performing methods among existing methodsare YOLO MS-S [7] for lightweight models, YOLO MS [7]for medium models, YOLOv7 AF [63] for general mod-els, and YOLOv8-X [15] for large models. Compared withlightweight and medium model YOLO MS [7], YOLOv9has about 10% less parameters and 5∼15% less calcula-tions, but still has a 0.4∼0.6% improvement in AP. Com-pared with YOLOv7 AF, YOLOv9-C has 42% less pa-rameters and 22% less calculations, but achieves the sameAP (53%). Compared with YOLOv8-X, YOLOv9-E has16% less parameters, 27% less calculations, and has sig-nificant improvement of 1.7% AP. The above comparisonresults show that our proposed YOLOv9 has significantlyimproved in all aspects compared with existing methods.On the other hand, we also include ImageNet pretrainedmodel in the comparison, and the results are shown in Fig-ure 5. We compare them based on the parameters and theamount of computation respectively. In terms of the num-ber of parameters, the best performing large model is RTDETR [43]. From Figure 5, we can see that YOLOv9 usingconventional convolution is even better than YOLO MS us-ing depth-wise convolution in parameter utilization. As forthe parameter utilization of large models, it also greatly sur-passes RT DETR using ImageNet pretrained model. Evenbetter is that in the deep model, YOLOv9 shows the hugeadvantages of using PGI. By accurately retaining and ex-tracting the information needed to map the data to the tar-get, our method requires only 66% of the parameters whilemaintaining the accuracy as RT DETR-X.7Figure 5. Comparison of state-of-the-art real-time object detectors. The methods participating in the comparison all use ImageNet aspre-trained weights, including RT DETR [43], RTMDet [44], and PP-YOLOE [74], etc. The YOLOv9 that uses train-from-scratch methodclearly surpasses the performance of other methods.As for the amount of computation, the best existing mod-els from the smallest to the largest are YOLO MS [7], PPYOLOE [74], and RT DETR [43]. From Figure 5, we cansee that YOLOv9 is far superior to the train-from-scratchmethods in terms of computational complexity.In addi-tion, if compared with those based on depth-wise convo-lution and ImageNet-based pretrained models, YOLOv9 isalso very competitive.5.4. Ablation Studies5.4.1 Generalized ELANFor GELAN, we first do ablation studies for computationalblocks. We used Res blocks [21], Dark blocks [49], andCSP blocks [64] to conduct experiments, respectively. Ta-ble 2 shows that after replacing convolutional layers inELAN with different computational blocks, the system canmaintain good performance. Users are indeed free to re-place computational blocks and use them on their respectiveinference devices. Among different computational block re-placements, CSP blocks perform particularly well. Theynot only reduce the amount of parameters and computation,but also improve AP by 0.7%. Therefore, we choose CSP-ELAN as the component unit of GELAN in YOLOv9.Table 2. Ablation study on various computational blocks.ModelCB type#Param.FLOPsGELAN-SGELAN-SGELAN-SGELAN-SConvRes [21]Dark [49]CSP [64]6.2M5.4M5.7M5.9M23.5G21.0G21.8G22.4GAPval50:9544.8%44.3%44.5%45.5%1 CB type nedotes as computational block type.2 -S nedotes small size model.Next, we conduct ELAN block-depth and CSP block-depth experiments on GELAN of different sizes, and dis-play the results in Table 3. We can see that when the depthof ELAN is increased from 1 to 2, the accuracy is signif-icantly improved. But when the depth is greater than orequal to 2, no matter it is improving the ELAN depth or theCSP depth, the number of parameters, the amount of com-putation, and the accuracy will always show a linear rela-tionship. This means GELAN is not sensitive to the depth.In other words, users can arbitrarily combine the compo-nents in GELAN to design the network architecture, andhave a model with stable performance without special de-sign. In Table 3, for YOLOv9-{S,M,C}, we set the pairingof the ELAN depth and the CSP depth to {{2, 3}, {2, 1},{2, 1}}.Table 3. Ablation study on ELAN and CSP depth.ModelDELAN DCSP#Param.FLOPs APval50:95GELAN-SGELAN-SGELAN-SGELAN-SGELAN-MGELAN-MGELAN-MGELAN-MGELAN-CGELAN-CGELAN-CGELAN-CGELAN-C223222321223212131213112135.9M6.5M7.1M7.1M20.0M22.2M24.3M24.4M18.9M25.3M28.6M31.7M31.9M22.4G24.4G26.3G26.4G76.3G85.1G93.5G94.0G77.5G102.1G114.4G126.8G126.7G45.5%46.0%46.5%46.7%51.1%51.7%51.8%52.3%50.7%52.5%53.0%53.2%53.3%1 DELAN and DCSP respectively nedotes depth of ELAN and CSP.2 -{S, M, C} indicate small, medium, and compact models.85.4.2 Programmable Gradient InformationTable 5. Ablation study on PGI.In terms of PGI, we performed ablation studies on auxiliaryreversible branch and multi-level auxiliary information onthe backbone and neck, respectively. We designed auxiliaryreversible branch ICN to use DHLC [34] linkage to obtainmulti-level reversible information. As for multi-level aux-iliary information, we use FPN and PAN for ablation stud-ies and the role of PFH is equivalent to the traditional deepsupervision. The results of all experiments are listed in Ta-ble 4. From Table 4, we can see that PFH is only effective indeep models, while our proposed PGI can improve accuracyunder different combinations. Especially when using ICN,we get stable and better results. We also tried to apply thelead-head guided assignment proposed in YOLOv7 [63] tothe PGI’s auxiliary supervision, and achieved much betterperformance.Table 4. Ablation study on PGI of backbone and neck.Gbackbone Gneck APvalModelGELAN-CGELAN-CGELAN-CGELAN-CGELAN-CGELAN-CGELAN-C LHG-ICN–PFHFPN–FPNICNGELAN-EGELAN-EGELAN-EGELAN-EGELAN-E–PFHFPNPANFPN–––ICNICN––––––ICNSAPval50:95 APvalM APvalL52.5% 35.8% 57.6% 69.4%52.5% 35.3% 58.1% 68.9%52.6% 35.3% 58.1% 68.9%52.7% 35.3% 58.4% 68.9%52.8% 35.8% 58.2% 69.1%52.9% 35.2% 58.7% 68.6%53.0% 36.3% 58.5% 69.1%55.0% 38.0% 60.6% 70.9%55.3% 38.3% 60.3% 71.6%55.6% 40.2% 61.0% 71.4%55.5% 39.0% 61.1% 71.5%55.6% 39.8% 60.9% 71.9%ModelGELAN-S+ DS+ PGIAPval50:9546.7%46.5%-0.246.8% +0.1APval5063.0%62.9% -0.163.4% +0.4APval7550.7%50.5% -0.250.7%=GELAN-M 51.1%+ DS51.2%+0.151.4% +0.3+ PGI67.9%68.2% +0.368.1% +0.255.7%55.7%56.1% +0.4=GELAN-C+ DS+ PGIGELAN-E+ DS+ PGI52.5%52.5%53.0% +0.5=69.5%69.9% +0.470.3% +0.857.3%57.1% -0.257.8% +0.555.0%55.3%+0.355.6% +0.671.9%72.3% +0.472.8% +0.960.0%60.2% +0.260.6% +0.61 DS indicates deep supervision.2 -{S, M, C, E} indicate small, medium, compact, and extended models.Finally, we show in the table the results of gradually in-creasing components from baseline YOLOv7 to YOLOv9-E. The GELAN and PGI we proposed have brought all-round improvement to the model.Table 6. Ablation study on GELAN and PGI.ModelYOLOv7 [63]+ AF [63]+ GELAN+ DHLC [34]+ PGISAPval50:95 APval#Param. FLOPs APval104.7130.5126.4189.0189.0M APvalL51.2% 31.8% 55.5% 65.0%53.0% 35.8% 58.7% 68.9%53.2% 36.2% 58.5% 69.9%55.0% 38.0% 60.6% 70.9%55.6% 40.2% 61.0% 71.4%36.943.641.257.357.31 DELAN and DCSP respectively nedotes depth of ELAN and CSP.2 LHG indicates lead head guided training proposed by YOLOv7 [63].We further implemented the concepts of PGI and deepsupervision on models of various sizes and compared theresults, these results are shown in Table 5. As analyzed atthe beginning, introduction of deep supervision will causea loss of accuracy for shallow models. As for general mod-els, introducing deep supervision will cause unstable perfor-mance, and the design concept of deep supervision can onlybring gains in extremely deep models. The proposed PGIcan effectively handle problems such as information bottle-neck and information broken, and can comprehensively im-prove the accuracy of models of different sizes. The conceptof PGI brings two valuable contributions. The first one is tomake the auxiliary supervision method applicable to shal-low models, while the second one is to make the deep modeltraining process obtain more reliable gradients. These gra-dients enable deep models to use more accurate informationto establish correct correlations between data and targets.5.5. VisualizationThis section will explore the information bottleneck is-sues and visualize them. In addition, we will also visualizehow the proposed PGI uses reliable gradients to find thecorrect correlations between data and targets. In Figure 6we show the visualization results of feature maps obtainedby using random initial weights as feedforward under dif-ferent architectures. We can see that as the number of lay-ers increases, the original information of all architecturesgradually decreases. For example, at the 50th layer of thePlainNet, it is difficult to see the location of objects, and alldistinguishable features will be lost at the 100th layer. Asfor ResNet, although the position of object can still be seenat the 50th layer, the boundary information has been lost.When the depth reached to the 100th layer, the whole imagebecomes blurry. Both CSPNet and the proposed GELANperform very well, and they both can maintain features thatsupport clear identification of objects until the 200th layer.Among the comparisons, GELAN has more stable resultsand clearer boundary information.9Figure 6. Feature maps (visualization results) output by random initial weights of PlainNet, ResNet, CSPNet, and GELAN at differentdepths. After 100 layers, ResNet begins to produce feedforward output that is enough to obfuscate object information. Our proposedGELAN can still retain quite complete information up to the 150th layer, and is still sufficiently discriminative up to the 200th layer.ject boundaries, and it also produced unexpected responsesin some background areas. This experiment confirms thatPGI can indeed provide better gradients to update parame-ters and enable the feedforward stage of the main branch toretain more important features.6. ConclusionsIn this paper, we propose to use PGI to solve the infor-mation bottleneck problem and the problem that the deepsupervision mechanism is not suitable for lightweight neu-ral networks. We designed GELAN, a highly efficientand lightweight neural network. In terms of object detec-tion, GELAN has strong and stable performance at differentcomputational blocks and depth settings. It can indeed bewidely expanded into a model suitable for various inferencedevices. For the above two issues, the introduction of PGIallows both lightweight models and deep models to achievesignificant improvements in accuracy. The YOLOv9, de-signed by combining PGI and GELAN, has shown strongcompetitiveness. Its excellent design allows the deep modelto reduce the number of parameters by 49% and the amountof calculations by 43% compared with YOLOv8, but it stillhas a 0.6% AP improvement on MS COCO dataset.7. AcknowledgementsThe authors wish to thank National Center for High-performance Computing (NCHC) for providing computa-tional and storage resources.Figure 7. PAN feature maps (visualization results) of GELANand YOLOv9 (GELAN + PGI) after one epoch of bias warm-up.GELAN originally had some divergence, but after adding PGI’sreversible branch, it is more capable of focusing on the target ob-ject.Figure 7 is used to show whether PGI can provide morereliable gradients during the training process, so that theparameters used for updating can effectively capture therelationship between the input data and the target. Fig-ure 7 shows the visualization results of the feature map ofGELAN and YOLOv9 (GELAN + PGI) in PAN bias warm-up. From the comparison of Figure 7(b) and (c), we canclearly see that PGI accurately and concisely captures thearea containing objects. As for GELAN that does not usePGI, we found that it had divergence when detecting ob-10References[1] Hangbo Bao, Li Dong, Songhao Piao, and Furu Wei. BEiT:BERT pre-training of image transformers. In InternationalConference on Learning Representations (ICLR), 2022. 2[2] Alexey Bochkovskiy, Chien-Yao Wang,and Hong-Yuan Mark Liao. YOLOv4: Optimal speed and accuracy ofobject detection. arXiv preprint arXiv:2004.10934, 2020. 3[3] Yuxuan Cai, Yizhuang Zhou, Qi Han, Jianjian Sun, Xiang-wen Kong, Jun Li, and Xiangyu Zhang. Reversible columnnetworks. In International Conference on Learning Repre-sentations (ICLR), 2023. 2, 3, 5[4] Nicolas Carion, Francisco Massa, Gabriel Synnaeve, NicolasUsunier, Alexander Kirillov, and Sergey Zagoruyko. End-In Proceedingsto-end object detection with transformers.of the European Conference on Computer Vision (ECCV),pages 213–229, 2020. 3[5] Kean Chen, Weiyao Lin, Jianguo Li, John See, Ji Wang, andJunni Zou. AP-loss for accurate one-stage object detection.IEEE Transactions on Pattern Analysis and Machine Intelli-gence (TPAMI), 43(11):3782–3798, 2020. 1[6] Yabo Chen, Yuchen Liu, Dongsheng Jiang, Xiaopeng Zhang,Wenrui Dai, Hongkai Xiong, and Qi Tian. SdAE: Self-distillated masked autoencoder. In Proceedings of the Euro-pean Conference on Computer Vision (ECCV), pages 108–124, 2022. 2[7] Yuming Chen, Xinbin Yuan, Ruiqi Wu, Jiabao Wang, QibinHou, and Ming-Ming Cheng. YOLO-MS: rethinking multi-scale representation learning for real-time object detection.arXiv preprint arXiv:2308.05480, 2023. 1, 3, 7, 8[8] Mingyu Ding, Bin Xiao, Noel Codella, Ping Luo, JingdongWang, and Lu Yuan. DaVIT: Dual attention vision trans-In Proceedings of the European Conference onformers.Computer Vision (ECCV), pages 74–92, 2022. 1[9] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov,Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner,Mostafa Dehghani, Matthias Minderer, Georg Heigold, Syl-vain Gelly, et al. An image is worth 16x16 words: Trans-formers for image recognition at scale. In International Con-ference on Learning Representations (ICLR), 2021. 1, 2[10] Chengjian Feng, Yujie Zhong, Yu Gao, Matthew R Scott,and Weilin Huang. TOOD: Task-aligned one-stage objectIn Proceedings of the IEEE/CVF Internationaldetection.Conference on Computer Vision (ICCV), pages 3490–3499,2021. 1[11] Shang-Hua Gao, Ming-Ming Cheng, Kai Zhao, Xin-YuZhang, Ming-Hsuan Yang, and Philip Torr. Res2Net: AIEEE Transac-new multi-scale backbone architecture.tions on Pattern Analysis and Machine Intelligence (TPAMI),43(2):652–662, 2019. 3[12] Zheng Ge, Songtao Liu, Zeming Li, Osamu Yoshie, and JianSun. OTA: Optimal transport assignment for object detec-tion. In Proceedings of the IEEE/CVF Conference on Com-puter Vision and Pattern Recognition (CVPR), pages 303–312, 2021. 1[13] Zheng Ge, Songtao Liu, Feng Wang, Zeming Li, and JianSun. YOLOX: Exceeding YOLO series in 2021. arXivpreprint arXiv:2107.08430, 2021. 3[14] Jocher Glenn. YOLOv5 release v7.0. https://github.com/ultralytics/yolov5/releases/tag/v7.0, 2022. 3, 7[15] Jocher Glenn.https :/ / github . com / ultralytics / ultralytics /releases/tag/v8.1.0, 2024. 3, 7YOLOv8 release v8.1.0.[16] Aidan N Gomez, Mengye Ren, Raquel Urtasun, and Roger BGrosse. The reversible residual network: Backpropagationwithout storing activations. Advances in Neural InformationProcessing Systems (NeurIPS), 2017. 2, 3[17] Albert Gu and Tri Dao. Mamba: Linear-time sequencearXiv preprintmodeling with selective state spaces.arXiv:2312.00752, 2023. 1[18] Chaoxu Guo, Bin Fan, Qian Zhang, Shiming Xiang, andAugFPN: Improving multi-scale fea-Chunhong Pan.In Proceedings of theture learning for object detection.IEEE/CVF Conference on Computer Vision and PatternRecognition (CVPR), pages 12595–12604, 2020. 1, 3[19] Qi Han, Yuxuan Cai, and Xiangyu Zhang. RevColV2: Ex-ploring disentangled representations in masked image mod-eling. Advances in Neural Information Processing Systems(NeurIPS), 2023. 2, 3[20] Zeeshan Hayder, Xuming He, and Mathieu Salzmann.In Proceedings ofBoundary-aware instance segmentation.the IEEE/CVF Conference on Computer Vision and PatternRecognition (CVPR), pages 5696–5704, 2017. 1, 3[21] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.Deep residual learning for image recognition. In Proceed-ings of the IEEE/CVF Conference on Computer Vision andPattern Recognition (CVPR), pages 770–778, 2016. 1, 4, 8[22] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.Identity mappings in deep residual networks. In Proceedingsof the European Conference on Computer Vision (ECCV),pages 630–645. Springer, 2016. 1, 4[23] Gao Huang, Zhuang Liu, Laurens Van Der Maaten, and Kil-ian Q Weinberger. Densely connected convolutional net-In Proceedings of the IEEE/CVF Conference onworks.Computer Vision and Pattern Recognition (CVPR), pages4700–4708, 2017. 1[24] Kuan-Chih Huang, Tsung-Han Wu, Hung-Ting Su, and Win-ston H Hsu. MonoDTR: Monocular 3D object detection withdepth-aware transformer. In Proceedings of the IEEE/CVFConference on Computer Vision and Pattern Recognition(CVPR), pages 4012–4021, 2022. 1, 3[25] Lin Huang, Weisheng Li, Linlin Shen, Haojie Fu, Xue Xiao,and Suihan Xiao. YOLOCS: Object detection based on densechannel compression for feature spatial solidification. arXivpreprint arXiv:2305.04170, 2023. 3[26] Andrew Jaegle, Felix Gimeno, Andy Brock, Oriol Vinyals,Andrew Zisserman, and Joao Carreira. Perceiver: Generalperception with iterative attention. In International Confer-ence on Machine Learning (ICML), pages 4651–4664, 2021.1[27] Jacob Devlin Ming-Wei Chang Kenton and Lee KristinaToutanova. BERT: Pre-training of deep bidirectional trans-In Proceedings offormers for language understanding.NAACL-HLT, volume 1, page 2, 2019. 211[28] Chen-Yu Lee, Saining Xie, Patrick Gallagher, ZhengyouIn Ar-Zhang, and Zhuowen Tu. Deeply-supervised nets.tificial Intelligence and Statistics, pages 562–570, 2015. 1,2, 3[29] Alex Levinshtein, Alborz Rezazadeh Sereshkeh, and Kon-stantinos Derpanis. DATNet: Dense auxiliary tasks for ob-ject detection. In Proceedings of the IEEE/CVF Winter Con-ference on Applications of Computer Vision (WACV), pages1419–1427, 2020. 1, 3[30] Chuyi Li, Lulu Li, Yifei Geng, Hongliang Jiang, MengCheng, Bo Zhang, Zaidan Ke, Xiaoming Xu, and Xiangx-iang Chu. YOLOv6 v3.0: A full-scale reloading. arXivpreprint arXiv:2301.05586, 2023. 3, 7, 2, 4[31] Chuyi Li, Lulu Li, Hongliang Jiang, Kaiheng Weng, YifeiGeng, Liang Li, Zaidan Ke, Qingyuan Li, Meng Cheng,Weiqiang Nie, et al. YOLOv6: A single-stage object de-tection framework for industrial applications. arXiv preprintarXiv:2209.02976, 2022. 3[32] Hao Li, Jinguo Zhu, Xiaohu Jiang, Xizhou Zhu, HongshengLi, Chun Yuan, Xiaohua Wang, Yu Qiao, Xiaogang Wang,Wenhai Wang, et al. Uni-perceiver v2: A generalist modelfor large-scale vision and vision-language tasks. In Proceed-ings of the IEEE/CVF Conference on Computer Vision andPattern Recognition (CVPR), pages 2691–2700, 2023. 1[33] Shuai Li, Chenhang He, Ruihuang Li, and Lei Zhang. Adual weighting label assignment scheme for object detection.In Proceedings of the IEEE/CVF Conference on ComputerVision and Pattern Recognition (CVPR), pages 9387–9396,2022. 1[34] Tingting Liang, Xiaojie Chu, Yudong Liu, Yongtao Wang,Zhi Tang, Wei Chu, Jingdong Chen, and Haibin Ling. CB-Net: A composite backbone network architecture for objectIEEE Transactions on Image Processing (TIP),detection.2022. 3, 9[35] Tsung-Yi Lin, Piotr Doll´ar, Ross Girshick, Kaiming He,Feature pyra-Bharath Hariharan, and Serge Belongie.In Proceedings of themid networks for object detection.IEEE/CVF Conference on Computer Vision and PatternRecognition (CVPR), pages 2117–2125, 2017. 3[36] Zhihao Lin, Yongtao Wang, Jinhe Zhang, and Xiaojie Chu.DynamicDet: A unified dynamic architecture for object de-In Proceedings of the IEEE/CVF Conference ontection.Computer Vision and Pattern Recognition (CVPR), pages6282–6291, 2023. 3, 6, 2, 4[37] Shu Liu, Lu Qi, Haifang Qin, Jianping Shi, and Jiaya Jia.Path aggregation network for instance segmentation. In Pro-ceedings of the IEEE/CVF Conference on Computer Visionand Pattern Recognition (CVPR), pages 8759–8768, 2018.3, 5[38] Yue Liu, Yunjie Tian, Yuzhong Zhao, Hongtian Yu, LingxiXie, Yaowei Wang, Qixiang Ye, and Yunfan Liu. Vmamba:Visual state space model. arXiv preprint arXiv:2401.10166,2024. 1[39] Yudong Liu, Yongtao Wang, Siwei Wang, TingTing Liang,Qijie Zhao, Zhi Tang, and Haibin Ling. CBNet: A novelcomposite backbone network architecture for object detec-In Proceedings of the AAAI Conference on Artificialtion.Intelligence (AAAI), pages 11653–11660, 2020. 3[40] Ze Liu, Han Hu, Yutong Lin, Zhuliang Yao, Zhenda Xie,Yixuan Wei, Jia Ning, Yue Cao, Zheng Zhang, Li Dong, et al.Swin transformer v2: Scaling up capacity and resolution. InProceedings of the IEEE/CVF Conference on Computer Vi-sion and Pattern Recognition (CVPR), 2022. 1[41] Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, ZhengZhang, Stephen Lin, and Baining Guo. Swin transformer:Hierarchical vision transformer using shifted windows.InProceedings of the IEEE/CVF International Conference onComputer Vision (ICCV), pages 10012–10022, 2021. 1[42] Zhuang Liu, Hanzi Mao, Chao-Yuan Wu, Christoph Feicht-enhofer, Trevor Darrell, and Saining Xie. A ConvNet for the2020s. In Proceedings of the IEEE/CVF Conference on Com-puter Vision and Pattern Recognition (CVPR), pages 11976–11986, 2022. 1[43] Wenyu Lv, Shangliang Xu, Yian Zhao, Guanzhong Wang,Jinman Wei, Cheng Cui, Yuning Du, Qingqing Dang, andYi Liu. DETRs beat YOLOs on real-time object detection.arXiv preprint arXiv:2304.08069, 2023. 1, 3, 7, 8, 2, 4[44] Chengqi Lyu, Wenwei Zhang, Haian Huang, Yue Zhou,Yudong Wang, Yanyi Liu, Shilong Zhang, and Kai Chen.RTMDet: An empirical study of designing real-time objectdetectors. arXiv preprint arXiv:2212.07784, 2022. 8, 2, 3, 4[45] Kemal Oksuz, Baris Can Cam, Emre Akbas, and SinanKalkan. A ranking-based, balanced loss function unify-ing classification and localisation in object detection. Ad-vances in Neural Information Processing Systems (NeurIPS),33:15534–15545, 2020. 1[46] Kemal Oksuz, Baris Can Cam, Emre Akbas, and SinanKalkan. Rank & sort loss for object detection and instanceIn Proceedings of the IEEE/CVF Interna-segmentation.tional Conference on Computer Vision (ICCV), pages 3009–3018, 2021. 1[47] Joseph Redmon, Santosh Divvala, Ross Girshick, and AliFarhadi. You only look once: Unified, real-time object detec-tion. In Proceedings of the IEEE/CVF Conference on Com-puter Vision and Pattern Recognition (CVPR), pages 779–788, 2016. 3[48] Joseph Redmon and Ali Farhadi. YOLO9000: better, faster,In Proceedings of the IEEE/CVF Conference onstronger.Computer Vision and Pattern Recognition (CVPR), pages7263–7271, 2017. 3[49] Joseph Redmon and Ali Farhadi. YOLOv3: An incrementalimprovement. arXiv preprint arXiv:1804.02767, 2018. 3, 8[50] Hamid Rezatofighi, Nathan Tsoi, JunYoung Gwak, AmirSadeghian, Ian Reid, and Silvio Savarese. Generalized in-tersection over union: A metric and a loss for bounding boxIn Proceedings of the IEEE/CVF Conferenceregression.on Computer Vision and Pattern Recognition (CVPR), pages658–666, 2019. 1[51] Zhiqiang Shen, Zhuang Liu, Jianguo Li, Yu-Gang Jiang,Yurong Chen, and Xiangyang Xue. Object detection fromscratch with deep supervision. IEEE Transactions on PatternAnalysis and Machine Intelligence (TPAMI), 42(2):398–412,2019. 1, 2[52] Mohit Shridhar, Lucas Manuelli, and Dieter Fox. Perceiver-actor: A multi-task transformer for robotic manipulation.12In Conference on Robot Learning (CoRL), pages 785–799,2023. 1[53] Peize Sun, Yi Jiang, Enze Xie, Wenqi Shao, Zehuan Yuan,Changhu Wang, and Ping Luo. What makes for end-to-endobject detection? In International Conference on MachineLearning (ICML), pages 9934–9944, 2021. 3[54] Christian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet,Scott Reed, Dragomir Anguelov, Dumitru Erhan, VincentVanhoucke, and Andrew Rabinovich. Going deeper withconvolutions. In Proceedings of the IEEE/CVF Conferenceon Computer Vision and Pattern Recognition (CVPR), pages1–9, 2015. 1, 2, 3[55] Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, JonShlens, and Zbigniew Wojna. Rethinking the inception archi-tecture for computer vision. In Proceedings of the IEEE/CVFConference on Computer Vision and Pattern Recognition(CVPR), pages 2818–2826, 2016. 1[56] Zineng Tang, Jaemin Cho, Jie Lei, and Mohit Bansal.Perceiver-VL: Efficient vision-and-language modeling withIn Proceedings of the IEEE/CVFiterative latent attention.Winter Conference on Applications of Computer Vision(WACV), pages 4410–4420, 2023. 1[57] Zhi Tian, Chunhua Shen, Hao Chen, and Tong He. FCOS:Fully convolutional one-stage object detection. In Proceed-ings of the IEEE/CVF International Conference on Com-puter Vision (ICCV), pages 9627–9636, 2019. 3[58] Zhi Tian, Chunhua Shen, Hao Chen, and Tong He. FCOS:IEEEA simple and strong anchor-free object detector.Transactions on Pattern Analysis and Machine Intelligence(TPAMI), 44(4):1922–1933, 2022. 3[59] Naftali Tishby and Noga Zaslavsky. Deep learning and theinformation bottleneck principle. In IEEE Information The-ory Workshop (ITW), pages 1–5, 2015. 2, 4[60] Zhengzhong Tu, Hossein Talebi, Han Zhang, Feng Yang,Peyman Milanfar, Alan Bovik, and Yinxiao Li. MaxVIT:Multi-axis vision transformer. In Proceedings of the Euro-pean Conference on Computer Vision (ECCV), pages 459–479, 2022. 1[61] Chengcheng Wang, Wei He, Ying Nie, Jianyuan Guo,Chuanjian Liu, Kai Han, and Yunhe Wang. Gold-YOLO:Efficient object detector via gather-and-distribute mecha-nism. Advances in Neural Information Processing Systems(NeurIPS), 2023. 3, 7, 2, 4[62] Chien-Yao Wang, Alexey Bochkovskiy,and Hong-Yuan Mark Liao. Scaled-YOLOv4: Scaling cross stageIn Proceedings of the IEEE/CVF Confer-partial network.ence on Computer Vision and Pattern Recognition (CVPR),pages 13029–13038, 2021. 3[63] Chien-Yao Wang, Alexey Bochkovskiy,and Hong-Yuan Mark Liao. YOLOv7: Trainable bag-of-freebiessets new state-of-the-art for real-time object detectors.InProceedings of the IEEE/CVF Conference on ComputerVision and Pattern Recognition (CVPR), pages 7464–7475,2023. 3, 6, 7, 9, 1In Proceedings of the IEEE/CVF Conference on ComputerVision and Pattern Recognition Workshops (CVPRW), pages390–391, 2020. 3, 6, 8[65] Chien-Yao Wang, Hong-Yuan Mark Liao, and I-Hau Yeh.Designing network design strategies through gradient pathanalysis. Journal of Information Science and Engineering(JISE), 39(4):975–995, 2023. 2, 3, 6[66] Chien-Yao Wang, I-Hau Yeh, and Hong-Yuan Mark Liao.You only learn one representation: Unified network for mul-tiple tasks. Journal of Information Science & Engineering(JISE), 39(3):691–709, 2023. 2, 3, 4[67] Jianfeng Wang, Lin Song, Zeming Li, Hongbin Sun, JianEnd-to-end object detectionSun, and Nanning Zheng.In Proceedings of thewith fully convolutional network.IEEE/CVF Conference on Computer Vision and PatternRecognition (CVPR), pages 15849–15858, 2021. 1, 3[68] Liwei Wang, Chen-Yu Lee, Zhuowen Tu, and SvetlanaLazebnik. Training deeper convolutional networks with deepsupervision. arXiv preprint arXiv:1505.02496, 2015. 1, 2, 3[69] Wenhai Wang, Enze Xie, Xiang Li, Deng-Ping Fan, KaitaoSong, Ding Liang, Tong Lu, Ping Luo, and Ling Shao.Pyramid vision transformer: A versatile backbone for denseIn Proceedings of theprediction without convolutions.IEEE/CVF International Conference on Computer Vision(ICCV), pages 568–578, 2021. 1[70] Wenhai Wang, Enze Xie, Xiang Li, Deng-Ping Fan, KaitaoSong, Ding Liang, Tong Lu, Ping Luo, and Ling Shao. PVTImproved baselines with pyramid vision transformer.v2:Computational Visual Media, 8(3):415–424, 2022. 1[71] Sanghyun Woo, Shoubhik Debnath, Ronghang Hu, XinleiChen, Zhuang Liu, In So Kweon, and Saining Xie. Con-vNeXt v2: Co-designing and scaling convnets with maskedautoencoders. In Proceedings of the IEEE/CVF Conferenceon Computer Vision and Pattern Recognition (CVPR), pages16133–16142, 2023. 1, 2[72] Saining Xie, Ross Girshick, Piotr Doll´ar, Zhuowen Tu, andKaiming He. Aggregated residual transformations for deepneural networks. In Proceedings of the IEEE/CVF Confer-ence on Computer Vision and Pattern Recognition (CVPR),pages 1492–1500, 2017. 1[73] Zhenda Xie, Zheng Zhang, Yue Cao, Yutong Lin, JianminBao, Zhuliang Yao, Qi Dai, and Han Hu. SimMIM: A simpleframework for masked image modeling. In Proceedings ofthe IEEE/CVF Conference on Computer Vision and PatternRecognition (CVPR), pages 9653–9663, 2022. 2[74] Shangliang Xu, Xinxin Wang, Wenyu Lv, Qinyao Chang,Cheng Cui, Kaipeng Deng, Guanzhong Wang, QingqingDang, Shengyu Wei, Yuning Du, et al. PP-YOLOE: Anevolved version of YOLO. arXiv preprint arXiv:2203.16250,2022. 3, 8, 2, 4[75] Xianzhe Xu, Yiqi Jiang, Weihua Chen, Yilun Huang,Yuan Zhang, and Xiuyu Sun.DAMO-YOLO: A re-port on real-time object detection design. arXiv preprintarXiv:2211.15444, 2022. 3, 7, 2, 4[64] Chien-Yao Wang, Hong-Yuan Mark Liao, Yueh-Hua Wu,Ping-Yang Chen, Jun-Wei Hsieh, and I-Hau Yeh. CSPNet: Anew backbone that can enhance learning capability of CNN.[76] Renrui Zhang, Han Qiu, Tai Wang, Ziyu Guo, Ziteng Cui, YuQiao, Hongsheng Li, and Peng Gao. MonoDETR: Depth-Inguided transformer for monocular 3D object detection.13Proceedings of the IEEE/CVF International Conference onComputer Vision (ICCV), pages 9155–9166, 2023. 1, 3[77] Zhaohui Zheng, Ping Wang, Wei Liu, Jinze Li, RongguangYe, and Dongwei Ren. Distance-IoU loss: Faster and bet-ter learning for bounding box regression. In Proceedings ofthe AAAI Conference on Artificial Intelligence (AAAI), vol-ume 34, pages 12993–13000, 2020. 1[78] Dingfu Zhou, Jin Fang, Xibin Song, Chenye Guan, JunboYin, Yuchao Dai, and Ruigang Yang.IoU loss for 2D/3Dobject detection. In International Conference on 3D Vision(3DV), pages 85–94, 2019. 1[79] Benjin Zhu, Jianfeng Wang, Zhengkai Jiang, Fuhang Zong,Songtao Liu, Zeming Li, and Jian Sun. AutoAssign: Differ-entiable label assignment for dense object detection. arXivpreprint arXiv:2007.03496, 2020. 1[80] Lianghui Zhu, Bencheng Liao, Qian Zhang, Xinlong Wang,Wenyu Liu, and Xinggang Wang. Vision mamba: Efficientvisual representation learning with bidirectional state spacemodel. arXiv preprint arXiv:2401.09417, 2024. 1[81] Xizhou Zhu, Jinguo Zhu, Hao Li, Xiaoshi Wu, HongshengLi, Xiaohua Wang, and Jifeng Dai. Uni-perceiver: Pre-training unified architecture for generic perception for zero-In Proceedings of the IEEE/CVFshot and few-shot tasks.Conference on Computer Vision and Pattern Recognition(CVPR), pages 16804–16815, 2022. 1[82] Zhuofan Zong, Guanglu Song, and Yu Liu. DETRs withcollaborative hybrid assignments training. In Proceedings ofthe IEEE/CVF Conference on Computer Vision and PatternRecognition (CVPR), pages 6748–6758, 2023. 314A. Implementation DetailsTable 2. Network configurations of YOLOv9.AppendixTable 1. Hyper parameter settings of YOLOv9.hyper parameterepochsoptimizerinitial learning ratefinish learning ratelearning rate decaymomentumweight decaywarm-up epochswarm-up momentumwarm-up bias learning ratebox loss gainclass loss gainDFL loss gainHSV saturation augmentationHSV value augmentationtranslation augmentationscale augmentationmosaic augmentationMixUp augmentationcopy & paste augmentationclose mosaic epochsvalue500SGD0.010.0001linear0.9370.000530.80.17.50.51.50.70.40.10.91.00.150.315The training parameters of YOLOv9 are shown in Ta-ble 1. We fully follow the settings of YOLOv7 AF [63],which is to use SGD optimizer to train 500 epochs. We firstwarm-up for 3 epochs and only update the bias during thewarm-up stage. Next we step down from the initial learningrate 0.01 to 0.0001 in linear decay manner, and the data aug-mentation settings are listed in the bottom part of Table 1.We shut down mosaic data augmentation operations on thelast 15 epochs.Index Module012345678910111213141516171819202122ConvConvCSP-ELANDOWNCSP-ELANDOWNCSP-ELANDOWNCSP-ELANSPP-ELANUpConcatCSP-ELANUpConcatCSP-ELANDOWNConcatCSP-ELANDOWNConcatCSP-ELANPredictRoute–012345678910, 6111213, 4141516, 12171819, 92015, 18, 21Filters64128256, 128, 64256––––2, 1–Depth Size Stride33–3–3–3––––––––3––3–––2212121211211211211211––––512, 256, 128 2, 1512, 512, 256 2, 1512, 512, 256 2, 1512, 256, 256 3, 15125125121024512, 512, 256 2, 12567685121024––256, 256, 128 2, 1––512, 512, 256 2, 1––512, 512, 256 2, 15121024–The network topology of YOLOv9 completely followsYOLOv7 AF [63], that is, we replace ELAN with the pro-posed CSP-ELAN block. As listed in Table 2, the depthparameters of CSP-ELAN are represented as ELAN depthand CSP depth, respectively. As for the parameters of CSP-ELAN filters, they are represented as ELAN output fil-ter, CSP output filter, and CSP inside filter. In the down-sampling module part, we simplify CSP-DOWN module toDOWN module. DOWN module is composed of a poolinglayer with size 2 and stride 1, and a Conv layer with size 3and stride 2. Finally, we optimized the prediction layer andreplaced top, left, bottom, and right in the regression branchwith decoupled branch.1Table 3. Comparison of state-of-the-art object detectors with different training settings.Model#Param. (M) FLOPs (G) AP50:95 (%) AP50 (%) AP75 (%) APS (%) APM (%) APL (%)h Dy-YOLOv7 [36]ctarcs-morf-niarTdeniarterPteNegamInoitallitsiDegdelwonKgnitteSxelpmoCDy-YOLOv7-X [36]YOLOv9-S (Ours)YOLOv9-M (Ours)YOLOv9-C (Ours)YOLOv9-E (Ours)YOLOv9-E (Ours)YOLOv9-E (Ours)RTMDet-T [44]RTMDet-S [44]RTMDet-M [44]RTMDet-L [44]RTMDet-X [44]PPYOLOE-S [74]PPYOLOE-M [74]PPYOLOE-L [74]PPYOLOE-X [74]RT DETR-L [43]RT DETR-X [43]RT DETR-R18 [43]RT DETR-R34 [43]RT DETR-R50M [43]RT DETR-R50 [43]RT DETR-R101 [43]Gold YOLO-S [61]Gold YOLO-M [61]Gold YOLO-L [61]YOLOv6-N v3.0 [30]YOLOv6-S v3.0 [30]YOLOv6-M v3.0 [30]YOLOv6-L v3.0 [30]DAMO YOLO-T [75]DAMO YOLO-S [75]DAMO YOLO-M [75]DAMO YOLO-L [75]Gold YOLO-N [61]Gold YOLO-S [61]Gold YOLO-M [61]Gold YOLO-L [61]Gold YOLO-S [61]Gold YOLO-M [61]Gold YOLO-L [61]YOLOR-CSP [66]YOLOR-CSP-X [66]PPYOLOE+-S [74]PPYOLOE+-M [74]PPYOLOE+-L [74]PPYOLOE+-X [74]––7.120.025.334.744.057.34.89.024.752.394.97.923.452.298.43267203136427621.541.375.14.718.534.959.68.516.328.242.15.621.541.375.121.541.375.152.996.97.923.452.298.4181.7307.926.476.3102.1147.1183.9189.012.625.678.6160.4283.414.449.9110.1206.6110234609210013625946.057.5151.711.445.385.8150.718.137.861.897.312.146.057.5151.746.057.5151.7120.4226.814.449.9110.1206.653.955.046.851.453.054.555.155.641.144.649.451.552.843.049.051.452.353.054.846.548.951.353.154.345.550.252.337.545.050.052.843.647.750.451.939.946.150.953.246.451.153.352.854.843.749.852.954.772.273.263.468.170.271.772.372.857.961.966.868.870.460.566.568.969.571.673.163.866.869.671.372.762.267.569.653.161.866.970.359.463.567.268.555.963.368.270.563.468.570.971.273.160.667.170.172.058.760.050.756.157.859.260.760.6–––––46.653.055.656.857.359.4–––57.758.6–––––––46.651.155.156.7–––––––57.659.747.954.557.959.935.336.626.633.636.238.138.740.2–––––23.228.631.435.134.635.7–––34.836.0–––––––23.326.931.633.3–––––––––23.231.835.237.957.658.756.057.058.559.960.661.0–––––46.452.955.357.057.359.6–––58.058.8–––––––47.451.755.357.0–––––––––46.453.957.559.366.468.564.568.069.370.371.471.4–––––56.963.866.168.671.272.9–––70.072.1–––––––61.064.967.167.6–––––––––56.966.269.170.4B. More ComparisonWe compare YOLOv9 to state-of-the-art real-time objectdetectors trained with different methods. It mainly includesfour different training methods: (1) train-from-scratch: wehave completed most of the comparisons in the text. Hereare only list of additional data of DynamicDet [36] for com-parisons; (2) Pretrained by ImageNet:this includes twomethods of using ImageNet for supervised pretrain and self-supervised pretrain; (3) knowledge distillation: a methodto perform additional self-distillation after training is com-pleted; and (4) a more complex training process: a combi-nation of steps including pretrained by ImageNet, knowl-edge distillation, DAMO-YOLO and even additional pre-trained large object detection dataset. We show the resultsin Table 3. From this table, we can see that our proposedYOLOv9 performed better than all other methods. Com-pared with PPYOLOE+-X trained using ImageNet and Ob-jects365, our method still reduces the number of parame-ters by 55% and the amount of computation by 11%, andimproving 0.4% AP.2L (%)Model#Param. (M) FLOPs (G) APvalTable 4. Comparison of state-of-the-art object detectors with different training settings (sorted by number of parameters).M (%) APval–––64.556.956.961.0–64.9––68.0–––66.263.8–69.367.1–71.270.3–––––70.067.671.469.166.1––71.4–72.9–––72.1––70.468.650 (%) APval50:95 (%) APval37.5–53.141.1–57.9–55.939.946.850.763.447.960.643.746.660.543.046.659.443.6–61.944.647.751.163.5–61.845.0–63.846.551.456.168.1–63.446.4–63.346.1–62.245.554.567.149.853.066.549.0–66.849.453.057.870.255.167.250.4–66.848.957.371.653.054.559.271.7–66.950.0–69.651.3–68.551.1–68.250.9–67.550.257.771.353.156.768.551.955.160.772.357.970.152.955.668.951.4–68.851.557.671.252.855.660.672.8–70.352.859.473.154.8–70.953.3–70.553.2–69.652.358.672.754.3–70.452.859.773.154.859.972.054.756.869.552.3YOLOv6-N v3.0 [30] (D)RTMDet-T [44] (I)Gold YOLO-N [61] (D)YOLOv9-S (S)PPYOLOE+-S [74] (C)PPYOLOE-S [74] (I)DAMO YOLO-T [75] (D)RTMDet-S [44] (I)DAMO YOLO-S [75] (D)YOLOv6-S v3.0 [30] (D)RT DETR-R18 [43] (I)YOLOv9-M (S)Gold YOLO-S [61] (C)Gold YOLO-S [61] (D)Gold YOLO-S [61] (I)PPYOLOE+-M [74] (C)PPYOLOE-M [74] (I)RTMDet-M [44] (I)YOLOv9-C (S)DAMO YOLO-M [75] (D)RT DETR-R34 [43] (I)RT DETR-L [43] (I)YOLOv9-E (S)YOLOv6-M v3.0 [30] (D)RT DETR-R50M [43] (I)Gold YOLO-M [61] (C)Gold YOLO-M [61] (D)Gold YOLO-M [61] (I)RT DETR-R50 [43] (I)DAMO YOLO-L [75] (D)YOLOv9-E (S)PPYOLOE+-L [74] (C)PPYOLOE-L [74] (I)RTMDet-L [44] (I)YOLOR-CSP [66] (C)YOLOv9-E (S)YOLOv6-L v3.0 [30] (D)RT DETR-X [43] (I)Gold YOLO-L [61] (C)Gold YOLO-L [61] (D)Gold YOLO-L [61] (I)RT DETR-R101 [43] (I)RTMDet-X [44] (I)YOLOR-CSP-X [66] (C)PPYOLOE+-X [74] (C)PPYOLOE-X [74] (I)S (%) APval–––56.046.446.447.4–51.7––57.0–––53.952.9–58.555.3–57.359.9–––––58.057.060.657.555.3––61.0–59.6–––58.8––59.357.075 (%) APval–––26.623.223.223.3–26.9––33.6–––31.828.6–36.231.6–34.638.1–––––34.833.338.735.231.4––40.2–35.7–––36.0––37.935.111.412.612.126.414.414.418.125.637.845.36076.346.046.046.049.949.978.6102.161.892110147.185.810057.557.557.513697.3183.9110.1110.1160.4120.4189.0150.7234151.7151.7151.7259283.4226.8206.6206.64.74.85.67.17.97.98.59.016.318.52020.021.521.521.523.423.424.725.328.2313234.734.93641.341.341.34242.144.052.252.252.352.957.359.66775.175.175.17694.996.998.498.41 (S), (I), (D), (C) indicate train-from-scratch, ImageNet pretrained, knowledge distillation, and complex setting, respectively.Table 4 shows the performance of all models sorted byparameter size. Our proposed YOLOv9 is Pareto optimalin all models of different sizes. Among them, we found noother method for Pareto optimal in models with more than20M parameters. The above experimental data shows thatour YOLOv9 has excellent parameter usage efficiency.Shown in Table 5 is the performance of all participat-ing models sorted by the amount of computation. Our pro-posed YOLOv9 is Pareto optimal in all models with differ-ent scales. Among models with more than 60 GFLOPs, onlyELAN-based DAMO-YOLO and DETR-based RT DETRcan rival the proposed YOLOv9. The above comparisonresults show that YOLOv9 has the most outstanding per-formance in the trade-off between computation complexityand accuracy.3L (%)Model#Param. (M) FLOPs (G) APvalTable 5. Comparison of state-of-the-art object detectors with different training settings (sorted by amount of computation).M (%) APval–––56.956.961.0–64.564.9––––66.263.8––––67.168.0–––67.6–69.371.269.166.1–70.070.3–––––66.471.471.470.468.6–72.972.1–68.550 (%) APval50:95 (%) APval37.5–53.139.9–55.941.1–57.943.747.960.646.660.543.046.659.443.644.6–61.946.850.763.447.751.163.5–61.845.0–63.446.4–63.346.1–62.245.549.854.567.153.066.549.051.1–68.5–68.250.9–67.550.2–63.846.555.167.250.451.456.168.1–66.849.4–66.950.0–66.848.951.956.768.5–69.651.353.057.870.257.371.653.057.970.152.955.668.951.457.671.252.853.157.771.354.559.271.7–70.352.8–70.953.3–70.553.2–69.652.3–68.851.558.772.253.955.160.772.355.660.672.859.972.054.756.869.552.359.773.154.859.473.154.858.672.754.3–70.452.860.073.255.0YOLOv6-N v3.0 [30] (D)Gold YOLO-N [61] (D)RTMDet-T [44] (I)PPYOLOE+-S [74] (C)PPYOLOE-S [74] (I)DAMO YOLO-T [75] (D)RTMDet-S [44] (I)YOLOv9-S (S)DAMO YOLO-S [75] (D)YOLOv6-S v3.0 [30] (D)Gold YOLO-S [61] (C)Gold YOLO-S [61] (D)Gold YOLO-S [61] (I)PPYOLOE+-M [74] (C)PPYOLOE-M [74] (I)Gold YOLO-M [61] (C)Gold YOLO-M [61] (D)Gold YOLO-M [61] (I)RT DETR-R18 [43] (I)DAMO YOLO-M [75] (D)YOLOv9-M (S)RTMDet-M [44] (I)YOLOv6-M v3.0 [30] (D)RT DETR-R34 [43] (I)DAMO YOLO-L [75] (D)RT DETR-R50M [43] (I)YOLOv9-C (S)RT DETR-L [43] (I)PPYOLOE+-L [74] (C)PPYOLOE-L [74] (I)YOLOR-CSP [66] (C)RT DETR-R50 [43] (I)YOLOv9-E (S)YOLOv6-L v3.0 [30] (D)Gold YOLO-L [61] (C)Gold YOLO-L [61] (D)Gold YOLO-L [61] (I)RTMDet-L [44] (I)Dy-YOLOv7 [36] (S)YOLOv9-E (S)YOLOv9-E (S)PPYOLOE+-X [74] (C)PPYOLOE-X [74] (I)YOLOR-CSP-X [66] (C)RT DETR-X [43] (I)RT DETR-R101 [43] (I)RTMDet-X [44] (I)Dy-YOLOv7-X [36] (S)S (%) APval–––46.446.447.4–56.051.7––––53.952.9––––55.357.0–––57.0–58.557.357.555.3–58.059.9–––––57.660.661.059.357.0–59.658.8–58.775 (%) APval–––23.223.223.3–26.626.9––––31.828.6––––31.633.6–––33.3–36.234.635.231.4–34.838.1–––––35.338.740.237.935.1–35.736.0–36.611.412.112.614.414.418.125.626.437.845.346.046.046.049.949.957.557.557.56061.876.378.685.89297.3100102.1110110.1110.1120.4136147.1150.7151.7151.7151.7160.4181.7183.9189.0206.6206.6226.8234259283.4307.94.75.64.87.97.98.59.07.116.318.521.521.521.523.423.441.341.341.32028.220.024.734.93142.13625.33252.252.252.94234.759.675.175.175.152.3–44.057.398.498.496.9677694.9–1 (S), (I), (D), (C) indicate train-from-scratch, ImageNet pretrained, knowledge distillation, and complex setting, respectively.4', 'pdf/YOLOv9__Learning_What_You_Want_to_Learn_Using_Programmable_Gradient_Information.pdf', '[[-0.2461393624544143676757812500000000000000\n  0.0882047787308692932128906250000000000000\n  0.2085456252098083496093750000000000000000\n  0.0881516784429550170898437500000000000000\n  -0.0903084948658943176269531250000000000000\n  -0.1088174134492874145507812500000000000000\n  0.2339711487293243408203125000000000000000\n  -0.0863623246550559997558593750000000000000\n  0.0498214140534400939941406250000000000000\n  -0.2520729005336761474609375000000000000000\n  -0.2378430813550949096679687500000000000000\n  -0.1103397384285926818847656250000000000000\n  0.2766186892986297607421875000000000000000\n  0.2200381457805633544921875000000000000000\n  -0.0715140625834465026855468750000000000000\n  0.0869259834289550781250000000000000000000\n  0.1057398766279220581054687500000000000000\n  0.1999591588973999023437500000000000000000\n  0.0177912004292011260986328125000000000000\n  0.0898907259106636047363281250000000000000\n  0.1494992673397064208984375000000000000000\n  -0.1696832776069641113281250000000000000000\n  0.4749993681907653808593750000000000000000\n  0.1318902522325515747070312500000000000000\n  -0.1594199836254119873046875000000000000000\n  0.0831205546855926513671875000000000000000\n  0.0637756437063217163085937500000000000000\n  0.1990060806274414062500000000000000000000\n  0.1345200836658477783203125000000000000000\n  0.1792190521955490112304687500000000000000\n  -0.0390117056667804718017578125000000000000\n  -0.0820741206407546997070312500000000000000\n  0.2259791940450668334960937500000000000000\n  -0.2386192977428436279296875000000000000000\n  0.0567798279225826263427734375000000000000\n  -0.1075563728809356689453125000000000000000\n  -0.0228120610117912292480468750000000000000\n  -0.0441335551440715789794921875000000000000\n  0.3750110268592834472656250000000000000000\n  -0.0838597714900970458984375000000000000000\n  0.0261066313832998275756835937500000000000\n  -0.0376925989985466003417968750000000000000\n  -0.0612859316170215606689453125000000000000\n  -0.0858886539936065673828125000000000000000\n  -0.0065208673477172851562500000000000000000\n  -0.1490971297025680541992187500000000000000\n  -3.1390938758850097656250000000000000000000\n  0.1730481833219528198242187500000000000000\n  -0.1819009184837341308593750000000000000000\n  -0.2209491133689880371093750000000000000000\n  0.3118023872375488281250000000000000000000\n  0.2231430113315582275390625000000000000000\n  0.0592205785214900970458984375000000000000\n  0.2120884954929351806640625000000000000000\n  0.1319353282451629638671875000000000000000\n  0.1309136152267456054687500000000000000000\n  -0.0067018251866102218627929687500000000000\n  0.1271063387393951416015625000000000000000\n  0.2418195009231567382812500000000000000000\n  0.0676378011703491210937500000000000000000\n  0.2746869325637817382812500000000000000000\n  0.3097370862960815429687500000000000000000\n  -0.2260760068893432617187500000000000000000\n  0.1998261958360671997070312500000000000000\n  -0.1744097471237182617187500000000000000000\n  0.2434797883033752441406250000000000000000\n  -0.2703727483749389648437500000000000000000\n  0.2707164287567138671875000000000000000000\n  -0.5532405376434326171875000000000000000000\n  0.5515757799148559570312500000000000000000\n  -0.2475742995738983154296875000000000000000\n  -0.0575789809226989746093750000000000000000\n  0.1175919398665428161621093750000000000000\n  -0.0108095221221446990966796875000000000000\n  0.1979035437107086181640625000000000000000\n  -0.0984210073947906494140625000000000000000\n  0.0538098961114883422851562500000000000000\n  0.1592347919940948486328125000000000000000\n  0.2023340612649917602539062500000000000000\n  0.3294335901737213134765625000000000000000\n  0.1654319912195205688476562500000000000000\n  -0.1228573471307754516601562500000000000000\n  0.3485898971557617187500000000000000000000\n  -0.3107722401618957519531250000000000000000\n  0.0622463151812553405761718750000000000000\n  0.3456447422504425048828125000000000000000\n  0.0471116416156291961669921875000000000000\n  -0.0560880452394485473632812500000000000000\n  0.1698657870292663574218750000000000000000\n  0.1775410771369934082031250000000000000000\n  0.0792137160897254943847656250000000000000\n  -0.0385878123342990875244140625000000000000\n  0.2745848298072814941406250000000000000000\n  0.0426662489771842956542968750000000000000\n  0.1226086765527725219726562500000000000000\n  -0.1520171612501144409179687500000000000000\n  -0.1607234328985214233398437500000000000000\n  -0.1028274223208427429199218750000000000000\n  0.1671809703111648559570312500000000000000\n  0.0685109943151473999023437500000000000000\n  0.1854675263166427612304687500000000000000\n  -0.0143609233200550079345703125000000000000\n  0.1752227246761322021484375000000000000000\n  -0.5259374976158142089843750000000000000000\n  0.1441472768783569335937500000000000000000\n  -0.1840795725584030151367187500000000000000\n  -0.3452319502830505371093750000000000000000\n  -0.2865119576454162597656250000000000000000\n  -0.0657364502549171447753906250000000000000\n  -2.0030977725982666015625000000000000000000\n  0.0085263773798942565917968750000000000000\n  0.5211985707283020019531250000000000000000\n  -0.1867371648550033569335937500000000000000\n  -0.3776238858699798583984375000000000000000\n  0.2272499203681945800781250000000000000000\n  0.0522743687033653259277343750000000000000\n  0.0415123105049133300781250000000000000000\n  0.2871179580688476562500000000000000000000\n  -0.0263218432664871215820312500000000000000\n  -0.0633720234036445617675781250000000000000\n  -0.1274340003728866577148437500000000000000\n  0.3934170901775360107421875000000000000000\n  0.2116629034280776977539062500000000000000\n  -0.2514107525348663330078125000000000000000\n  -0.1361854672431945800781250000000000000000\n  0.2135167121887207031250000000000000000000\n  0.1291496753692626953125000000000000000000\n  -0.4377849400043487548828125000000000000000\n  0.4111848771572113037109375000000000000000\n  0.3286845982074737548828125000000000000000\n  -0.0216963458806276321411132812500000000000\n  0.0593284554779529571533203125000000000000\n  -0.2069549709558486938476562500000000000000\n  0.0585554949939250946044921875000000000000\n  -0.1869541406631469726562500000000000000000\n  -0.1175692453980445861816406250000000000000\n  0.2379885613918304443359375000000000000000\n  -0.0786385312676429748535156250000000000000\n  0.0203449204564094543457031250000000000000\n  0.0657728165388107299804687500000000000000\n  -0.4176194369792938232421875000000000000000\n  0.0087185408920049667358398437500000000000\n  -2.2773988246917724609375000000000000000000\n  0.0753852576017379760742187500000000000000\n  0.3023571670055389404296875000000000000000\n  -0.0682531371712684631347656250000000000000\n  -0.0344682708382606506347656250000000000000\n  0.3046592473983764648437500000000000000000\n  -0.4976996183395385742187500000000000000000\n  -0.0700605660676956176757812500000000000000\n  0.0628771260380744934082031250000000000000\n  -0.1733363717794418334960937500000000000000\n  -0.3038735389709472656250000000000000000000\n  0.0253869779407978057861328125000000000000\n  -0.1102646067738533020019531250000000000000\n  0.1781984865665435791015625000000000000000\n  -0.0973328873515129089355468750000000000000\n  -0.1269816607236862182617187500000000000000\n  0.2127669453620910644531250000000000000000\n  0.0712857693433761596679687500000000000000\n  -0.2272799313068389892578125000000000000000\n  -0.0015827938914299011230468750000000000000\n  -0.1317319422960281372070312500000000000000\n  -0.0941962003707885742187500000000000000000\n  -0.0435748547315597534179687500000000000000\n  -0.1717087179422378540039062500000000000000\n  0.2176391929388046264648437500000000000000\n  0.0626058727502822875976562500000000000000\n  -0.1780248880386352539062500000000000000000\n  -0.1521709263324737548828125000000000000000\n  0.3377269506454467773437500000000000000000\n  -0.0360486358404159545898437500000000000000\n  0.2447282075881958007812500000000000000000\n  -0.2523934245109558105468750000000000000000\n  0.2263683080673217773437500000000000000000\n  0.0529856793582439422607421875000000000000\n  0.0161749757826328277587890625000000000000\n  0.0631292536854743957519531250000000000000\n  -0.0351786725223064422607421875000000000000\n  0.0546712949872016906738281250000000000000\n  -0.3211489915847778320312500000000000000000\n  0.3097837269306182861328125000000000000000\n  -0.0331487357616424560546875000000000000000\n  -0.2617855668067932128906250000000000000000\n  -0.2660098373889923095703125000000000000000\n  0.0811107903718948364257812500000000000000\n  0.1677567660808563232421875000000000000000\n  -0.0905912593007087707519531250000000000000\n  0.0747216045856475830078125000000000000000\n  0.2461571246385574340820312500000000000000\n  -0.3206529021263122558593750000000000000000\n  -0.1025016158819198608398437500000000000000\n  0.3093702793121337890625000000000000000000\n  0.0647298991680145263671875000000000000000\n  0.4014000892639160156250000000000000000000\n  0.3273402750492095947265625000000000000000\n  0.0105446688830852508544921875000000000000\n  -0.0636376589536666870117187500000000000000\n  0.1566754430532455444335937500000000000000\n  -0.1696810126304626464843750000000000000000\n  -0.0982775986194610595703125000000000000000\n  0.0533539094030857086181640625000000000000\n  -0.2546462714672088623046875000000000000000\n  0.1863445341587066650390625000000000000000\n  -0.2349715679883956909179687500000000000000\n  2.7595617771148681640625000000000000000000\n  0.1642679274082183837890625000000000000000\n  0.0312285572290420532226562500000000000000\n  -0.0243219733238220214843750000000000000000\n  -0.1278279572725296020507812500000000000000\n  -0.0568500608205795288085937500000000000000\n  -0.2377797961235046386718750000000000000000\n  0.0093121249228715896606445312500000000000\n  -0.3455348908901214599609375000000000000000\n  0.0187149122357368469238281250000000000000\n  -0.0875069499015808105468750000000000000000\n  0.1027254760265350341796875000000000000000\n  -0.1293140053749084472656250000000000000000\n  0.0897065252065658569335937500000000000000\n  -0.1657909154891967773437500000000000000000\n  0.0973890423774719238281250000000000000000\n  0.1193153858184814453125000000000000000000\n  0.0174333825707435607910156250000000000000\n  0.3032774627208709716796875000000000000000\n  -0.1241253912448883056640625000000000000000\n  0.0664047673344612121582031250000000000000\n  -0.0139688104391098022460937500000000000000\n  -0.3802901506423950195312500000000000000000\n  0.0912517309188842773437500000000000000000\n  -1.2156615257263183593750000000000000000000\n  0.0996197015047073364257812500000000000000\n  -0.2912312746047973632812500000000000000000\n  -0.0026281699538230895996093750000000000000\n  -0.0314938575029373168945312500000000000000\n  -0.2146741896867752075195312500000000000000\n  -0.0746098607778549194335937500000000000000\n  -0.0494356341660022735595703125000000000000\n  -0.2379589378833770751953125000000000000000\n  0.2574024796485900878906250000000000000000\n  -0.2064388692378997802734375000000000000000\n  -0.0998527258634567260742187500000000000000\n  0.2805440127849578857421875000000000000000\n  -0.0183202233165502548217773437500000000000\n  -0.0465963408350944519042968750000000000000\n  -0.2582532763481140136718750000000000000000\n  0.1256517618894577026367187500000000000000\n  0.0996021628379821777343750000000000000000\n  0.0531621500849723815917968750000000000000\n  0.0381929427385330200195312500000000000000\n  -0.1122504398226737976074218750000000000000\n  0.2919421792030334472656250000000000000000\n  -0.0364064052700996398925781250000000000000\n  -0.0692528113722801208496093750000000000000\n  -0.0191909782588481903076171875000000000000\n  0.3174722790718078613281250000000000000000\n  0.0748561248183250427246093750000000000000\n  0.1316939890384674072265625000000000000000\n  -0.0423889681696891784667968750000000000000\n  -0.3883496224880218505859375000000000000000\n  -0.1245800256729125976562500000000000000000\n  -0.2104495018720626831054687500000000000000\n  0.0117888972163200378417968750000000000000\n  0.1437007188796997070312500000000000000000\n  0.1286375522613525390625000000000000000000\n  -0.1313569545745849609375000000000000000000\n  -0.0393867082893848419189453125000000000000\n  0.3966639041900634765625000000000000000000\n  -0.4354940652847290039062500000000000000000\n  0.2881127893924713134765625000000000000000\n  -0.1250502765178680419921875000000000000000\n  -0.1582488268613815307617187500000000000000\n  -0.0085663590580224990844726562500000000000\n  -0.3136597871780395507812500000000000000000\n  -2.4099645614624023437500000000000000000000\n  -0.2543191015720367431640625000000000000000\n  -0.0640358328819274902343750000000000000000\n  0.4623171389102935791015625000000000000000\n  -0.1987777799367904663085937500000000000000\n  -0.0217185430228710174560546875000000000000\n  -0.0055454149842262268066406250000000000000\n  0.0984120666980743408203125000000000000000\n  0.2381324917078018188476562500000000000000\n  0.0821723192930221557617187500000000000000\n  -0.0316595956683158874511718750000000000000\n  0.0982425063848495483398437500000000000000\n  0.1374568492174148559570312500000000000000\n  0.0872362256050109863281250000000000000000\n  -0.1852555274963378906250000000000000000000\n  0.5775738954544067382812500000000000000000\n  -0.0121443746611475944519042968750000000000\n  0.0052598849870264530181884765625000000000\n  0.2459695339202880859375000000000000000000\n  -0.1089938431978225708007812500000000000000\n  0.1431892812252044677734375000000000000000\n  0.1310418993234634399414062500000000000000\n  -0.3389140963554382324218750000000000000000\n  0.2016223073005676269531250000000000000000\n  -0.0119520425796508789062500000000000000000\n  -0.0357715822756290435791015625000000000000\n  -0.0110669359564781188964843750000000000000\n  -0.4539188146591186523437500000000000000000\n  -0.0874293893575668334960937500000000000000\n  0.0391261391341686248779296875000000000000\n  0.0826519951224327087402343750000000000000\n  -0.0254659708589315414428710937500000000000\n  0.1103206425905227661132812500000000000000\n  -0.1915007233619689941406250000000000000000\n  -0.1348630189895629882812500000000000000000\n  -3.9218652248382568359375000000000000000000\n  -0.1694570779800415039062500000000000000000\n  -0.3115774691104888916015625000000000000000\n  -0.3581146895885467529296875000000000000000\n  0.1551073938608169555664062500000000000000\n  -0.1078411936759948730468750000000000000000\n  0.5359758138656616210937500000000000000000\n  0.0648907572031021118164062500000000000000\n  0.0330517776310443878173828125000000000000\n  0.0094308182597160339355468750000000000000\n  -0.1117596700787544250488281250000000000000\n  0.1945133954286575317382812500000000000000\n  -0.1671983450651168823242187500000000000000\n  -0.1291748434305191040039062500000000000000\n  -0.1658311784267425537109375000000000000000\n  0.1321360915899276733398437500000000000000\n  0.4245670437812805175781250000000000000000\n  0.0092207752168178558349609375000000000000\n  0.0947068259119987487792968750000000000000\n  -0.0025634169578552246093750000000000000000\n  -0.2477008104324340820312500000000000000000\n  -0.1090551614761352539062500000000000000000\n  0.0518953204154968261718750000000000000000\n  0.2684487998485565185546875000000000000000\n  0.2429763525724411010742187500000000000000\n  0.2226807326078414916992187500000000000000\n  -0.5902801752090454101562500000000000000000\n  -0.1019605845212936401367187500000000000000\n  0.0374027937650680541992187500000000000000\n  -0.1399046033620834350585937500000000000000\n  0.1068362668156623840332031250000000000000\n  -0.4787433445453643798828125000000000000000\n  -0.1859681010246276855468750000000000000000\n  0.1776347756385803222656250000000000000000\n  -0.1828810572624206542968750000000000000000\n  -0.2096273452043533325195312500000000000000\n  0.0275973435491323471069335937500000000000\n  0.1692434251308441162109375000000000000000\n  0.3727343082427978515625000000000000000000\n  -0.2180633842945098876953125000000000000000\n  0.2538617253303527832031250000000000000000\n  0.3806513249874114990234375000000000000000\n  0.0819949358701705932617187500000000000000\n  -0.0702247023582458496093750000000000000000\n  0.3844337761402130126953125000000000000000\n  -0.0635504499077796936035156250000000000000\n  0.0723332315683364868164062500000000000000\n  0.4337463974952697753906250000000000000000\n  0.0890959203243255615234375000000000000000\n  0.0811980366706848144531250000000000000000\n  -0.0094064883887767791748046875000000000000\n  0.1506208330392837524414062500000000000000\n  1.0657607316970825195312500000000000000000\n  -0.0907155349850654602050781250000000000000\n  0.0814225375652313232421875000000000000000\n  -0.0825072154402732849121093750000000000000\n  0.1124553233385086059570312500000000000000\n  -0.0101514235138893127441406250000000000000\n  -0.0923375338315963745117187500000000000000\n  0.2414999306201934814453125000000000000000\n  0.3571192622184753417968750000000000000000\n  -0.1430454105138778686523437500000000000000\n  0.1953193694353103637695312500000000000000\n  -0.1344188451766967773437500000000000000000\n  0.0554036423563957214355468750000000000000\n  -0.1524603962898254394531250000000000000000\n  0.2946031987667083740234375000000000000000\n  -0.0560186728835105895996093750000000000000\n  -0.2230214476585388183593750000000000000000\n  -0.1054693460464477539062500000000000000000\n  -0.4688873589038848876953125000000000000000\n  0.1867841184139251708984375000000000000000\n  0.2454743981361389160156250000000000000000\n  -0.4551137387752532958984375000000000000000\n  0.1937617659568786621093750000000000000000\n  -0.0771100595593452453613281250000000000000\n  -0.1051967293024063110351562500000000000000\n  0.1801799237728118896484375000000000000000\n  -0.0568943843245506286621093750000000000000\n  0.1039177030324935913085937500000000000000\n  -0.7451091408729553222656250000000000000000\n  0.0925897434353828430175781250000000000000\n  0.1202854961156845092773437500000000000000\n  0.3312881588935852050781250000000000000000\n  -0.1847933679819107055664062500000000000000\n  0.1745192259550094604492187500000000000000\n  -0.0176103282719850540161132812500000000000\n  -0.1233124360442161560058593750000000000000\n  -0.2102144360542297363281250000000000000000\n  0.4747673869132995605468750000000000000000\n  -0.1388841122388839721679687500000000000000\n  -0.0357521735131740570068359375000000000000\n  0.2905557155609130859375000000000000000000\n  0.0942327380180358886718750000000000000000\n  -0.0764425545930862426757812500000000000000\n  0.0555362664163112640380859375000000000000\n  0.2208127379417419433593750000000000000000\n  -0.2850230932235717773437500000000000000000\n  0.1117046251893043518066406250000000000000\n  -0.3806998431682586669921875000000000000000\n  0.3403900265693664550781250000000000000000\n  -0.3167297840118408203125000000000000000000\n  -0.1641069352626800537109375000000000000000\n  -0.1211013868451118469238281250000000000000\n  -0.1713717132806777954101562500000000000000\n  -0.0165398120880126953125000000000000000000\n  -0.2979701161384582519531250000000000000000\n  0.0575418472290039062500000000000000000000\n  0.0969747230410575866699218750000000000000\n  0.0823877453804016113281250000000000000000\n  -0.1726796627044677734375000000000000000000\n  0.4742178618907928466796875000000000000000\n  -0.2516765892505645751953125000000000000000\n  0.2118124067783355712890625000000000000000\n  1.0172204971313476562500000000000000000000\n  -0.3347363770008087158203125000000000000000\n  -0.2714084684848785400390625000000000000000\n  0.1272054016590118408203125000000000000000\n  0.1931281834840774536132812500000000000000\n  -0.0451032519340515136718750000000000000000\n  -0.0583672672510147094726562500000000000000\n  -0.0390630364418029785156250000000000000000\n  0.0527629777789115905761718750000000000000\n  0.1891848444938659667968750000000000000000\n  -0.2071588486433029174804687500000000000000\n  0.1234417632222175598144531250000000000000\n  0.1635489314794540405273437500000000000000\n  -0.1157438009977340698242187500000000000000\n  -0.2257174253463745117187500000000000000000\n  -0.2135993838310241699218750000000000000000\n  0.1829434335231781005859375000000000000000\n  -0.1006477326154708862304687500000000000000\n  -0.4037601351737976074218750000000000000000\n  -0.2953754067420959472656250000000000000000\n  -0.0085442811250686645507812500000000000000\n  -0.1247043013572692871093750000000000000000\n  -0.0672230497002601623535156250000000000000\n  0.1753425002098083496093750000000000000000\n  0.1768881976604461669921875000000000000000\n  -0.0080845095217227935791015625000000000000\n  0.2982315719127655029296875000000000000000\n  0.2570765018463134765625000000000000000000\n  0.0681419596076011657714843750000000000000\n  0.3605852425098419189453125000000000000000\n  0.2794806957244873046875000000000000000000\n  0.1764125078916549682617187500000000000000\n  0.0416663438081741333007812500000000000000\n  0.1061236262321472167968750000000000000000\n  -0.0975102037191390991210937500000000000000\n  0.0981106162071228027343750000000000000000\n  0.0555628687143325805664062500000000000000\n  -0.6566325426101684570312500000000000000000\n  0.0342925265431404113769531250000000000000\n  -0.0714370682835578918457031250000000000000\n  0.2643816471099853515625000000000000000000\n  0.4466491341590881347656250000000000000000\n  -0.1176705136895179748535156250000000000000\n  -0.1328645348548889160156250000000000000000\n  -0.2622300982475280761718750000000000000000\n  -0.0921615287661552429199218750000000000000\n  0.3328664302825927734375000000000000000000\n  0.1012666746973991394042968750000000000000\n  -1.7530581951141357421875000000000000000000\n  0.1331389099359512329101562500000000000000\n  0.1562172621488571166992187500000000000000\n  0.0001130402088165283203125000000000000000\n  0.1617526412010192871093750000000000000000\n  -0.0491286888718605041503906250000000000000\n  -0.0536105446517467498779296875000000000000\n  -0.0433621108531951904296875000000000000000\n  0.2732418179512023925781250000000000000000\n  0.1195808798074722290039062500000000000000\n  -0.1911835968494415283203125000000000000000\n  -0.1495657265186309814453125000000000000000\n  0.1244155019521713256835937500000000000000\n  0.0395801067352294921875000000000000000000\n  0.2780255675315856933593750000000000000000\n  -0.0449091494083404541015625000000000000000\n  -0.0084963738918304443359375000000000000000\n  0.2365524172782897949218750000000000000000\n  -0.0845640301704406738281250000000000000000\n  0.2004884183406829833984375000000000000000\n  0.0191909335553646087646484375000000000000\n  0.7547170519828796386718750000000000000000\n  0.1222203224897384643554687500000000000000\n  0.2128672301769256591796875000000000000000\n  0.2029897272586822509765625000000000000000\n  -0.1820976585149765014648437500000000000000\n  -0.1618736833333969116210937500000000000000\n  0.4298117756843566894531250000000000000000\n  0.1246978640556335449218750000000000000000\n  -0.1165871322154998779296875000000000000000\n  0.1655751764774322509765625000000000000000\n  -0.4172059893608093261718750000000000000000\n  0.0686027705669403076171875000000000000000\n  -0.0246389284729957580566406250000000000000\n  0.4705267250537872314453125000000000000000\n  0.2144178152084350585937500000000000000000\n  -0.1330263167619705200195312500000000000000\n  0.0715989693999290466308593750000000000000\n  0.0209740716964006423950195312500000000000\n  0.3056932687759399414062500000000000000000\n  -0.2777669131755828857421875000000000000000\n  0.3376619219779968261718750000000000000000\n  0.4707831144332885742187500000000000000000\n  0.1092545986175537109375000000000000000000\n  0.1177945062518119812011718750000000000000\n  0.1574075520038604736328125000000000000000\n  -0.1404619365930557250976562500000000000000\n  -0.3582990765571594238281250000000000000000\n  0.0601831674575805664062500000000000000000\n  0.0348888263106346130371093750000000000000\n  0.1875944137573242187500000000000000000000\n  0.1710089147090911865234375000000000000000\n  0.0901354402303695678710937500000000000000\n  -0.3103052675724029541015625000000000000000\n  -0.2639082074165344238281250000000000000000\n  -0.0319280475378036499023437500000000000000\n  -0.0309523437172174453735351562500000000000\n  -0.1308518648147583007812500000000000000000\n  -0.0136653287336230278015136718750000000000\n  -0.0317221544682979583740234375000000000000\n  0.0853547826409339904785156250000000000000\n  -0.4494339227676391601562500000000000000000\n  0.2382058501243591308593750000000000000000\n  0.1163923591375350952148437500000000000000\n  -0.1160649210214614868164062500000000000000\n  -0.6607686281204223632812500000000000000000\n  0.3061181008815765380859375000000000000000\n  -0.0295340046286582946777343750000000000000\n  -0.0414401292800903320312500000000000000000\n  -0.2722190320491790771484375000000000000000\n  0.2979911267757415771484375000000000000000\n  -0.0356567725539207458496093750000000000000\n  -0.4912999570369720458984375000000000000000\n  0.0068951044231653213500976562500000000000\n  0.0514637678861618041992187500000000000000\n  0.1659497469663619995117187500000000000000\n  0.2797828912734985351562500000000000000000\n  0.3712728917598724365234375000000000000000\n  -0.1637519598007202148437500000000000000000\n  -0.3503734171390533447265625000000000000000\n  0.1270700544118881225585937500000000000000\n  -0.4130846560001373291015625000000000000000\n  -0.2859075963497161865234375000000000000000\n  0.3406848013401031494140625000000000000000\n  0.0202673505991697311401367187500000000000\n  -0.0505675375461578369140625000000000000000\n  0.0271953828632831573486328125000000000000\n  0.1140924245119094848632812500000000000000\n  0.1131599247455596923828125000000000000000\n  0.0048690643161535263061523437500000000000\n  -0.2907350063323974609375000000000000000000\n  -0.2854954302310943603515625000000000000000\n  0.0674394965171813964843750000000000000000\n  0.3815332055091857910156250000000000000000\n  0.3025884032249450683593750000000000000000\n  -0.2125575542449951171875000000000000000000\n  -0.0557054542005062103271484375000000000000\n  -0.1537322998046875000000000000000000000000\n  -0.2171000689268112182617187500000000000000\n  0.0149295944720506668090820312500000000000\n  -0.1469361484050750732421875000000000000000\n  0.0452229306101799011230468750000000000000\n  -0.1534119993448257446289062500000000000000\n  0.1713307797908782958984375000000000000000\n  -0.0462095364928245544433593750000000000000\n  -0.0875146389007568359375000000000000000000\n  0.3733239471912384033203125000000000000000\n  -0.2657512426376342773437500000000000000000\n  -0.2532340288162231445312500000000000000000\n  0.1434055268764495849609375000000000000000\n  -0.0253078490495681762695312500000000000000\n  0.1396377235651016235351562500000000000000\n  0.0734023898839950561523437500000000000000\n  -0.0405264720320701599121093750000000000000\n  0.1800656914710998535156250000000000000000\n  0.0282125920057296752929687500000000000000\n  -0.2980645596981048583984375000000000000000\n  -0.0862734615802764892578125000000000000000\n  1.6407508850097656250000000000000000000000\n  0.1686923950910568237304687500000000000000\n  0.2666181921958923339843750000000000000000\n  0.0961234346032142639160156250000000000000\n  0.3100343942642211914062500000000000000000\n  0.1507060527801513671875000000000000000000\n  0.1912916004657745361328125000000000000000\n  -0.0502870231866836547851562500000000000000\n  -0.0795570984482765197753906250000000000000\n  0.0460341796278953552246093750000000000000\n  -0.2079335749149322509765625000000000000000\n  0.0525466762483119964599609375000000000000\n  -0.1474486440420150756835937500000000000000\n  -0.0400157496333122253417968750000000000000\n  0.1479229629039764404296875000000000000000\n  0.4179852008819580078125000000000000000000\n  0.0077438428997993469238281250000000000000\n  -0.1744948923587799072265625000000000000000\n  -0.3045710921287536621093750000000000000000\n  -0.0527788661420345306396484375000000000000\n  -0.0978608578443527221679687500000000000000\n  0.0213126409798860549926757812500000000000\n  0.3648539781570434570312500000000000000000\n  -0.0688353553414344787597656250000000000000\n  -0.3154717385768890380859375000000000000000\n  0.0666189864277839660644531250000000000000\n  0.2471508830785751342773437500000000000000\n  -0.1614445596933364868164062500000000000000\n  -0.1221379339694976806640625000000000000000\n  -0.1627447456121444702148437500000000000000\n  -0.0565656386315822601318359375000000000000\n  0.1181027144193649291992187500000000000000\n  0.2234182059764862060546875000000000000000\n  0.1106685921549797058105468750000000000000\n  0.0230056792497634887695312500000000000000\n  -0.0080451145768165588378906250000000000000\n  -0.1007806360721588134765625000000000000000\n  -0.1686504036188125610351562500000000000000\n  -0.0509558431804180145263671875000000000000\n  0.0601478703320026397705078125000000000000\n  0.0135938832536339759826660156250000000000\n  -0.1509208083152770996093750000000000000000\n  0.3374243676662445068359375000000000000000\n  -0.2084261029958724975585937500000000000000\n  0.0230191107839345932006835937500000000000\n  0.1130037009716033935546875000000000000000\n  0.1009765490889549255371093750000000000000\n  -0.4371096193790435791015625000000000000000\n  0.3362319469451904296875000000000000000000\n  0.5527070164680480957031250000000000000000\n  -0.0590620934963226318359375000000000000000\n  -0.1161796227097511291503906250000000000000\n  -0.1139035448431968688964843750000000000000\n  0.1017178297042846679687500000000000000000\n  -0.1640778183937072753906250000000000000000\n  0.1613727807998657226562500000000000000000\n  -0.0230339001864194869995117187500000000000\n  -0.3601811528205871582031250000000000000000\n  -0.1676610410213470458984375000000000000000\n  0.2694737613201141357421875000000000000000\n  -0.0915691554546356201171875000000000000000\n  0.4951941668987274169921875000000000000000\n  0.4447844028472900390625000000000000000000\n  -0.4444936513900756835937500000000000000000\n  -0.0617646053433418273925781250000000000000\n  0.0513172671198844909667968750000000000000\n  0.0045258132740855216979980468750000000000\n  0.0726811364293098449707031250000000000000\n  -0.2364112734794616699218750000000000000000\n  0.0178222544491291046142578125000000000000\n  0.0736520215868949890136718750000000000000\n  0.1679363697767257690429687500000000000000\n  0.2805446684360504150390625000000000000000\n  0.3929924964904785156250000000000000000000\n  0.0112199578434228897094726562500000000000\n  -0.1753184944391250610351562500000000000000\n  0.0182890668511390686035156250000000000000\n  -0.1444738656282424926757812500000000000000\n  -0.0936412364244461059570312500000000000000\n  -1.8951078653335571289062500000000000000000\n  0.0382210016250610351562500000000000000000\n  0.3000581562519073486328125000000000000000\n  0.5490064620971679687500000000000000000000\n  0.1382215917110443115234375000000000000000\n  -0.0727246329188346862792968750000000000000\n  -0.0722062811255455017089843750000000000000\n  0.0425523407757282257080078125000000000000\n  -0.0565874017775058746337890625000000000000\n  0.1658155918121337890625000000000000000000\n  0.2414480298757553100585937500000000000000\n  0.2571003139019012451171875000000000000000\n  0.3583932220935821533203125000000000000000\n  0.0837895348668098449707031250000000000000\n  -0.0389387458562850952148437500000000000000\n  0.0811660960316658020019531250000000000000\n  -0.0637571215629577636718750000000000000000\n  0.0387665480375289916992187500000000000000\n  -0.1873923540115356445312500000000000000000\n  -0.1071285307407379150390625000000000000000\n  -0.1072445660829544067382812500000000000000\n  0.3385023176670074462890625000000000000000\n  -0.1968974769115447998046875000000000000000\n  -0.1870239078998565673828125000000000000000\n  -0.1680403500795364379882812500000000000000\n  0.4517403542995452880859375000000000000000\n  0.0564493983983993530273437500000000000000\n  -0.3377945125102996826171875000000000000000\n  0.2676101922988891601562500000000000000000\n  -0.0313538089394569396972656250000000000000\n  0.0741268992424011230468750000000000000000\n  0.2932779788970947265625000000000000000000\n  -0.0542796254158020019531250000000000000000\n  0.1352778971195220947265625000000000000000\n  0.1820324361324310302734375000000000000000\n  -0.4265912175178527832031250000000000000000\n  0.2766666710376739501953125000000000000000\n  -0.4029114246368408203125000000000000000000\n  -0.0900552421808242797851562500000000000000\n  0.2603954374790191650390625000000000000000\n  -0.0184593945741653442382812500000000000000\n  0.3267085552215576171875000000000000000000\n  0.0122863724827766418457031250000000000000\n  0.2355808913707733154296875000000000000000\n  -0.0872364491224288940429687500000000000000\n  0.0976960062980651855468750000000000000000\n  0.2949783504009246826171875000000000000000\n  -0.2903108000755310058593750000000000000000\n  0.5915802121162414550781250000000000000000\n  -0.2713888287544250488281250000000000000000\n  -0.0341483056545257568359375000000000000000\n  0.0519502125680446624755859375000000000000\n  0.0080667026340961456298828125000000000000\n  0.0869495645165443420410156250000000000000\n  0.1324618011713027954101562500000000000000\n  0.1168187558650970458984375000000000000000\n  0.0663327947258949279785156250000000000000\n  -0.0063837999477982521057128906250000000000\n  -0.0262173358350992202758789062500000000000\n  -0.1935252547264099121093750000000000000000\n  -0.2373812794685363769531250000000000000000\n  0.2620297670364379882812500000000000000000\n  -0.0504738539457321166992187500000000000000\n  0.2500510811805725097656250000000000000000\n  0.1601144671440124511718750000000000000000\n  -0.1656916588544845581054687500000000000000\n  -0.3112721145153045654296875000000000000000\n  0.0582664608955383300781250000000000000000\n  -0.0942983776330947875976562500000000000000\n  -0.1452835202217102050781250000000000000000\n  -0.0785308480262756347656250000000000000000\n  -0.2361581474542617797851562500000000000000\n  0.0952342376112937927246093750000000000000\n  0.0266237147152423858642578125000000000000\n  0.1866415739059448242187500000000000000000\n  -0.1721097677946090698242187500000000000000\n  0.1840407699346542358398437500000000000000\n  0.2830873429775238037109375000000000000000\n  0.1324076652526855468750000000000000000000\n  0.3897080719470977783203125000000000000000\n  -0.0114605315029621124267578125000000000000\n  -0.1118944510817527770996093750000000000000\n  0.1574939638376235961914062500000000000000\n  -0.2669536769390106201171875000000000000000\n  0.2641922235488891601562500000000000000000\n  -5.2806782722473144531250000000000000000000\n  -0.1700581610202789306640625000000000000000\n  -0.3054938018321990966796875000000000000000\n  -0.0195060577243566513061523437500000000000\n  -0.3304924070835113525390625000000000000000\n  -0.2716582119464874267578125000000000000000\n  0.2568496465682983398437500000000000000000\n  -0.1814253926277160644531250000000000000000\n  -0.0077776387333869934082031250000000000000\n  -0.1420368999242782592773437500000000000000\n  0.1951476931571960449218750000000000000000\n  -0.0897792950272560119628906250000000000000\n  -0.0942759662866592407226562500000000000000\n  0.1441369503736495971679687500000000000000\n  0.5550062656402587890625000000000000000000\n  0.1288063079118728637695312500000000000000]]');
INSERT INTO `ms_file` (`file_id`, `file_name`, `file_content`, `file_url`, `file_content_vector`) VALUES
(7, 'YOLOv9_for_Fracture_Detection_in_Pediatric_Wrist_Trauma_X-ray_Images', 'YOLOv9 for Fracture Detection in PediatricWrist Trauma X-ray ImagesChun-Tse Chien,1 Rui-Yang Ju,2 Kuang-Yi Chou,3 andJen-Shiun Chiang(cid:66), 11Department of Electrical and Computer Engineering, TamkangUniversity, New Taipei City, 251301, Taiwan2Graduate Institute of Networking and Multimedia, National TaiwanUniversity, Taipei City, 106335, Taiwan3School of Nursing, National Taipei University of Nursing and HealthSciences, Taipei City, 112303, Taiwan(cid:66)Email: jsken.chiang@gmail.comThe introduction of YOLOv9, the latest version of the You Only LookOnce (YOLO) series, has led to its widespread adoption across variousscenarios. This paper is the first to apply the YOLOv9 algorithm modelto the fracture detection task as computer-assisted diagnosis (CAD) tohelp radiologists and surgeons to interpret X-ray images. Specifically,this paper trained the model on the GRAZPEDWRI-DX dataset andextended the training set using data augmentation techniques to improvethe model performance. Experimental results demonstrate that com-pared to the mAP 50-95 of the current state-of-the-art (SOTA) model,the YOLOv9 model increased the value from 42.16% to 43.73%, withan improvement of 3.7%. The implementation code is publicly availableat https://github.com/RuiyangJu/YOLOv9-Fracture-Detection.4202raM71]VI.ssee[1v94211.3042:viXraIntroduction: Computer-assisted diagnosis (CAD) helps specialistssuch as radiologists and surgeons to interpret medical images, includ-ing magnetic resonance imaging (MRI), computed tomography (CT),and X-ray images. The application of deep learning techniques to medi-cal images [1–4] has yielded increasingly satisfactory results, making ita popular research focus, especially in fracture detection [5–7].You Only Look Once (YOLO) series [8–16] are the main neural net-works for real-time object detection task, widely employed in fracturedetection [17–19]. Wrist fractures in children are more common casesand the GRAZPEDWRI-DX dataset [20] provides 20,327 X-ray imagesof pediatric wrist trauma that can be used in fracture detection tasks.Research [21] first used the YOLOv8 [16] model for fracture detec-tion on this dataset. Since attention mechanisms [22–25] have excellentresults in enhancing the performance of neural network models, Chienet al. achieved the state-of-the-art (SOTA) performance by incorporatingdifferent attention mechanisms into the YOLOv8 model.With the presentation of YOLOv9 [26], which achieved remarkablemodel performance on the MS COCO 2017 [27] benchmark dataset,this paper first trained the YOLOv9 model on the GRAZPEDWRI-DXdataset and obtained the SOTA performance, as shown in Fig 1.The main contributions of this paper are as follows:1. This paper is the first to apply YOLOv9 to the fracture detectiontask, demonstrating that the model not only has the excellent per-formance in real-time object detection across real-life scenarios, butalso has good results in medical image recognition.2. This paper addresses the issue of information loss in fracture detec-tion on X-ray images by employing YOLOv9 algorithm, and aimsto preserve more information during model training on low-featuresX-ray images, enhancing the performance of the model.3. The mAP 50-95 of YOLOv9 model trained on the GRAZPEDWRI-DX dataset is significantly improved, achieving the SOTA level.Related Works: In the field of object detection task, detectors typicallyemploy either one-stage or two-stage algorithms. Compared to two-stage object detectors, the models of YOLO series offer a more balancedcombination of accuracy and inference speed, making them suitable fordeployment on mobile computing platforms for medical image recog-nition. Son et al. [28] utilized YOLOv4 [9] and U-Net [29] as auxil-iary diagnostic tools to assist dentists in identifying mandibular frac-tures without resorting to cone beam computed tomography (CBCT).Jeon et al. [30] employed YOLOv4 [9] to aid surgeons in diagnosingtrauma by detecting the fracture and mapping it onto a 3D reconstructedFig 1 Comparisons of fracture detection models on the GRAZPEDWRI-DXdataset. In terms of accuracy, our models outperform all previous models atthe state-of-the-art level.bone image, providing a clear display of the fracture region through thered mask overlaid on the 3D bone image. Hrži´c et al. [18] employedthe YOLOv4 [9] model for fracture detection on the GRAZPEDWRI-DX dataset [20], which was the first to demonstrate that the models ofYOLO series can assist radiologists in more accurately predicting wristinjuries in children on X-ray images. Ahmed et al. [31] demonstratedthe potential of one-stage algorithm models in enhancing the accuracyof diagnosis on pediatric wrist X-ray images by employing YOLOv5[12], YOLOv6 [13], YOLOv7 [15], and YOLOv8 [16] models for wristanomaly detection, respectively. Warin et al. [32] utilized the YOLOv5[12] model to detect mandibular fractures in panoramic X-ray images,demonstrating the ability of the YOLOv5 model to recognize mandibu-lar fractures at an expert level. Gaikwad et al. [33] applied the YOLOv5[12] model to detect major and minor fractures of the C1 to C7 ver-tebrae, achieving an accuracy rate of 89%. Zou et al. [34] investigatedvarious fracture morphologies throughout the body, including angle frac-tures, normal fractures, line fractures, and disoriented angle fractures.They integrated the YOLOv7 [15] model with the attention mechanism[35], achieving the superior performance on the FracAtlas [36] dataset.Samothai et al. [17] demonstrated that the YOLOX [10] model exhibitsfaster convergence speed and higher accuracy than YOLOR [11] bydetecting the fracture region through the methods such as detector headdecoupling, anchor-free, and augmentation strategies. They also showedthat YOLOX can locate fractures even in low-featured X-ray images.Moon et al. [37] proposed a computer-aided facial bone fracture diagno-sis (CA-FBFD) system based on the YOLOX model, effectively reduc-ing the workload of doctors in diagnosing facial fractures in facial CTscans. While the application of the models of YOLO series to medicalimage recognition is a hot research topic, to date, no one has utilizedYOLOv9 [26] for fracture detection.Method:YOLOv9: Neural networks often have the challenge of information losssince the input data undergoes multiple layers of feature extraction andspatial transformation, resulting in the loss of the original information.This issue is particularly pronounced in X-ray images, where the low-features present significant difficult in fracture detection tasks. Specif-ically, models trained on such low-featured images tend to performpoorly, and addressing the problem of information loss could substan-tially enhance the accuracy of model predictions. To address this, weutilizing the YOLOv9 algorithm, which leverages the ProgrammableGradient Information (PGI) and the Generalized Efficient Layer Aggre-gation Network (GELAN) to more effectively extract key features.Programmable Gradient Information: Programmable Gradient Infor-mation (PGI) is an auxiliary supervision framework designed to man-age the propagation of gradient information across various semantic lev-ELECTRONICS LETTERS wileyonlinelibrary.com/iet-el1012345678910Inference Time (ms)60616263646566mAP 50 (%)Performance on GRAZPEDWRI-DX DatasetYOLOv8YOLOv8+SAYOLOv8+ECAYOLOv8+GAMYOLOv8+ResGAMYOLOv8+ResCBAMYOLOv9 (Ours)mAP50 SOTASMLCE2.3% HigherSMLMLSSMLSMLSML      Fig 2 An overall flowchart of the YOLOv9 algorithm model applied to the fracture detection task.Table 1. Quantitative comparison with other state-of-the-art modelsfor fracture detection on the GRAZPEDWRI-DX datasets when theinput image size is 640.Table 2. Quantitative comparison with other state-of-the-art modelsfor fracture detection on the GRAZPEDWRI-DX datasets when theinput image size is 1024.ModelParamsFLOPsF1mAP 50mAP 50-95SpeedaModelParamsFLOPsF1mAP 50mAP 50-95Speeda(M)(G)(%)(%)(%)(ms)(M)(G)(%)(%)(%)(ms)YOLOv8[16]43.61164.9YOLOv8+SA[22]43.64165.4YOLOv8+ECA[23]43.64165.5YOLOv8+GAM[24]49.29183.5YOLOv8+ResGAM[38]49.29183.5YOLOv8+ResCBAM[25]53.87196.2YOLOv9-C (Ours)51.02239.0YOLOv9-E (Ours)69.42244.9596261606262646462.4463.9962.6463.3263.9762.9565.3165.4640.3241.4940.2140.7441.1840.1042.6643.323.63.93.68.79.44.15.26.4YOLOv8[16]43.61164.9YOLOv8+SA[22]43.64165.4YOLOv8+ECA[23]43.64165.5YOLOv8+GAM[24]49.29183.5YOLOv8+ResGAM[38]49.29183.5YOLOv8+ResCBAM[25]53.87196.2YOLOv9-C (Ours)51.02239.0YOLOv9-E (Ours)69.42244.9626365656464666663.6364.2564.2664.2664.9865.7865.5765.6240.4141.6441.9441.0041.7542.1643.7043.737.78.07.712.718.18.712.716.1Note: The model size of all YOLOv8 and its variants listed in the table is large.a Speed is the total time for preprocessing, inference, and post-processing.Note: The model size of all YOLOv8 and its variants listed in the table is large.a Speed is the total time for preprocessing, inference, and post-processing.els, to improve the detection capability of the model. PGI comprisesthree main components: main branch, auxiliary reversible branch, andmulti-level auxiliary information. During the inference process, it exclu-sively employs the main branch, which handles both forward and backpropagation. As the network becomes deeper, an information bottleneckmay occur, leading to loss functions that fail to produce useful gradients.In such cases, auxiliary reversible branch employs reversible functionsto preserve information integrity and mitigate information loss in themain branch. Additionally, multi-level auxiliary information addressesthe issue of error accumulation from the deep supervision mechanism,improving the learning capacity of the model through the introductionof supplementary information at different levels. Notably, research [26]highlighted the efficacy of PGI in preserving information during train-ing, particularly in scenarios with limited features. This provides the the-oretical basis for the YOLOv9 model to have excellent performance infracture detection tasks.Generalized Efficient Layer Aggregation Network: To enhance informa-tion integration and propagation efficiency in model training, YOLOv9introduced a novel lightweight network architecture named GeneralizedEfficient Layer Aggregation Network (GELAN). GELAN integratesCSPNet [39] and ELAN [40] to efficiently aggregate network infor-mation, reducing information loss in propagation and enhancing inter-layer information interaction. This architecture is particularly suitablefor fracture detection in environments with limited computing resourcesdue to its lower parameters and computational complexity.Data Processing and Augmentation: Fig 2 illustrates the flowchart ofthe experiments conducted in this study. Since the publisher of theGRAZPEDWRI-DX [20] dataset did not provide predefined training,validation, and test sets, we randomly assigned 70% to training set,20% to validation set, and 10% to test set during the data processing.Moreover, due to the limited brightness diversity of low-featured X-rayimages, models trained only on these images may not generalize well toX-ray images in other environments. To enhance the robustness of themodel, we employed data augmentation techniques to extend the train-ing set. Specifically, we fine-tuned the contrast and luminance of theX-ray images using the addWeighted function from the OpenCV library.Experiment:Dataset: GRAZPEDWRI-DX [20] is a public dataset provided by theMedical University of Graz, which contains 20,327 X-ray images ofpediatric wrist trauma. These X-ray images were collected by a teamof pediatric radiologists at the University Hospital Graz from 2008 to2018. The dataset comprises 6,091 patients and 10,643 studies, with atotal of 74,459 labeled images, representing 67,771 labeled objects.Experiment setup: The experiments in this paper utilized one singleNVIDIA GeForce RTX 3090 GPU, employing Python with the PyTorchframework. Before training our model, we employed the YOLOv9model weights pretrained on the MS COCO 2017 [27] dataset. In thetraining process, we trained the model using the SGD [41] optimizer,with a weight decay rate set to 5e-4 and a momentum of 0.937. We fol-lowed the research [21] to set the initial learning rate to 1e-2, the numberof epochs to 100. Due to resource limitations (24GB memory) imposedby a single GPU, a batch size of 16 was employed for training the model.Experimental Results: To evaluate the performance of YOLOv9 andother SOTA models in real diagnostic scenarios, this study comparesmodel size (parameters and floating-point operations per second), accu-racy (F1 score, mean average precision at 50% (mAP 50), and meanaverage precision from 50% to 95% (mAP 50-95)), and inference time. Itis widely recognized that using larger input image sizes improves predic-tion accuracy but also requires more computational resources. Therefore,we conducted two experiments with input image sizes of 640 and 1024for various scenarios, and the results are presented in Tables 1 and 2.With the input size of 640, both YOLOv9-C (Compact) and YOLOv9-E(Extended) demonstrate significantly improved mAP, while maintaininga reasonable inference speed. Specifically, YOLOv9-E achieves mAP2ELECTRONICS LETTERS wileyonlinelibrary.com/iet-elX-ray Image DatasetRandom SplitTraining Set (70%)Validation Set (20%)Testing Set (10%)Data AugmentationExtended Training SetYOLOv9 AlgorithmOur ModelValidateEvaluatePredictionPredict50-95 of 43.32%, which is 4.4% higher than 41.49% achieved by thecurrent SOTA model YOLOv8+SA. When the input image size is 1024,the mAP 50-95 of YOLOv9-E reaches 43.73%, which also obtains theSOTA performance. However, due to the increased inference time, it ismore suitable for deployment on devices with high computing resources.Conclusion: The models of YOLO series can serve as CAD to assistradiologists and surgeons in interpreting X-ray images. However, thepredictions from the previous models are often unsatisfactory due to thelow features of X-ray images. This paper first introduces the applica-tion of YOLOv9 to fracture detection, addressing the issue of infor-mation loss during model training by employing the newly proposedPGI and GELAN. Experimental results indicate that the YOLOv9 modelachieves SOTA performance on the GRAZPEDWRI-DX dataset, prov-ing the effectiveness of this method.Acknowledgments: This research is supported by National Science andTechnology Council of Taiwan, under Grant Number: NSTC 112-2221-E-032-037-MY2.© 2024 The Authors. Electronics Letters published by John Wiley &Sons Ltd on behalf of The Institution of Engineering and TechnologyReferences1. Chung, S.W., et al.: Automated detection and classification of theproximal humerus fracture by using deep learning algorithm. Actaorthopaedica 89(4), 468–473 (2018)2. Choi, J.W., et al.: Using a dual-input convolutional neural networkfor automated detection of pediatric supracondylar fracture on conven-tional radiography. Investigative radiology 55(2), 101–110 (2020)3. Tanzi, L., et al.: Hierarchical fracture classification of proximal femurx-ray images using a multistage deep learning approach. Europeanjournal of radiology 133, 109373 (2020)4. Adams, S.J., et al.: Artificial intelligence solutions for analysis of x-rayimages. Canadian Association of Radiologists Journal 72(1), 60–72(2021)5. Gan, K., et al.: Artificial intelligence detection of distal radius frac-tures: a comparison between the convolutional neural network and pro-fessional assessments. Acta orthopaedica 90(4), 394–400 (2019)6. Yahalomi, E., Chernofsky, M., Werman, M.: Detection of distal radiusfractures trained by a small set of x-ray images and faster r-cnn. In:Intelligent Computing: Proceedings of the 2019 Computing Confer-ence, Volume 1, pp. 971–981. Springer (2019)7. Blüthgen, C., et al.: Detection and localization of distal radius frac-tures: Deep learning system versus radiologists. European journal ofradiology 126, 108925 (2020)8. Redmon, J., et al.: You only look once: Unified, real-time object detec-tion. In: Proceedings of the IEEE conference on computer vision andpattern recognition, pp. 779–788. (2016)9. Bochkovskiy, A., Wang, C.Y., Liao, H.Y.M.: Yolov4: Optimal speedand accuracy of object detection. arXiv preprint arXiv:2004.10934(2020)10. Ge, Z., et al.: Yolox: Exceeding yolo series in 2021. arXiv preprintarXiv:2107.08430 (2021)11. Wang, C.Y., Yeh, I.H., Liao, H.Y.M.: You only learn one representation:Unified network for multiple tasks. arXiv preprint arXiv:2105.04206(2021)12. Glenn, J.: Ultralytics yolov5. GitHub. https://github.com/ultralytics/yolov5 (2022)14.13. Li, C., et al.: Yolov6: A single-stage object detection framework forindustrial applications. arXiv preprint arXiv:2209.02976 (2022)Ju, R.Y., et al.: Resolution enhancement processing on low qualityimages using swin transformer based on interval dense connectionstrategy. Multimedia Tools and Applications pp. 1–17. (2023)15. Wang, C.Y., Bochkovskiy, A., Liao, H.Y.M.: Yolov7: Trainable bag-of-freebies sets new state-of-the-art for real-time object detectors. In: Pro-ceedings of the IEEE/CVF Conference on Computer Vision and PatternRecognition, pp. 7464–7475. (2023)16. Glenn, J.: Ultralytics yolov8. GitHub. https://github.com/ultralytics/ultralytics (2023)17. Samothai, P., et al.: The evaluation of bone fracture detection ofyolo series.In: 2022 37th International Technical Conferenceon Circuits/Systems, Computers and Communications (ITC-CSCC),pp. 1054–1057. IEEE (2022)18. Hrži´c, F., et al.: Fracture recognition in paediatric wrist radiographs:An object detection approach. Mathematics 10(16), 2939 (2022)19. Su, Z., et al.: Skeletal fracture detection with deep learning: A compre-hensive review. Diagnostics 13(20), 3245 (2023)20. Nagy, E., et al.: A pediatric wrist trauma x-ray dataset (grazpedwri-dx)21.for machine learning. Scientific Data 9(1), 222 (2022)Ju, R.Y., Cai, W.: Fracture detection in pediatric wrist trauma x-rayimages using yolov8 algorithm.arXiv preprint arXiv:2304.05071(2023)22. Zhang, Q.L., Yang, Y.B.: Sa-net: Shuffle attention for deep convolu-tional neural networks.In: ICASSP 2021-2021 IEEE InternationalConference on Acoustics, Speech and Signal Processing (ICASSP),pp. 2235–2239. IEEE (2021)23. Wang, Q., et al.: Eca-net: Efficient channel attention for deep convolu-tional neural networks. In: Proceedings of the IEEE/CVF conferenceon computer vision and pattern recognition, pp. 11534–11542. (2020)24. Liu, Y., Shao, Z., Hoffmann, N.: Global attention mechanism: RetainarXiv preprintinformation to enhance channel-spatial interactions.arXiv:2112.05561 (2021)25. Woo, S., et al.: Cbam: Convolutional block attention module.In:Proceedings of the European conference on computer vision (ECCV),pp. 3–19. (2018)26. Wang, C.Y., Yeh, I.H., Liao, H.Y.M.: Yolov9: Learning what you wantarXiv preprintto learn using programmable gradient information.arXiv:2402.13616 (2024)27. Lin, T.Y., et al.: Microsoft coco: Common objects in context. In: Com-puter Vision–ECCV 2014: 13th European Conference, Zurich, Switzer-land, September 6-12, 2014, Proceedings, Part V 13, pp. 740–755.Springer (2014)28. Son, D.M., et al.: Combined deep learning techniques for mandibularfracture diagnosis assistance. Life 12(11), 1711 (2022)29. Ronneberger, O., Fischer, P., Brox, T.: U-net: Convolutional networksfor biomedical image segmentation.In: Medical Image Computingand Computer-Assisted Intervention–MICCAI 2015: 18th InternationalConference, Munich, Germany, October 5-9, 2015, Proceedings, PartIII 18, pp. 234–241. Springer (2015)Jeon, Y.D., et al.: Deep learning model based on you only look oncealgorithm for detection and visualization of fracture areas in three-dimensional skeletal images. Diagnostics 14(1), 11 (2023)30.31. Ahmed, A., et al.: Enhancing wrist abnormality detection with yolo:Analysis of state-of-the-art single-stage detection models. BiomedicalSignal Processing and Control 93, 106144 (2024)32. Warin, K., et al.: Assessment of deep convolutional neural networkmodels for mandibular fracture detection in panoramic radiographs.International Journal of Oral and Maxillofacial Surgery 51(11), 1488–1494 (2022)33. Gaikwad, D., et al.: Identification of cervical spine fracture using deeplearning. Australian Journal of Multi-Disciplinary Engineering pp. 1–9. (2024)34. Zou, J., Arshad, M.R.: Detection of whole body bone fractures basedon improved yolov7. Biomedical Signal Processing and Control 91,105995 (2024)35. Hu, J., Shen, L., Sun, G.: Squeeze-and-excitation networks. In: Pro-ceedings of the IEEE conference on computer vision and pattern recog-nition, pp. 7132–7141. (2018)36. Abedeen, I., et al.: Fracatlas: A dataset for fracture classification, local-ization and segmentation of musculoskeletal radiographs. ScientificData 10(1), 521 (2023)37. Moon, G., et al.: Computer aided facial bone fracture diagnosis (ca-fbfd) system based on object detection model. IEEE Access 10, 79061–79070 (2022)38. Chien, C.T., et al.: Yolov8-am: Yolov8 with attention mechanisms forarXiv preprint arXiv:2402.09329pediatric wrist fracture detection.(2024)39. Wang, C.Y., et al.: Cspnet: A new backbone that can enhance learn-ing capability of cnn.In: Proceedings of the IEEE/CVF conferenceon computer vision and pattern recognition workshops, pp. 390–391.(2020)40. Wang, C.Y., Liao, H.Y.M., Yeh, I.H.: Designing network design strate-gies through gradient path analysis. arXiv preprint arXiv:2211.04800(2022)41. Ruder, S.: An overview of gradient descent optimization algorithms.arXiv preprint arXiv:1609.04747 (2016)ELECTRONICS LETTERS wileyonlinelibrary.com/iet-el3', 'pdf/YOLOv9_for_Fracture_Detection_in_Pediatric_Wrist_Trauma_X-ray_Images.pdf', '[[-0.2472720295190811157226562500000000000000\n  0.0364129766821861267089843750000000000000\n  0.1494421958923339843750000000000000000000\n  0.0607616677880287170410156250000000000000\n  -0.0209843516349792480468750000000000000000\n  -0.1164777278900146484375000000000000000000\n  0.0525716394186019897460937500000000000000\n  -0.0187160745263099670410156250000000000000\n  0.0879080295562744140625000000000000000000\n  -0.1466029584407806396484375000000000000000\n  -0.2422543764114379882812500000000000000000\n  -0.1151341944932937622070312500000000000000\n  0.1967711299657821655273437500000000000000\n  0.1736439019441604614257812500000000000000\n  -0.1576851308345794677734375000000000000000\n  0.0481917411088943481445312500000000000000\n  0.0591426789760589599609375000000000000000\n  0.2619606852531433105468750000000000000000\n  0.0359058342874050140380859375000000000000\n  0.0336162857711315155029296875000000000000\n  0.0340852960944175720214843750000000000000\n  -0.2455977499485015869140625000000000000000\n  0.5168278813362121582031250000000000000000\n  0.0977833420038223266601562500000000000000\n  -0.1989562064409255981445312500000000000000\n  0.0687516927719116210937500000000000000000\n  0.0700583234429359436035156250000000000000\n  0.1954622268676757812500000000000000000000\n  0.0291600227355957031250000000000000000000\n  0.1483705192804336547851562500000000000000\n  0.0340022817254066467285156250000000000000\n  -0.0687571391463279724121093750000000000000\n  0.1465258300304412841796875000000000000000\n  -0.1173872798681259155273437500000000000000\n  0.0283919312059879302978515625000000000000\n  -0.0492001138627529144287109375000000000000\n  0.0308023057878017425537109375000000000000\n  -0.0029294192790985107421875000000000000000\n  0.3105700910091400146484375000000000000000\n  -0.0452512763440608978271484375000000000000\n  0.0488022230565547943115234375000000000000\n  -0.0220676735043525695800781250000000000000\n  -0.1118313819169998168945312500000000000000\n  -0.0061013381928205490112304687500000000000\n  0.0168982893228530883789062500000000000000\n  -0.1536213159561157226562500000000000000000\n  -3.0622305870056152343750000000000000000000\n  0.1817391365766525268554687500000000000000\n  -0.1741504371166229248046875000000000000000\n  -0.2024996280670166015625000000000000000000\n  0.2552624344825744628906250000000000000000\n  0.2150121927261352539062500000000000000000\n  0.1492541134357452392578125000000000000000\n  0.1937345266342163085937500000000000000000\n  0.1558694839477539062500000000000000000000\n  0.1694414168596267700195312500000000000000\n  -0.0790396779775619506835937500000000000000\n  0.0612936541438102722167968750000000000000\n  0.1389965862035751342773437500000000000000\n  0.0291521996259689331054687500000000000000\n  0.3512175679206848144531250000000000000000\n  0.2464901208877563476562500000000000000000\n  -0.2063876241445541381835937500000000000000\n  0.2125547230243682861328125000000000000000\n  -0.0644317567348480224609375000000000000000\n  0.2248877584934234619140625000000000000000\n  -0.2748226225376129150390625000000000000000\n  0.2461139261722564697265625000000000000000\n  -0.4852201044559478759765625000000000000000\n  0.5104228258132934570312500000000000000000\n  -0.2263766527175903320312500000000000000000\n  -0.0804853811860084533691406250000000000000\n  0.1387754082679748535156250000000000000000\n  0.0524502731859683990478515625000000000000\n  0.2190751880407333374023437500000000000000\n  -0.1175814345479011535644531250000000000000\n  -0.0058161737397313117980957031250000000000\n  0.1410678476095199584960937500000000000000\n  0.1632666289806365966796875000000000000000\n  0.3473671972751617431640625000000000000000\n  0.2403890490531921386718750000000000000000\n  -0.0966564565896987915039062500000000000000\n  0.3043349385261535644531250000000000000000\n  -0.3122731149196624755859375000000000000000\n  0.0513124093413352966308593750000000000000\n  0.2500241696834564208984375000000000000000\n  0.0287626441568136215209960937500000000000\n  0.0001392439007759094238281250000000000000\n  0.1610935926437377929687500000000000000000\n  0.1905403286218643188476562500000000000000\n  0.1453438252210617065429687500000000000000\n  -0.0245148614048957824707031250000000000000\n  0.2192616462707519531250000000000000000000\n  0.0612853243947029113769531250000000000000\n  0.0376033633947372436523437500000000000000\n  -0.1447951346635818481445312500000000000000\n  -0.1963403820991516113281250000000000000000\n  -0.0950965657830238342285156250000000000000\n  0.1975300759077072143554687500000000000000\n  0.0528901964426040649414062500000000000000\n  0.1648199111223220825195312500000000000000\n  -0.0225073415786027908325195312500000000000\n  0.1428186297416687011718750000000000000000\n  -0.5683908462524414062500000000000000000000\n  0.1397426724433898925781250000000000000000\n  -0.2127584069967269897460937500000000000000\n  -0.2969382405281066894531250000000000000000\n  -0.1804187595844268798828125000000000000000\n  -0.0513451509177684783935546875000000000000\n  -2.1090042591094970703125000000000000000000\n  0.0578296780586242675781250000000000000000\n  0.4507547020912170410156250000000000000000\n  -0.1668273508548736572265625000000000000000\n  -0.4279779195785522460937500000000000000000\n  0.1564603447914123535156250000000000000000\n  0.1154951080679893493652343750000000000000\n  0.0910853669047355651855468750000000000000\n  0.2461594790220260620117187500000000000000\n  -0.0238078292459249496459960937500000000000\n  -0.0660580694675445556640625000000000000000\n  -0.0692191570997238159179687500000000000000\n  0.3444389104843139648437500000000000000000\n  0.1370861381292343139648437500000000000000\n  -0.2662808895111083984375000000000000000000\n  -0.0474384203553199768066406250000000000000\n  0.2481715530157089233398437500000000000000\n  0.0695848315954208374023437500000000000000\n  -0.3371189236640930175781250000000000000000\n  0.3451504409313201904296875000000000000000\n  0.3303797543048858642578125000000000000000\n  -0.2035432010889053344726562500000000000000\n  0.0821987986564636230468750000000000000000\n  -0.2760174870491027832031250000000000000000\n  0.0955655872821807861328125000000000000000\n  -0.0709048360586166381835937500000000000000\n  -0.1577297598123550415039062500000000000000\n  0.1525574475526809692382812500000000000000\n  -0.1491569876670837402343750000000000000000\n  0.0781922340393066406250000000000000000000\n  0.0329219326376914978027343750000000000000\n  -0.3559638261795043945312500000000000000000\n  0.0837195217609405517578125000000000000000\n  -2.3815906047821044921875000000000000000000\n  0.0825094729661941528320312500000000000000\n  0.3210685253143310546875000000000000000000\n  -0.0336487367749214172363281250000000000000\n  -0.0356115996837615966796875000000000000000\n  0.3192809224128723144531250000000000000000\n  -0.3872491121292114257812500000000000000000\n  -0.0654220879077911376953125000000000000000\n  0.0253780521452426910400390625000000000000\n  -0.2216758579015731811523437500000000000000\n  -0.2453576624393463134765625000000000000000\n  0.0985448360443115234375000000000000000000\n  -0.1476788669824600219726562500000000000000\n  0.1539402455091476440429687500000000000000\n  0.0359259434044361114501953125000000000000\n  -0.0992926508188247680664062500000000000000\n  0.1759878695011138916015625000000000000000\n  0.1639342755079269409179687500000000000000\n  -0.1473347544670104980468750000000000000000\n  -0.0381556339561939239501953125000000000000\n  -0.2403791695833206176757812500000000000000\n  -0.0276330374181270599365234375000000000000\n  -0.0404206328094005584716796875000000000000\n  -0.1413435637950897216796875000000000000000\n  0.1885788291692733764648437500000000000000\n  0.0908127576112747192382812500000000000000\n  -0.1267680525779724121093750000000000000000\n  -0.2015516310930252075195312500000000000000\n  0.2211744785308837890625000000000000000000\n  -0.1038844585418701171875000000000000000000\n  0.2192855179309844970703125000000000000000\n  -0.2237774431705474853515625000000000000000\n  0.3024950027465820312500000000000000000000\n  0.1685290187597274780273437500000000000000\n  0.0138278640806674957275390625000000000000\n  0.0115488283336162567138671875000000000000\n  -0.0500155314803123474121093750000000000000\n  0.0310265403240919113159179687500000000000\n  -0.3417122364044189453125000000000000000000\n  0.3933438658714294433593750000000000000000\n  0.0002345629036426544189453125000000000000\n  -0.1218004450201988220214843750000000000000\n  -0.2140896618366241455078125000000000000000\n  0.0985682308673858642578125000000000000000\n  0.2328722178936004638671875000000000000000\n  -0.0273962896317243576049804687500000000000\n  0.0966226011514663696289062500000000000000\n  0.3484168648719787597656250000000000000000\n  -0.2724332809448242187500000000000000000000\n  -0.1852111518383026123046875000000000000000\n  0.3814311623573303222656250000000000000000\n  0.0420675724744796752929687500000000000000\n  0.3524355590343475341796875000000000000000\n  0.3444938957691192626953125000000000000000\n  -0.0106769604608416557312011718750000000000\n  0.0172049775719642639160156250000000000000\n  0.1524702310562133789062500000000000000000\n  -0.2245393097400665283203125000000000000000\n  -0.0880812406539916992187500000000000000000\n  0.0370292104780673980712890625000000000000\n  -0.2073345035314559936523437500000000000000\n  0.1425569206476211547851562500000000000000\n  -0.1797641068696975708007812500000000000000\n  2.8669764995574951171875000000000000000000\n  0.2349436730146408081054687500000000000000\n  0.0471095219254493713378906250000000000000\n  0.0550506487488746643066406250000000000000\n  -0.0779394283890724182128906250000000000000\n  0.0514679551124572753906250000000000000000\n  -0.2911300957202911376953125000000000000000\n  0.0684323608875274658203125000000000000000\n  -0.3406119048595428466796875000000000000000\n  0.0082818660885095596313476562500000000000\n  -0.1237611174583435058593750000000000000000\n  0.1743977814912796020507812500000000000000\n  -0.2121240049600601196289062500000000000000\n  -0.0755132213234901428222656250000000000000\n  -0.0992159843444824218750000000000000000000\n  0.0278026629239320755004882812500000000000\n  0.1194203644990921020507812500000000000000\n  0.0384699255228042602539062500000000000000\n  0.3679295480251312255859375000000000000000\n  -0.0547824800014495849609375000000000000000\n  0.0920867398381233215332031250000000000000\n  -0.0398491658270359039306640625000000000000\n  -0.2831836938858032226562500000000000000000\n  0.0414117686450481414794921875000000000000\n  -1.0782839059829711914062500000000000000000\n  0.1803774386644363403320312500000000000000\n  -0.1733969748020172119140625000000000000000\n  0.0371257588267326354980468750000000000000\n  0.0160350538790225982666015625000000000000\n  -0.1645502597093582153320312500000000000000\n  -0.0975515693426132202148437500000000000000\n  -0.0833932906389236450195312500000000000000\n  -0.2543256580829620361328125000000000000000\n  0.2394871413707733154296875000000000000000\n  -0.1699513792991638183593750000000000000000\n  -0.0741059035062789916992187500000000000000\n  0.2566265463829040527343750000000000000000\n  -0.0226761661469936370849609375000000000000\n  0.0000001601874828338623046875000000000000\n  -0.3396089076995849609375000000000000000000\n  0.1311526298522949218750000000000000000000\n  0.0513100773096084594726562500000000000000\n  -0.0795691087841987609863281250000000000000\n  0.0459576100111007690429687500000000000000\n  -0.1282244026660919189453125000000000000000\n  0.3246573507785797119140625000000000000000\n  -0.0705384612083435058593750000000000000000\n  -0.0468430146574974060058593750000000000000\n  -0.0743533670902252197265625000000000000000\n  0.2939557433128356933593750000000000000000\n  0.0475156679749488830566406250000000000000\n  0.0779730975627899169921875000000000000000\n  -0.0034676212817430496215820312500000000000\n  -0.2665706574916839599609375000000000000000\n  -0.1764355450868606567382812500000000000000\n  -0.1350113600492477416992187500000000000000\n  -0.0526813790202140808105468750000000000000\n  0.1633401215076446533203125000000000000000\n  0.2425238490104675292968750000000000000000\n  -0.1748025119304656982421875000000000000000\n  0.0575263947248458862304687500000000000000\n  0.3986152410507202148437500000000000000000\n  -0.3845043778419494628906250000000000000000\n  0.2266369909048080444335937500000000000000\n  -0.1357069015502929687500000000000000000000\n  -0.1134672760963439941406250000000000000000\n  0.0793222337961196899414062500000000000000\n  -0.3684534430503845214843750000000000000000\n  -2.5517024993896484375000000000000000000000\n  -0.1876790821552276611328125000000000000000\n  0.0268291085958480834960937500000000000000\n  0.3709735274314880371093750000000000000000\n  -0.1004799753427505493164062500000000000000\n  -0.0015616677701473236083984375000000000000\n  0.0451832264661788940429687500000000000000\n  0.0476804450154304504394531250000000000000\n  0.2938563823699951171875000000000000000000\n  0.1084204092621803283691406250000000000000\n  -0.0952485799789428710937500000000000000000\n  0.1335937976837158203125000000000000000000\n  0.1499277949333190917968750000000000000000\n  0.0202903114259243011474609375000000000000\n  -0.1766919195652008056640625000000000000000\n  0.5979405641555786132812500000000000000000\n  -0.0643655732274055480957031250000000000000\n  -0.0522403754293918609619140625000000000000\n  0.1378012299537658691406250000000000000000\n  -0.0108505785465240478515625000000000000000\n  0.1064752638339996337890625000000000000000\n  0.0896374061703681945800781250000000000000\n  -0.2927702367305755615234375000000000000000\n  0.2406519204378128051757812500000000000000\n  0.0626429021358489990234375000000000000000\n  0.0631295666098594665527343750000000000000\n  -0.0272324122488498687744140625000000000000\n  -0.3124234080314636230468750000000000000000\n  -0.0058208629488945007324218750000000000000\n  0.0838831663131713867187500000000000000000\n  0.0858991518616676330566406250000000000000\n  -0.0588551983237266540527343750000000000000\n  0.0769425258040428161621093750000000000000\n  -0.2101571261882781982421875000000000000000\n  -0.1798554956912994384765625000000000000000\n  -3.8991343975067138671875000000000000000000\n  -0.0841243118047714233398437500000000000000\n  -0.3109065890312194824218750000000000000000\n  -0.3363238871097564697265625000000000000000\n  0.1374778747558593750000000000000000000000\n  -0.0757333636283874511718750000000000000000\n  0.6187845468521118164062500000000000000000\n  0.0946351736783981323242187500000000000000\n  -0.0044778473675251007080078125000000000000\n  0.0582370124757289886474609375000000000000\n  -0.1444973051548004150390625000000000000000\n  0.1644820421934127807617187500000000000000\n  -0.0836491286754608154296875000000000000000\n  -0.2016696780920028686523437500000000000000\n  -0.1253939270973205566406250000000000000000\n  0.1316413283348083496093750000000000000000\n  0.4500063061714172363281250000000000000000\n  -0.0237902067601680755615234375000000000000\n  0.1079125478863716125488281250000000000000\n  -0.0543914958834648132324218750000000000000\n  -0.2520788908004760742187500000000000000000\n  -0.1798981577157974243164062500000000000000\n  0.0463824272155761718750000000000000000000\n  0.3042268455028533935546875000000000000000\n  0.1902787238359451293945312500000000000000\n  0.1638420075178146362304687500000000000000\n  -0.5458615422248840332031250000000000000000\n  -0.0997942835092544555664062500000000000000\n  0.1134223714470863342285156250000000000000\n  -0.1457127928733825683593750000000000000000\n  0.1705539524555206298828125000000000000000\n  -0.4915557205677032470703125000000000000000\n  -0.2335881441831588745117187500000000000000\n  0.2703528106212615966796875000000000000000\n  -0.2081412225961685180664062500000000000000\n  -0.3163961768150329589843750000000000000000\n  0.0882139578461647033691406250000000000000\n  0.0928660407662391662597656250000000000000\n  0.3915141522884368896484375000000000000000\n  -0.2892025709152221679687500000000000000000\n  0.2065812945365905761718750000000000000000\n  0.3153192400932312011718750000000000000000\n  0.0897682011127471923828125000000000000000\n  0.0121184047311544418334960937500000000000\n  0.3482420146465301513671875000000000000000\n  0.0015491172671318054199218750000000000000\n  0.0766030400991439819335937500000000000000\n  0.3271803557872772216796875000000000000000\n  0.0488697327673435211181640625000000000000\n  0.1207644194364547729492187500000000000000\n  0.0244928151369094848632812500000000000000\n  0.1483182460069656372070312500000000000000\n  1.0175069570541381835937500000000000000000\n  -0.0644881576299667358398437500000000000000\n  0.1171315461397171020507812500000000000000\n  -0.0495755597949028015136718750000000000000\n  0.1144762784242630004882812500000000000000\n  -0.0027465000748634338378906250000000000000\n  -0.1055185347795486450195312500000000000000\n  0.2108231484889984130859375000000000000000\n  0.3655604422092437744140625000000000000000\n  -0.1616371572017669677734375000000000000000\n  0.2074591964483261108398437500000000000000\n  -0.0784822925925254821777343750000000000000\n  0.0419579967856407165527343750000000000000\n  -0.1216160580515861511230468750000000000000\n  0.3067493438720703125000000000000000000000\n  -0.0848599150776863098144531250000000000000\n  -0.0723632425069808959960937500000000000000\n  -0.1142441034317016601562500000000000000000\n  -0.4956518411636352539062500000000000000000\n  0.2294187247753143310546875000000000000000\n  0.1755967438220977783203125000000000000000\n  -0.4226806759834289550781250000000000000000\n  0.1973193287849426269531250000000000000000\n  -0.0957972779870033264160156250000000000000\n  -0.0947855189442634582519531250000000000000\n  0.1950935423374176025390625000000000000000\n  -0.0835177004337310791015625000000000000000\n  0.0650192871689796447753906250000000000000\n  -0.6205918788909912109375000000000000000000\n  0.0465734824538230895996093750000000000000\n  0.1397493630647659301757812500000000000000\n  0.3404798209667205810546875000000000000000\n  -0.0973578914999961853027343750000000000000\n  0.2439913302659988403320312500000000000000\n  0.0326647907495498657226562500000000000000\n  -0.1573407500982284545898437500000000000000\n  -0.1531492918729782104492187500000000000000\n  0.4753258824348449707031250000000000000000\n  -0.1557648032903671264648437500000000000000\n  0.0583147108554840087890625000000000000000\n  0.1667239964008331298828125000000000000000\n  0.0769977718591690063476562500000000000000\n  -0.0679289400577545166015625000000000000000\n  0.0363315492868423461914062500000000000000\n  0.1676276773214340209960937500000000000000\n  -0.2963781058788299560546875000000000000000\n  0.0742815434932708740234375000000000000000\n  -0.3312257826328277587890625000000000000000\n  0.3613545298576354980468750000000000000000\n  -0.3169350326061248779296875000000000000000\n  -0.2414239495992660522460937500000000000000\n  -0.1949927061796188354492187500000000000000\n  -0.1603540778160095214843750000000000000000\n  -0.0796177536249160766601562500000000000000\n  -0.3265589475631713867187500000000000000000\n  0.0690781921148300170898437500000000000000\n  0.0295884665101766586303710937500000000000\n  0.0318698845803737640380859375000000000000\n  -0.1074092015624046325683593750000000000000\n  0.4287797212600708007812500000000000000000\n  -0.2435192465782165527343750000000000000000\n  0.1299955844879150390625000000000000000000\n  1.0238146781921386718750000000000000000000\n  -0.2143792361021041870117187500000000000000\n  -0.2169280350208282470703125000000000000000\n  0.1156790778040885925292968750000000000000\n  0.1776789575815200805664062500000000000000\n  -0.0246520154178142547607421875000000000000\n  -0.0370741710066795349121093750000000000000\n  -0.0464306846261024475097656250000000000000\n  0.1259000003337860107421875000000000000000\n  0.0676974728703498840332031250000000000000\n  -0.1603874564170837402343750000000000000000\n  0.0488508976995944976806640625000000000000\n  0.1837034821510314941406250000000000000000\n  -0.0995243266224861145019531250000000000000\n  -0.2326496243476867675781250000000000000000\n  -0.1940098106861114501953125000000000000000\n  0.1903454214334487915039062500000000000000\n  -0.1117085218429565429687500000000000000000\n  -0.4303445518016815185546875000000000000000\n  -0.2931552529335021972656250000000000000000\n  0.0338390693068504333496093750000000000000\n  -0.0706735998392105102539062500000000000000\n  -0.1745567619800567626953125000000000000000\n  0.1491177678108215332031250000000000000000\n  0.2078863829374313354492187500000000000000\n  0.0110707022249698638916015625000000000000\n  0.2502206265926361083984375000000000000000\n  0.1351188719272613525390625000000000000000\n  0.0088176494464278221130371093750000000000\n  0.3738533556461334228515625000000000000000\n  0.3419964611530303955078125000000000000000\n  0.1289701759815216064453125000000000000000\n  -0.0356254167854785919189453125000000000000\n  0.1762538552284240722656250000000000000000\n  -0.0709580481052398681640625000000000000000\n  0.0885581597685813903808593750000000000000\n  0.0785668641328811645507812500000000000000\n  -0.6704692840576171875000000000000000000000\n  -0.0350090526044368743896484375000000000000\n  -0.0213848799467086791992187500000000000000\n  0.1505078226327896118164062500000000000000\n  0.4524377584457397460937500000000000000000\n  -0.0280276648700237274169921875000000000000\n  -0.0512338802218437194824218750000000000000\n  -0.2739193141460418701171875000000000000000\n  -0.0638110935688018798828125000000000000000\n  0.2777951359748840332031250000000000000000\n  0.0734342560172080993652343750000000000000\n  -1.7045531272888183593750000000000000000000\n  0.1665517240762710571289062500000000000000\n  0.1095723211765289306640625000000000000000\n  -0.0595071837306022644042968750000000000000\n  0.1869225949048995971679687500000000000000\n  -0.0399566292762756347656250000000000000000\n  -0.0547261945903301239013671875000000000000\n  -0.0236073210835456848144531250000000000000\n  0.2145759314298629760742187500000000000000\n  0.1190520524978637695312500000000000000000\n  -0.2314395308494567871093750000000000000000\n  -0.0546537712216377258300781250000000000000\n  0.1413490474224090576171875000000000000000\n  0.0441215522587299346923828125000000000000\n  0.2312214821577072143554687500000000000000\n  0.0307961199432611465454101562500000000000\n  0.0741991922259330749511718750000000000000\n  0.2407593727111816406250000000000000000000\n  -0.0247895941138267517089843750000000000000\n  0.1821978986263275146484375000000000000000\n  -0.1129403710365295410156250000000000000000\n  0.7340980172157287597656250000000000000000\n  0.1650781631469726562500000000000000000000\n  0.1882013231515884399414062500000000000000\n  0.2606922090053558349609375000000000000000\n  -0.2415369451045989990234375000000000000000\n  -0.2262332439422607421875000000000000000000\n  0.4316331148147583007812500000000000000000\n  0.1970994025468826293945312500000000000000\n  -0.0369030907750129699707031250000000000000\n  0.1752468347549438476562500000000000000000\n  -0.3313068151473999023437500000000000000000\n  0.0598050765693187713623046875000000000000\n  -0.0753872543573379516601562500000000000000\n  0.4316236078739166259765625000000000000000\n  0.2221853286027908325195312500000000000000\n  -0.1482722312211990356445312500000000000000\n  0.1987995058298110961914062500000000000000\n  0.0123978871852159500122070312500000000000\n  0.3613434433937072753906250000000000000000\n  -0.3099353015422821044921875000000000000000\n  0.2799298167228698730468750000000000000000\n  0.3859719634056091308593750000000000000000\n  0.1060370653867721557617187500000000000000\n  0.0314808338880538940429687500000000000000\n  0.0807936266064643859863281250000000000000\n  -0.1345767676830291748046875000000000000000\n  -0.3798382580280303955078125000000000000000\n  0.0546457171440124511718750000000000000000\n  0.0594653189182281494140625000000000000000\n  0.1833425313234329223632812500000000000000\n  0.1452979594469070434570312500000000000000\n  0.1014784872531890869140625000000000000000\n  -0.2840036749839782714843750000000000000000\n  -0.2823695540428161621093750000000000000000\n  -0.0585252232849597930908203125000000000000\n  -0.1248067319393157958984375000000000000000\n  -0.1155293658375740051269531250000000000000\n  -0.0530451089143753051757812500000000000000\n  -0.0095920842140913009643554687500000000000\n  0.1202828064560890197753906250000000000000\n  -0.4342721104621887207031250000000000000000\n  0.2034921646118164062500000000000000000000\n  0.1344493776559829711914062500000000000000\n  -0.1161876767873764038085937500000000000000\n  -0.7117428779602050781250000000000000000000\n  0.2850411236286163330078125000000000000000\n  -0.0807340294122695922851562500000000000000\n  -0.1530199795961380004882812500000000000000\n  -0.3078878223896026611328125000000000000000\n  0.3098181486129760742187500000000000000000\n  -0.0241770148277282714843750000000000000000\n  -0.5026336908340454101562500000000000000000\n  0.0674710571765899658203125000000000000000\n  0.0963512808084487915039062500000000000000\n  0.1632209271192550659179687500000000000000\n  0.2733746170997619628906250000000000000000\n  0.3414404690265655517578125000000000000000\n  -0.1693148314952850341796875000000000000000\n  -0.3236549198627471923828125000000000000000\n  0.1368647962808609008789062500000000000000\n  -0.4417271912097930908203125000000000000000\n  -0.2681506276130676269531250000000000000000\n  0.2874839603900909423828125000000000000000\n  0.0925559997558593750000000000000000000000\n  0.0758728981018066406250000000000000000000\n  0.0345133021473884582519531250000000000000\n  0.2277661114931106567382812500000000000000\n  0.0660802945494651794433593750000000000000\n  0.0017738919705152511596679687500000000000\n  -0.2605628371238708496093750000000000000000\n  -0.3189359903335571289062500000000000000000\n  0.0257332660257816314697265625000000000000\n  0.2865821123123168945312500000000000000000\n  0.2743372917175292968750000000000000000000\n  -0.1967374384403228759765625000000000000000\n  -0.0147842951118946075439453125000000000000\n  -0.1402860581874847412109375000000000000000\n  -0.2366952747106552124023437500000000000000\n  0.0012277355417609214782714843750000000000\n  -0.1328446567058563232421875000000000000000\n  -0.0197183787822723388671875000000000000000\n  -0.0201799012720584869384765625000000000000\n  0.1312263309955596923828125000000000000000\n  -0.0821386799216270446777343750000000000000\n  -0.0227123871445655822753906250000000000000\n  0.2959114015102386474609375000000000000000\n  -0.1756496876478195190429687500000000000000\n  -0.2436986565589904785156250000000000000000\n  0.1038982272148132324218750000000000000000\n  -0.1141758337616920471191406250000000000000\n  0.1366646140813827514648437500000000000000\n  0.0778284817934036254882812500000000000000\n  -0.0660053044557571411132812500000000000000\n  0.1666167378425598144531250000000000000000\n  0.0710818916559219360351562500000000000000\n  -0.3563314080238342285156250000000000000000\n  -0.0780594646930694580078125000000000000000\n  1.6740522384643554687500000000000000000000\n  0.1569082736968994140625000000000000000000\n  0.2507076561450958251953125000000000000000\n  0.0579352453351020812988281250000000000000\n  0.2746475636959075927734375000000000000000\n  0.0800773799419403076171875000000000000000\n  0.2047102004289627075195312500000000000000\n  -0.1499496996402740478515625000000000000000\n  -0.1264898031949996948242187500000000000000\n  0.0218252390623092651367187500000000000000\n  -0.0559111088514328002929687500000000000000\n  0.0427498221397399902343750000000000000000\n  -0.1367022097110748291015625000000000000000\n  -0.0323213040828704833984375000000000000000\n  0.1357490420341491699218750000000000000000\n  0.4567792117595672607421875000000000000000\n  -0.0223855413496494293212890625000000000000\n  -0.0871778279542922973632812500000000000000\n  -0.3668156862258911132812500000000000000000\n  -0.1368442326784133911132812500000000000000\n  -0.1175258234143257141113281250000000000000\n  0.0212057419121265411376953125000000000000\n  0.3798660933971405029296875000000000000000\n  -0.0221196003258228302001953125000000000000\n  -0.3780778050422668457031250000000000000000\n  0.0297306533902883529663085937500000000000\n  0.1656568497419357299804687500000000000000\n  -0.1816940307617187500000000000000000000000\n  -0.1889413446187973022460937500000000000000\n  -0.0851093083620071411132812500000000000000\n  -0.0038571795448660850524902343750000000000\n  0.0763948559761047363281250000000000000000\n  0.1589496582746505737304687500000000000000\n  0.0736963078379631042480468750000000000000\n  0.1110440641641616821289062500000000000000\n  -0.0711160898208618164062500000000000000000\n  -0.1549802720546722412109375000000000000000\n  -0.1251936107873916625976562500000000000000\n  -0.1197829842567443847656250000000000000000\n  0.0257638394832611083984375000000000000000\n  -0.0305985137820243835449218750000000000000\n  -0.2210496515035629272460937500000000000000\n  0.2468763142824172973632812500000000000000\n  -0.1778702437877655029296875000000000000000\n  0.0330792218446731567382812500000000000000\n  0.0836480259895324707031250000000000000000\n  0.0884579047560691833496093750000000000000\n  -0.3071962296962738037109375000000000000000\n  0.2750171720981597900390625000000000000000\n  0.5097516775131225585937500000000000000000\n  0.0010456796735525131225585937500000000000\n  -0.0959096625447273254394531250000000000000\n  0.0125840976834297180175781250000000000000\n  0.0593517124652862548828125000000000000000\n  -0.1140958517789840698242187500000000000000\n  0.1298242062330245971679687500000000000000\n  -0.0343137755990028381347656250000000000000\n  -0.3779603242874145507812500000000000000000\n  -0.1619412302970886230468750000000000000000\n  0.3356157243251800537109375000000000000000\n  -0.1488896459341049194335937500000000000000\n  0.5072698593139648437500000000000000000000\n  0.4824952483177185058593750000000000000000\n  -0.3794581294059753417968750000000000000000\n  0.0201909635215997695922851562500000000000\n  0.0185716673731803894042968750000000000000\n  -0.0629366040229797363281250000000000000000\n  0.0520991086959838867187500000000000000000\n  -0.1072030737996101379394531250000000000000\n  -0.0554358512163162231445312500000000000000\n  0.1599606722593307495117187500000000000000\n  0.2296994477510452270507812500000000000000\n  0.2384491413831710815429687500000000000000\n  0.3091536164283752441406250000000000000000\n  0.0114821307361125946044921875000000000000\n  -0.1974284648895263671875000000000000000000\n  -0.0039968416094779968261718750000000000000\n  -0.2103561609983444213867187500000000000000\n  -0.1221612095832824707031250000000000000000\n  -1.9764742851257324218750000000000000000000\n  -0.0272091552615165710449218750000000000000\n  0.2411813586950302124023437500000000000000\n  0.4853417575359344482421875000000000000000\n  0.2131534367799758911132812500000000000000\n  -0.0708608999848365783691406250000000000000\n  -0.0433964431285858154296875000000000000000\n  0.0233258828520774841308593750000000000000\n  -0.0541302636265754699707031250000000000000\n  0.2254400551319122314453125000000000000000\n  0.2183672040700912475585937500000000000000\n  0.3220705389976501464843750000000000000000\n  0.2558275163173675537109375000000000000000\n  0.1151167228817939758300781250000000000000\n  0.0052603753283619880676269531250000000000\n  0.0708802714943885803222656250000000000000\n  -0.0895232409238815307617187500000000000000\n  0.0549466758966445922851562500000000000000\n  -0.2082659900188446044921875000000000000000\n  -0.1637495309114456176757812500000000000000\n  -0.1185555905103683471679687500000000000000\n  0.2642731070518493652343750000000000000000\n  -0.1610702276229858398437500000000000000000\n  -0.0922030732035636901855468750000000000000\n  -0.2039327770471572875976562500000000000000\n  0.4200322031974792480468750000000000000000\n  0.0752066820859909057617187500000000000000\n  -0.3898092806339263916015625000000000000000\n  0.2106011658906936645507812500000000000000\n  0.1533819586038589477539062500000000000000\n  0.0953367352485656738281250000000000000000\n  0.2861562967300415039062500000000000000000\n  -0.0481946170330047607421875000000000000000\n  0.1282767057418823242187500000000000000000\n  0.1374807655811309814453125000000000000000\n  -0.3923578858375549316406250000000000000000\n  0.2805750370025634765625000000000000000000\n  -0.2916547656059265136718750000000000000000\n  -0.1566838920116424560546875000000000000000\n  0.2496568858623504638671875000000000000000\n  -0.0824895799160003662109375000000000000000\n  0.2281562685966491699218750000000000000000\n  0.0221429169178009033203125000000000000000\n  0.2155333310365676879882812500000000000000\n  -0.1149425804615020751953125000000000000000\n  0.1297246515750885009765625000000000000000\n  0.2009250819683074951171875000000000000000\n  -0.2187256515026092529296875000000000000000\n  0.6525444984436035156250000000000000000000\n  -0.2934409379959106445312500000000000000000\n  -0.1280640065670013427734375000000000000000\n  0.0793702602386474609375000000000000000000\n  0.0726855397224426269531250000000000000000\n  0.0679880902171134948730468750000000000000\n  0.1092821732163429260253906250000000000000\n  0.0823530703783035278320312500000000000000\n  0.0733584240078926086425781250000000000000\n  -0.0580668896436691284179687500000000000000\n  -0.0648048371076583862304687500000000000000\n  -0.2628716230392456054687500000000000000000\n  -0.2046963721513748168945312500000000000000\n  0.2010259330272674560546875000000000000000\n  -0.0521624013781547546386718750000000000000\n  0.2473520934581756591796875000000000000000\n  0.0610916316509246826171875000000000000000\n  -0.1580230742692947387695312500000000000000\n  -0.2937031090259552001953125000000000000000\n  0.1559340357780456542968750000000000000000\n  -0.0924995690584182739257812500000000000000\n  -0.1514770686626434326171875000000000000000\n  -0.0448849499225616455078125000000000000000\n  -0.3187240064144134521484375000000000000000\n  0.1261052638292312622070312500000000000000\n  -0.0339522883296012878417968750000000000000\n  0.1215923652052879333496093750000000000000\n  -0.1306513994932174682617187500000000000000\n  0.2115012258291244506835937500000000000000\n  0.3559450209140777587890625000000000000000\n  0.1881848871707916259765625000000000000000\n  0.2887943089008331298828125000000000000000\n  -0.0469298474490642547607421875000000000000\n  -0.0634924918413162231445312500000000000000\n  0.1650983393192291259765625000000000000000\n  -0.1808279156684875488281250000000000000000\n  0.2846289277076721191406250000000000000000\n  -5.3298339843750000000000000000000000000000\n  -0.1683974564075469970703125000000000000000\n  -0.2919130325317382812500000000000000000000\n  0.0027267080731689929962158203125000000000\n  -0.3573650419712066650390625000000000000000\n  -0.3407913446426391601562500000000000000000\n  0.3727463185787200927734375000000000000000\n  -0.1759054660797119140625000000000000000000\n  0.0433866158127784729003906250000000000000\n  -0.1666479855775833129882812500000000000000\n  0.1665399372577667236328125000000000000000\n  -0.0802728533744812011718750000000000000000\n  -0.2272039055824279785156250000000000000000\n  0.1691700518131256103515625000000000000000\n  0.6345713138580322265625000000000000000000\n  0.2318912148475646972656250000000000000000]]');
INSERT INTO `ms_file` (`file_id`, `file_name`, `file_content`, `file_url`, `file_content_vector`) VALUES
(8, 'YOLOv9__Learning_What_You_Want_to_Learn_Using_Programmable_Gradient_Information', '4202beF92]VC.sc[2v61631.2042:viXraYOLOv9: Learning What You Want to LearnUsing Programmable Gradient InformationChien-Yao Wang1,2, I-Hau Yeh2, and Hong-Yuan Mark Liao1,2,31Institute of Information Science, Academia Sinica, Taiwan2National Taipei University of Technology, Taiwan3Department of Information and Computer Engineering, Chung Yuan Christian University, Taiwankinyiu@iis.sinica.edu.tw, ihyeh@emc.com.tw, and liao@iis.sinica.edu.twAbstractToday’s deep learning methods focus on how to designthe most appropriate objective functions so that the pre-diction results of the model can be closest to the groundtruth. Meanwhile, an appropriate architecture that canfacilitate acquisition of enough information for predictionhas to be designed. Existing methods ignore a fact thatwhen input data undergoes layer-by-layer feature extrac-tion and spatial transformation, large amount of informa-tion will be lost. This paper will delve into the important is-sues of data loss when data is transmitted through deep net-works, namely information bottleneck and reversible func-tions. We proposed the concept of programmable gradi-ent information (PGI) to cope with the various changesrequired by deep networks to achieve multiple objectives.PGI can provide complete input information for the tar-get task to calculate objective function, so that reliablegradient information can be obtained to update networkweights. In addition, a new lightweight network architec-ture – Generalized Efficient Layer Aggregation Network(GELAN), based on gradient path planning is designed.GELAN’s architecture confirms that PGI has gained su-perior results on lightweight models. We verified the pro-posed GELAN and PGI on MS COCO dataset based ob-ject detection. The results show that GELAN only usesconventional convolution operators to achieve better pa-rameter utilization than the state-of-the-art methods devel-oped based on depth-wise convolution. PGI can be usedfor variety of models from lightweight to large. It can beused to obtain complete information, so that train-from-scratch models can achieve better results than state-of-the-art models pre-trained using large datasets, the compari-son results are shown in Figure 1. The source codes are at:https://github.com/WongKinYiu/yolov9.1. IntroductionDeep learning-based models have demonstrated far bet-ter performance than past artificial intelligence systems invarious fields, such as computer vision, language process-In recent years, researchersing, and speech recognition.Figure 1. Comparisons of the real-time object detecors on MSCOCO dataset. The GELAN and PGI-based object detectionmethod surpassed all previous train-from-scratch methods in termsof object detection performance. In terms of accuracy, the newmethod outperforms RT DETR [43] pre-trained with a largedataset, and it also outperforms depth-wise convolution-based de-sign YOLO MS [7] in terms of parameters utilization.in the field of deep learning have mainly focused on howto develop more powerful system architectures and learn-ing methods, such as CNNs [21–23, 42, 55, 71, 72], Trans-formers [8, 9, 40, 41, 60, 69, 70], Perceivers [26, 26, 32, 52,56, 81, 81], and Mambas [17, 38, 80].In addition, someresearchers have tried to develop more general objectivefunctions, such as loss function [5, 45, 46, 50, 77, 78], la-bel assignment [10, 12, 33, 67, 79] and auxiliary supervi-sion [18, 20, 24, 28, 29, 51, 54, 68, 76]. The above studiesall try to precisely find the mapping between input and tar-get tasks. However, most past approaches have ignored thatinput data may have a non-negligible amount of informa-tion loss during the feedforward process. This loss of in-formation can lead to biased gradient flows, which are sub-sequently used to update the model. The above problemscan result in deep networks to establish incorrect associa-tions between targets and inputs, causing the trained modelto produce incorrect predictions.1      Figure 2. Visualization results of random initial weight output feature maps for different network architectures: (a) input image, (b)PlainNet, (c) ResNet, (d) CSPNet, and (e) proposed GELAN. From the figure, we can see that in different architectures, the informationprovided to the objective function to calculate the loss is lost to varying degrees, and our architecture can retain the most completeinformation and provide the most reliable gradient information for calculating the objective function.In deep networks, the phenomenon of input data losinginformation during the feedforward process is commonlyknown as information bottleneck [59], and its schematic di-agram is as shown in Figure 2. At present, the main meth-ods that can alleviate this phenomenon are as follows: (1)The use of reversible architectures [3, 16, 19]: this methodmainly uses repeated input data and maintains the informa-tion of the input data in an explicit way; (2) The use ofmasked modeling [1, 6, 9, 27, 71, 73]: it mainly uses recon-struction loss and adopts an implicit way to maximize theextracted features and retain the input information; and (3)Introduction of the deep supervision concept [28,51,54,68]:it uses shallow features that have not lost too much impor-tant information to pre-establish a mapping from featuresto targets to ensure that important information can be trans-ferred to deeper layers. However, the above methods havedifferent drawbacks in the training process and inferenceprocess. For example, a reversible architecture requires ad-ditional layers to combine repeatedly fed input data, whichwill significantly increase the inference cost. In addition,since the input data layer to the output layer cannot have atoo deep path, this limitation will make it difficult to modelhigh-order semantic information during the training pro-cess. As for masked modeling, its reconstruction loss some-times conflicts with the target loss. In addition, most maskmechanisms also produce incorrect associations with data.For the deep supervision mechanism, it will produce erroraccumulation, and if the shallow supervision loses informa-tion during the training process, the subsequent layers willnot be able to retrieve the required information. The abovephenomenon will be more significant on difficult tasks andsmall models.To address the above-mentioned issues, we propose anew concept, which is programmable gradient information(PGI). The concept is to generate reliable gradients throughauxiliary reversible branch, so that the deep features canstill maintain key characteristics for executing target task.The design of auxiliary reversible branch can avoid the se-mantic loss that may be caused by a traditional deep super-vision process that integrates multi-path features. In otherwords, we are programming gradient information propaga-tion at different semantic levels, and thereby achieving thebest training results. The reversible architecture of PGI isbuilt on auxiliary branch, so there is no additional cost.Since PGI can freely select loss function suitable for thetarget task, it also overcomes the problems encountered bymask modeling. The proposed PGI mechanism can be ap-plied to deep neural networks of various sizes and is moregeneral than the deep supervision mechanism, which is onlysuitable for very deep neural networks.In this paper, we also designed generalized ELAN(GELAN) based on ELAN [65], the design of GELAN si-multaneously takes into account the number of parameters,computational complexity, accuracy and inference speed.This design allows users to arbitrarily choose appropriatecomputational blocks for different inference devices. Wecombined the proposed PGI and GELAN, and then de-signed a new generation of YOLO series object detectionsystem, which we call YOLOv9. We used the MS COCOdataset to conduct experiments, and the experimental resultsverified that our proposed YOLOv9 achieved the top perfor-mance in all comparisons.We summarize the contributions of this paper as follows:1. We theoretically analyzed the existing deep neural net-work architecture from the perspective of reversiblefunction, and through this process we successfully ex-plained many phenomena that were difficult to explainin the past. We also designed PGI and auxiliary re-versible branch based on this analysis and achieved ex-cellent results.2. The PGI we designed solves the problem that deep su-pervision can only be used for extremely deep neu-ral network architectures, and therefore allows newlightweight architectures to be truly applied in dailylife.3. The GELAN we designed only uses conventional con-volution to achieve a higher parameter usage than thedepth-wise convolution design that based on the mostadvanced technology, while showing great advantagesof being light, fast, and accurate.4. Combining the proposed PGI and GELAN, the objectdetection performance of the YOLOv9 on MS COCOdataset greatly surpasses the existing real-time objectdetectors in all aspects.22. Related work2.1. Real-time Object DetectorsThe current mainstream real-time object detectors are theYOLO series [2, 7, 13–15, 25, 30, 31, 47–49, 61–63, 74, 75],and most of these models use CSPNet [64] or ELAN [65]and their variants as the main computing units. In terms offeature integration, improved PAN [37] or FPN [35] is of-ten used as a tool, and then improved YOLOv3 head [49] orFCOS head [57, 58] is used as prediction head. Recentlysome real-time object detectors, such as RT DETR [43],which puts its fundation on DETR [4], have also been pro-posed. However, since it is extremely difficult for DETRseries object detector to be applied to new domains withouta corresponding domain pre-trained model, the most widelyused real-time object detector at present is still YOLO se-ries. This paper chooses YOLOv7 [63], which has beenproven effective in a variety of computer vision tasks andvarious scenarios, as a base to develop the proposed method.We use GELAN to improve the architecture and the trainingprocess with the proposed PGI. The above novel approachmakes the proposed YOLOv9 the top real-time object de-tector of the new generation.2.2. Reversible ArchitecturesThe operation unit of reversible architectures [3, 16, 19]must maintain the characteristics of reversible conversion,so it can be ensured that the output feature map of eachlayer of operation unit can retain complete original informa-tion. Before, RevCol [3] generalizes traditional reversibleunit to multiple levels, and in doing so can expand the se-mantic levels expressed by different layer units. Througha literature review of various neural network architectures,we found that there are many high-performing architectureswith varying degree of reversible properties. For exam-ple, Res2Net module [11] combines different input parti-tions with the next partition in a hierarchical manner, andconcatenates all converted partitions before passing thembackwards. CBNet [34, 39] re-introduces the original in-put data through composite backbone to obtain completeoriginal information, and obtains different levels of multi-level reversible information through various compositionmethods. These network architectures generally have ex-cellent parameter utilization, but the extra composite layerscause slow inference speeds. DynamicDet [36] combinesCBNet [34] and the high-efficiency real-time object detec-tor YOLOv7 [63] to achieve a very good trade-off amongspeed, number of parameters, and accuracy. This paper in-troduces the DynamicDet architecture as the basis for de-signing reversible branches.In addition, reversible infor-mation is further introduced into the proposed PGI. Theproposed new architecture does not require additional con-nections during the inference process, so it can fully retainthe advantages of speed, parameter amount, and accuracy.2.3. Auxiliary SupervisionDeep supervision [28, 54, 68] is the most common auxil-iary supervision method, which performs training by insert-ing additional prediction layers in the middle layers. Es-pecially the application of multi-layer decoders introducedin the transformer-based methods is the most common one.Another common auxiliary supervision method is to utilizethe relevant meta information to guide the feature maps pro-duced by the intermediate layers and make them have theproperties required by the target tasks [18, 20, 24, 29, 76].Examples of this type include using segmentation loss ordepth loss to enhance the accuracy of object detectors. Re-cently, there are many reports in the literature [53, 67, 82]that use different label assignment methods to generate dif-ferent auxiliary supervision mechanisms to speed up theconvergence speed of the model and improve the robustnessat the same time. However, the auxiliary supervision mech-anism is usually only applicable to large models, so whenit is applied to lightweight models, it is easy to cause anunder parameterization phenomenon, which makes the per-formance worse. The PGI we proposed designed a way toreprogram multi-level semantic information, and this designallows lightweight models to also benefit from the auxiliarysupervision mechanism.3. Problem StatementUsually, people attribute the difficulty of deep neural net-work convergence problem due to factors such as gradientvanish or gradient saturation, and these phenomena do ex-ist in traditional deep neural networks. However, moderndeep neural networks have already fundamentally solvedthe above problem by designing various normalization andactivation functions. Nevertheless, deep neural networksstill have the problem of slow convergence or poor conver-gence results.In this paper, we explore the nature of the above issuefurther. Through in-depth analysis of information bottle-neck, we deduced that the root cause of this problem is thatthe initial gradient originally coming from a very deep net-work has lost a lot of information needed to achieve thegoal soon after it is transmitted.In order to confirm thisinference, we feedforward deep networks of different archi-tectures with initial weights, and then visualize and illus-trate them in Figure 2. Obviously, PlainNet has lost a lot ofimportant information required for object detection in deeplayers. As for the proportion of important information thatResNet, CSPNet, and GELAN can retain, it is indeed posi-tively related to the accuracy that can be obtained after train-ing. We further design reversible network-based methods tosolve the causes of the above problems. In this section weshall elaborate our analysis of information bottleneck prin-ciple and reversible functions.33.1. Information Bottleneck PrincipleAccording to information bottleneck principle, we knowthat data X may cause information loss when going throughtransformation, as shown in Eq. 1 below:I(X, X) ≥ I(X, fθ(X)) ≥ I(X, gϕ(fθ(X))),(1)where I indicates mutual information, f and g are transfor-mation functions, and θ and ϕ are parameters of f and g,respectively.In deep neural networks, fθ(·) and gϕ(·) respectivelyrepresent the operations of two consecutive layers in deepneural network. From Eq. 1, we can predict that as the num-ber of network layer becomes deeper, the original data willbe more likely to be lost. However, the parameters of thedeep neural network are based on the output of the networkas well as the given target, and then update the network aftergenerating new gradients by calculating the loss function.As one can imagine, the output of a deeper neural networkis less able to retain complete information about the pre-diction target. This will make it possible to use incompleteinformation during network training, resulting in unreliablegradients and poor convergence.One way to solve the above problem is to directly in-crease the size of the model. When we use a large numberof parameters to construct a model, it is more capable ofperforming a more complete transformation of the data. Theabove approach allows even if information is lost during thedata feedforward process, there is still a chance to retainenough information to perform the mapping to the target.The above phenomenon explains why the width is more im-portant than the depth in most modern models. However,the above conclusion cannot fundamentally solve the prob-lem of unreliable gradients in very deep neural network.Below, we will introduce how to use reversible functionsto solve problems and conduct relative analysis.3.2. Reversible FunctionsWhen a function r has an inverse transformation func-tion v, we call this function reversible function, as shown inEq. 2.X = vζ(rψ(X)),(2)where ψ and ζ are parameters of r and v, respectively. DataX is converted by reversible function without losing infor-mation, as shown in Eq. 3.I(X, X) = I(X, rψ(X)) = I(X, vζ(rψ(X))).(3)When the network’s transformation function is composedof reversible functions, more reliable gradients can be ob-tained to update the model. Almost all of today’s populardeep learning methods are architectures that conform to thereversible property, such as Eq. 4.X l+1 = X l + f l+1θ(X l),(4)where l indicates the l-th layer of a PreAct ResNet andf is the transformation function of the l-th layer. PreActResNet [22] repeatedly passes the original data X to sub-sequent layers in an explicit way. Although such a designcan make a deep neural network with more than a thousandlayers converge very well, it destroys an important reasonwhy we need deep neural networks. That is, for difficultproblems, it is difficult for us to directly find simple map-ping functions to map data to targets. This also explainswhy PreAct ResNet performs worse than ResNet [21] whenthe number of layers is small.In addition, we tried to use masked modeling that al-lowed the transformer model to achieve significant break-throughs. We use approximation methods, such as Eq. 5,to try to find the inverse transformation v of r, so that thetransformed features can retain enough information usingsparse features. The form of Eq. 5 is as follows:X = vζ(rψ(X) · M ),(5)where M is a dynamic binary mask. Other methods thatare commonly used to perform the above tasks are diffusionmodel and variational autoencoder, and they both have thefunction of finding the inverse function. However, whenwe apply the above approach to a lightweight model, therewill be defects because the lightweight model will be underparameterized to a large amount of raw data. Because ofthe above reason, important information I(Y, X) that mapsdata X to target Y will also face the same problem. For thisissue, we will explore it using the concept of informationbottleneck [59]. The formula for information bottleneck isas follows:I(X, X) ≥ I(Y, X) ≥ I(Y, fθ(X)) ≥ ... ≥ I(Y, ˆY ). (6)Generally speaking, I(Y, X) will only occupy a very smallpart of I(X, X). However, it is critical to the target mis-sion. Therefore, even if the amount of information lost inthe feedforward stage is not significant, as long as I(Y, X)is covered, the training effect will be greatly affected. Thelightweight model itself is in an under parameterized state,so it is easy to lose a lot of important information in thefeedforward stage. Therefore, our goal for the lightweightmodel is how to accurately filter I(Y, X) from I(X, X). Asfor fully preserving the information of X, that is difficult toachieve. Based on the above analysis, we hope to propose anew deep neural network training method that can not onlygenerate reliable gradients to update the model, but also besuitable for shallow and lightweight neural networks.4(a) Path Aggregation Network (PAN)) [37], (b) Reversible ColumnsFigure 3. PGI and related network architectures and methods.(RevCol) [3], (c) conventional deep supervision, and (d) our proposed Programmable Gradient Information (PGI). PGI is mainly composedof three components: (1) main branch: architecture used for inference, (2) auxiliary reversible branch: generate reliable gradients to supplymain branch for backward transmission, and (3) multi-level auxiliary information: control main branch learning plannable multi-level ofsemantic information.4. Methodology4.1. Programmable Gradient InformationIn order to solve the aforementioned problems, we pro-pose a new auxiliary supervision framework called Pro-grammable Gradient Information (PGI), as shown in Fig-ure 3 (d). PGI mainly includes three components, namely(1) main branch, (2) auxiliary reversible branch, and (3)multi-level auxiliary information. From Figure 3 (d) wesee that the inference process of PGI only uses main branchand therefore does not require any additional inference cost.As for the other two components, they are used to solve orslow down several important issues in deep learning meth-ods. Among them, auxiliary reversible branch is designedto deal with the problems caused by the deepening of neuralnetworks. Network deepening will cause information bot-tleneck, which will make the loss function unable to gener-ate reliable gradients. As for multi-level auxiliary informa-tion, it is designed to handle the error accumulation problemcaused by deep supervision, especially for the architectureand lightweight model of multiple prediction branch. Next,we will introduce these two components step by step.4.1.1 Auxiliary Reversible BranchIn PGI, we propose auxiliary reversible branch to gener-ate reliable gradients and update network parameters. Byproviding information that maps from data to targets, theloss function can provide guidance and avoid the possibil-ity of finding false correlations from incomplete feedfor-ward features that are less relevant to the target. We pro-pose the maintenance of complete information by introduc-ing reversible architecture, but adding main branch to re-versible architecture will consume a lot of inference costs.We analyzed the architecture of Figure 3 (b) and found thatwhen additional connections from deep to shallow layersare added, the inference time will increase by 20%. Whenwe repeatedly add the input data to the high-resolution com-puting layer of the network (yellow box), the inference timeeven exceeds twice the time.Since our goal is to use reversible architecture to ob-tain reliable gradients, “reversible” is not the only neces-sary condition in the inference stage. In view of this, weregard reversible branch as an expansion of deep supervi-sion branch, and then design auxiliary reversible branch, asshown in Figure 3 (d). As for the main branch deep fea-tures that would have lost important information due to in-formation bottleneck, they will be able to receive reliablegradient information from the auxiliary reversible branch.These gradient information will drive parameter learning toassist in extracting correct and important information, andthe above actions can enable the main branch to obtain fea-tures that are more effective for the target task. Moreover,the reversible architecture performs worse on shallow net-works than on general networks because complex tasks re-quire conversion in deeper networks. Our proposed methoddoes not force the main branch to retain complete origi-nal information but updates it by generating useful gradientthrough the auxiliary supervision mechanism. The advan-tage of this design is that the proposed method can also beapplied to shallower networks.5Figure 4. The architecture of GELAN: (a) CSPNet [64], (b) ELAN [65], and (c) proposed GELAN. We imitate CSPNet and extend ELANinto GELAN that can support any computational blocks.Finally, since auxiliary reversible branch can be removedduring the inference phase, the inference capabilities of theoriginal network can be retained. We can also choose anyreversible architectures in PGI to play the role of auxiliaryreversible branch.4.1.2 Multi-level Auxiliary InformationIn this section we will discuss how multi-level auxiliary in-formation works. The deep supervision architecture includ-ing multiple prediction branch is shown in Figure 3 (c). Forobject detection, different feature pyramids can be used toperform different tasks, for example together they can de-tect objects of different sizes. Therefore, after connectingto the deep supervision branch, the shallow features will beguided to learn the features required for small object detec-tion, and at this time the system will regard the positionsof objects of other sizes as the background. However, theabove deed will cause the deep feature pyramids to lose a lotof information needed to predict the target object. Regard-ing this issue, we believe that each feature pyramid needsto receive information about all target objects so that subse-quent main branch can retain complete information to learnpredictions for various targets.The concept of multi-level auxiliary information is to in-sert an integration network between the feature pyramid hi-erarchy layers of auxiliary supervision and the main branch,and then uses it to combine returned gradients from differ-ent prediction heads, as shown in Figure 3 (d). Multi-levelauxiliary information is then to aggregate the gradient infor-mation containing all target objects, and pass it to the mainbranch and then update parameters. At this time, the charac-teristics of the main branch’s feature pyramid hierarchy willnot be dominated by some specific object’s information. Asa result, our method can alleviate the broken informationproblem in deep supervision.In addition, any integratednetwork can be used in multi-level auxiliary information.Therefore, we can plan the required semantic levels to guidethe learning of network architectures of different sizes.4.2. Generalized ELANIn this Section we describe the proposed new networkarchitecture – GELAN. By combining two neural networkarchitectures, CSPNet [64] and ELAN [65], which are de-signed with gradient path planning, we designed gener-alized efficient layer aggregation network (GELAN) thattakes into account lighweight, inference speed, and accu-racy. Its overall architecture is shown in Figure 4. We gen-eralized the capability of ELAN [65], which originally onlyused stacking of convolutional layers, to a new architecturethat can use any computational blocks.5. Experiments5.1. Experimental SetupWe verify the proposed method with MS COCO dataset.All experimental setups follow YOLOv7 AF [63], while thedataset is MS COCO 2017 splitting. All models we men-tioned are trained using the train-from-scratch strategy, andthe total number of training times is 500 epochs. In settingthe learning rate, we use linear warm-up in the first threeepochs, and the subsequent epochs set the correspondingdecay manner according to the model scale. As for the last15 epochs, we turn mosaic data augmentation off. For moresettings, please refer to Appendix.5.2. Implimentation DetailsWe built general and extended version of YOLOv9 basedon YOLOv7 [63] and Dynamic YOLOv7 [36] respectively.In the design of the network architecture, we replacedELAN [65] with GELAN using CSPNet blocks [64] withplanned RepConv [63] as computational blocks. We alsosimplified downsampling module and optimized anchor-free prediction head. As for the auxiliary loss part of PGI,we completely follow YOLOv7’s auxiliary head setting.Please see Appendix for more details.6ModelYOLOv5-N r7.0 [14]YOLOv5-S r7.0 [14]YOLOv5-M r7.0 [14]YOLOv5-L r7.0 [14]YOLOv5-X r7.0 [14]YOLOv6-N v3.0 [30]YOLOv6-S v3.0 [30]YOLOv6-M v3.0 [30]YOLOv6-L v3.0 [30]YOLOv7 [63]YOLOv7-X [63]YOLOv7-N AF [63]YOLOv7-S AF [63]YOLOv7 AF [63]YOLOv8-N [15]YOLOv8-S [15]YOLOv8-M [15]YOLOv8-L [15]YOLOv8-X [15]DAMO YOLO-T [75]DAMO YOLO-S [75]DAMO YOLO-M [75]DAMO YOLO-L [75]Gold YOLO-N [61]Gold YOLO-S [61]Gold YOLO-M [61]Gold YOLO-L [61]YOLO MS-N [7]YOLO MS-S [7]YOLO MS [7]GELAN-S (Ours)GELAN-M (Ours)GELAN-C (Ours)GELAN-E (Ours)YOLOv9-S (Ours)YOLOv9-M (Ours)YOLOv9-C (Ours)YOLOv9-E (Ours)#Param. (M) FLOPs (G) APvalTable 1. Comparison of state-of-the-art real-time object detectors.75 (%) APval50 (%) APval50:95 (%) APval––45.728.0––56.837.4––64.145.4––67.349.0––68.950.74.516.549.0109.1205.71.97.221.246.586.7S (%) APval–––––M (%) APval–––––L (%)4.718.534.959.636.971.33.111.043.63.211.225.943.768.28.512.328.242.15.621.541.375.14.58.122.27.120.025.357.37.120.025.357.311.445.385.8150.7104.7189.98.728.1130.58.728.678.9165.2257.818.137.861.897.312.146.087.5151.717.431.280.226.476.3102.1189.026.476.3102.1189.037.044.349.151.851.252.937.645.153.037.344.950.252.953.942.046.049.250.839.645.449.851.843.446.251.046.751.152.555.046.851.453.055.652.761.266.169.269.771.153.361.870.252.661.867.269.871.058.061.965.567.555.762.567.068.960.463.768.663.067.969.571.963.468.170.272.8––––55.951.440.648.957.5–––57.558.745.249.553.055.5––––47.650.555.750.755.757.360.050.756.157.860.6––––31.836.918.725.735.8–––35.335.723.025.929.733.219.725.332.334.123.726.933.125.933.635.838.026.633.636.240.2––––55.557.741.750.258.7–––58.359.346.150.653.155.744.150.255.357.448.350.556.151.556.457.660.656.057.058.561.0––––65.068.652.861.268.9–––69.870.758.562.566.166.657.062.666.368.260.363.066.564.067.369.470.964.568.069.371.45.3. Comparison with state-of-the-artsTable 1 lists comparison of our proposed YOLOv9 withother train-from-scratch real-time object detectors. Over-all, the best performing methods among existing methodsare YOLO MS-S [7] for lightweight models, YOLO MS [7]for medium models, YOLOv7 AF [63] for general mod-els, and YOLOv8-X [15] for large models. Compared withlightweight and medium model YOLO MS [7], YOLOv9has about 10% less parameters and 5∼15% less calcula-tions, but still has a 0.4∼0.6% improvement in AP. Com-pared with YOLOv7 AF, YOLOv9-C has 42% less pa-rameters and 22% less calculations, but achieves the sameAP (53%). Compared with YOLOv8-X, YOLOv9-E has16% less parameters, 27% less calculations, and has sig-nificant improvement of 1.7% AP. The above comparisonresults show that our proposed YOLOv9 has significantlyimproved in all aspects compared with existing methods.On the other hand, we also include ImageNet pretrainedmodel in the comparison, and the results are shown in Fig-ure 5. We compare them based on the parameters and theamount of computation respectively. In terms of the num-ber of parameters, the best performing large model is RTDETR [43]. From Figure 5, we can see that YOLOv9 usingconventional convolution is even better than YOLO MS us-ing depth-wise convolution in parameter utilization. As forthe parameter utilization of large models, it also greatly sur-passes RT DETR using ImageNet pretrained model. Evenbetter is that in the deep model, YOLOv9 shows the hugeadvantages of using PGI. By accurately retaining and ex-tracting the information needed to map the data to the tar-get, our method requires only 66% of the parameters whilemaintaining the accuracy as RT DETR-X.7Figure 5. Comparison of state-of-the-art real-time object detectors. The methods participating in the comparison all use ImageNet aspre-trained weights, including RT DETR [43], RTMDet [44], and PP-YOLOE [74], etc. The YOLOv9 that uses train-from-scratch methodclearly surpasses the performance of other methods.As for the amount of computation, the best existing mod-els from the smallest to the largest are YOLO MS [7], PPYOLOE [74], and RT DETR [43]. From Figure 5, we cansee that YOLOv9 is far superior to the train-from-scratchmethods in terms of computational complexity.In addi-tion, if compared with those based on depth-wise convo-lution and ImageNet-based pretrained models, YOLOv9 isalso very competitive.5.4. Ablation Studies5.4.1 Generalized ELANFor GELAN, we first do ablation studies for computationalblocks. We used Res blocks [21], Dark blocks [49], andCSP blocks [64] to conduct experiments, respectively. Ta-ble 2 shows that after replacing convolutional layers inELAN with different computational blocks, the system canmaintain good performance. Users are indeed free to re-place computational blocks and use them on their respectiveinference devices. Among different computational block re-placements, CSP blocks perform particularly well. Theynot only reduce the amount of parameters and computation,but also improve AP by 0.7%. Therefore, we choose CSP-ELAN as the component unit of GELAN in YOLOv9.Table 2. Ablation study on various computational blocks.ModelCB type#Param.FLOPsGELAN-SGELAN-SGELAN-SGELAN-SConvRes [21]Dark [49]CSP [64]6.2M5.4M5.7M5.9M23.5G21.0G21.8G22.4GAPval50:9544.8%44.3%44.5%45.5%1 CB type nedotes as computational block type.2 -S nedotes small size model.Next, we conduct ELAN block-depth and CSP block-depth experiments on GELAN of different sizes, and dis-play the results in Table 3. We can see that when the depthof ELAN is increased from 1 to 2, the accuracy is signif-icantly improved. But when the depth is greater than orequal to 2, no matter it is improving the ELAN depth or theCSP depth, the number of parameters, the amount of com-putation, and the accuracy will always show a linear rela-tionship. This means GELAN is not sensitive to the depth.In other words, users can arbitrarily combine the compo-nents in GELAN to design the network architecture, andhave a model with stable performance without special de-sign. In Table 3, for YOLOv9-{S,M,C}, we set the pairingof the ELAN depth and the CSP depth to {{2, 3}, {2, 1},{2, 1}}.Table 3. Ablation study on ELAN and CSP depth.ModelDELAN DCSP#Param.FLOPs APval50:95GELAN-SGELAN-SGELAN-SGELAN-SGELAN-MGELAN-MGELAN-MGELAN-MGELAN-CGELAN-CGELAN-CGELAN-CGELAN-C223222321223212131213112135.9M6.5M7.1M7.1M20.0M22.2M24.3M24.4M18.9M25.3M28.6M31.7M31.9M22.4G24.4G26.3G26.4G76.3G85.1G93.5G94.0G77.5G102.1G114.4G126.8G126.7G45.5%46.0%46.5%46.7%51.1%51.7%51.8%52.3%50.7%52.5%53.0%53.2%53.3%1 DELAN and DCSP respectively nedotes depth of ELAN and CSP.2 -{S, M, C} indicate small, medium, and compact models.85.4.2 Programmable Gradient InformationTable 5. Ablation study on PGI.In terms of PGI, we performed ablation studies on auxiliaryreversible branch and multi-level auxiliary information onthe backbone and neck, respectively. We designed auxiliaryreversible branch ICN to use DHLC [34] linkage to obtainmulti-level reversible information. As for multi-level aux-iliary information, we use FPN and PAN for ablation stud-ies and the role of PFH is equivalent to the traditional deepsupervision. The results of all experiments are listed in Ta-ble 4. From Table 4, we can see that PFH is only effective indeep models, while our proposed PGI can improve accuracyunder different combinations. Especially when using ICN,we get stable and better results. We also tried to apply thelead-head guided assignment proposed in YOLOv7 [63] tothe PGI’s auxiliary supervision, and achieved much betterperformance.Table 4. Ablation study on PGI of backbone and neck.Gbackbone Gneck APvalModelGELAN-CGELAN-CGELAN-CGELAN-CGELAN-CGELAN-CGELAN-C LHG-ICN–PFHFPN–FPNICNGELAN-EGELAN-EGELAN-EGELAN-EGELAN-E–PFHFPNPANFPN–––ICNICN––––––ICNSAPval50:95 APvalM APvalL52.5% 35.8% 57.6% 69.4%52.5% 35.3% 58.1% 68.9%52.6% 35.3% 58.1% 68.9%52.7% 35.3% 58.4% 68.9%52.8% 35.8% 58.2% 69.1%52.9% 35.2% 58.7% 68.6%53.0% 36.3% 58.5% 69.1%55.0% 38.0% 60.6% 70.9%55.3% 38.3% 60.3% 71.6%55.6% 40.2% 61.0% 71.4%55.5% 39.0% 61.1% 71.5%55.6% 39.8% 60.9% 71.9%ModelGELAN-S+ DS+ PGIAPval50:9546.7%46.5%-0.246.8% +0.1APval5063.0%62.9% -0.163.4% +0.4APval7550.7%50.5% -0.250.7%=GELAN-M 51.1%+ DS51.2%+0.151.4% +0.3+ PGI67.9%68.2% +0.368.1% +0.255.7%55.7%56.1% +0.4=GELAN-C+ DS+ PGIGELAN-E+ DS+ PGI52.5%52.5%53.0% +0.5=69.5%69.9% +0.470.3% +0.857.3%57.1% -0.257.8% +0.555.0%55.3%+0.355.6% +0.671.9%72.3% +0.472.8% +0.960.0%60.2% +0.260.6% +0.61 DS indicates deep supervision.2 -{S, M, C, E} indicate small, medium, compact, and extended models.Finally, we show in the table the results of gradually in-creasing components from baseline YOLOv7 to YOLOv9-E. The GELAN and PGI we proposed have brought all-round improvement to the model.Table 6. Ablation study on GELAN and PGI.ModelYOLOv7 [63]+ AF [63]+ GELAN+ DHLC [34]+ PGISAPval50:95 APval#Param. FLOPs APval104.7130.5126.4189.0189.0M APvalL51.2% 31.8% 55.5% 65.0%53.0% 35.8% 58.7% 68.9%53.2% 36.2% 58.5% 69.9%55.0% 38.0% 60.6% 70.9%55.6% 40.2% 61.0% 71.4%36.943.641.257.357.31 DELAN and DCSP respectively nedotes depth of ELAN and CSP.2 LHG indicates lead head guided training proposed by YOLOv7 [63].We further implemented the concepts of PGI and deepsupervision on models of various sizes and compared theresults, these results are shown in Table 5. As analyzed atthe beginning, introduction of deep supervision will causea loss of accuracy for shallow models. As for general mod-els, introducing deep supervision will cause unstable perfor-mance, and the design concept of deep supervision can onlybring gains in extremely deep models. The proposed PGIcan effectively handle problems such as information bottle-neck and information broken, and can comprehensively im-prove the accuracy of models of different sizes. The conceptof PGI brings two valuable contributions. The first one is tomake the auxiliary supervision method applicable to shal-low models, while the second one is to make the deep modeltraining process obtain more reliable gradients. These gra-dients enable deep models to use more accurate informationto establish correct correlations between data and targets.5.5. VisualizationThis section will explore the information bottleneck is-sues and visualize them. In addition, we will also visualizehow the proposed PGI uses reliable gradients to find thecorrect correlations between data and targets. In Figure 6we show the visualization results of feature maps obtainedby using random initial weights as feedforward under dif-ferent architectures. We can see that as the number of lay-ers increases, the original information of all architecturesgradually decreases. For example, at the 50th layer of thePlainNet, it is difficult to see the location of objects, and alldistinguishable features will be lost at the 100th layer. Asfor ResNet, although the position of object can still be seenat the 50th layer, the boundary information has been lost.When the depth reached to the 100th layer, the whole imagebecomes blurry. Both CSPNet and the proposed GELANperform very well, and they both can maintain features thatsupport clear identification of objects until the 200th layer.Among the comparisons, GELAN has more stable resultsand clearer boundary information.9Figure 6. Feature maps (visualization results) output by random initial weights of PlainNet, ResNet, CSPNet, and GELAN at differentdepths. After 100 layers, ResNet begins to produce feedforward output that is enough to obfuscate object information. Our proposedGELAN can still retain quite complete information up to the 150th layer, and is still sufficiently discriminative up to the 200th layer.ject boundaries, and it also produced unexpected responsesin some background areas. This experiment confirms thatPGI can indeed provide better gradients to update parame-ters and enable the feedforward stage of the main branch toretain more important features.6. ConclusionsIn this paper, we propose to use PGI to solve the infor-mation bottleneck problem and the problem that the deepsupervision mechanism is not suitable for lightweight neu-ral networks. We designed GELAN, a highly efficientand lightweight neural network. In terms of object detec-tion, GELAN has strong and stable performance at differentcomputational blocks and depth settings. It can indeed bewidely expanded into a model suitable for various inferencedevices. For the above two issues, the introduction of PGIallows both lightweight models and deep models to achievesignificant improvements in accuracy. The YOLOv9, de-signed by combining PGI and GELAN, has shown strongcompetitiveness. Its excellent design allows the deep modelto reduce the number of parameters by 49% and the amountof calculations by 43% compared with YOLOv8, but it stillhas a 0.6% AP improvement on MS COCO dataset.7. AcknowledgementsThe authors wish to thank National Center for High-performance Computing (NCHC) for providing computa-tional and storage resources.Figure 7. PAN feature maps (visualization results) of GELANand YOLOv9 (GELAN + PGI) after one epoch of bias warm-up.GELAN originally had some divergence, but after adding PGI’sreversible branch, it is more capable of focusing on the target ob-ject.Figure 7 is used to show whether PGI can provide morereliable gradients during the training process, so that theparameters used for updating can effectively capture therelationship between the input data and the target. Fig-ure 7 shows the visualization results of the feature map ofGELAN and YOLOv9 (GELAN + PGI) in PAN bias warm-up. From the comparison of Figure 7(b) and (c), we canclearly see that PGI accurately and concisely captures thearea containing objects. As for GELAN that does not usePGI, we found that it had divergence when detecting ob-10References[1] Hangbo Bao, Li Dong, Songhao Piao, and Furu Wei. BEiT:BERT pre-training of image transformers. In InternationalConference on Learning Representations (ICLR), 2022. 2[2] Alexey Bochkovskiy, Chien-Yao Wang,and Hong-Yuan Mark Liao. YOLOv4: Optimal speed and accuracy ofobject detection. arXiv preprint arXiv:2004.10934, 2020. 3[3] Yuxuan Cai, Yizhuang Zhou, Qi Han, Jianjian Sun, Xiang-wen Kong, Jun Li, and Xiangyu Zhang. Reversible columnnetworks. In International Conference on Learning Repre-sentations (ICLR), 2023. 2, 3, 5[4] Nicolas Carion, Francisco Massa, Gabriel Synnaeve, NicolasUsunier, Alexander Kirillov, and Sergey Zagoruyko. End-In Proceedingsto-end object detection with transformers.of the European Conference on Computer Vision (ECCV),pages 213–229, 2020. 3[5] Kean Chen, Weiyao Lin, Jianguo Li, John See, Ji Wang, andJunni Zou. AP-loss for accurate one-stage object detection.IEEE Transactions on Pattern Analysis and Machine Intelli-gence (TPAMI), 43(11):3782–3798, 2020. 1[6] Yabo Chen, Yuchen Liu, Dongsheng Jiang, Xiaopeng Zhang,Wenrui Dai, Hongkai Xiong, and Qi Tian. SdAE: Self-distillated masked autoencoder. In Proceedings of the Euro-pean Conference on Computer Vision (ECCV), pages 108–124, 2022. 2[7] Yuming Chen, Xinbin Yuan, Ruiqi Wu, Jiabao Wang, QibinHou, and Ming-Ming Cheng. YOLO-MS: rethinking multi-scale representation learning for real-time object detection.arXiv preprint arXiv:2308.05480, 2023. 1, 3, 7, 8[8] Mingyu Ding, Bin Xiao, Noel Codella, Ping Luo, JingdongWang, and Lu Yuan. DaVIT: Dual attention vision trans-In Proceedings of the European Conference onformers.Computer Vision (ECCV), pages 74–92, 2022. 1[9] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov,Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner,Mostafa Dehghani, Matthias Minderer, Georg Heigold, Syl-vain Gelly, et al. An image is worth 16x16 words: Trans-formers for image recognition at scale. In International Con-ference on Learning Representations (ICLR), 2021. 1, 2[10] Chengjian Feng, Yujie Zhong, Yu Gao, Matthew R Scott,and Weilin Huang. TOOD: Task-aligned one-stage objectIn Proceedings of the IEEE/CVF Internationaldetection.Conference on Computer Vision (ICCV), pages 3490–3499,2021. 1[11] Shang-Hua Gao, Ming-Ming Cheng, Kai Zhao, Xin-YuZhang, Ming-Hsuan Yang, and Philip Torr. Res2Net: AIEEE Transac-new multi-scale backbone architecture.tions on Pattern Analysis and Machine Intelligence (TPAMI),43(2):652–662, 2019. 3[12] Zheng Ge, Songtao Liu, Zeming Li, Osamu Yoshie, and JianSun. OTA: Optimal transport assignment for object detec-tion. In Proceedings of the IEEE/CVF Conference on Com-puter Vision and Pattern Recognition (CVPR), pages 303–312, 2021. 1[13] Zheng Ge, Songtao Liu, Feng Wang, Zeming Li, and JianSun. YOLOX: Exceeding YOLO series in 2021. arXivpreprint arXiv:2107.08430, 2021. 3[14] Jocher Glenn. YOLOv5 release v7.0. https://github.com/ultralytics/yolov5/releases/tag/v7.0, 2022. 3, 7[15] Jocher Glenn.https :/ / github . com / ultralytics / ultralytics /releases/tag/v8.1.0, 2024. 3, 7YOLOv8 release v8.1.0.[16] Aidan N Gomez, Mengye Ren, Raquel Urtasun, and Roger BGrosse. The reversible residual network: Backpropagationwithout storing activations. Advances in Neural InformationProcessing Systems (NeurIPS), 2017. 2, 3[17] Albert Gu and Tri Dao. Mamba: Linear-time sequencearXiv preprintmodeling with selective state spaces.arXiv:2312.00752, 2023. 1[18] Chaoxu Guo, Bin Fan, Qian Zhang, Shiming Xiang, andAugFPN: Improving multi-scale fea-Chunhong Pan.In Proceedings of theture learning for object detection.IEEE/CVF Conference on Computer Vision and PatternRecognition (CVPR), pages 12595–12604, 2020. 1, 3[19] Qi Han, Yuxuan Cai, and Xiangyu Zhang. RevColV2: Ex-ploring disentangled representations in masked image mod-eling. Advances in Neural Information Processing Systems(NeurIPS), 2023. 2, 3[20] Zeeshan Hayder, Xuming He, and Mathieu Salzmann.In Proceedings ofBoundary-aware instance segmentation.the IEEE/CVF Conference on Computer Vision and PatternRecognition (CVPR), pages 5696–5704, 2017. 1, 3[21] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.Deep residual learning for image recognition. In Proceed-ings of the IEEE/CVF Conference on Computer Vision andPattern Recognition (CVPR), pages 770–778, 2016. 1, 4, 8[22] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.Identity mappings in deep residual networks. In Proceedingsof the European Conference on Computer Vision (ECCV),pages 630–645. Springer, 2016. 1, 4[23] Gao Huang, Zhuang Liu, Laurens Van Der Maaten, and Kil-ian Q Weinberger. Densely connected convolutional net-In Proceedings of the IEEE/CVF Conference onworks.Computer Vision and Pattern Recognition (CVPR), pages4700–4708, 2017. 1[24] Kuan-Chih Huang, Tsung-Han Wu, Hung-Ting Su, and Win-ston H Hsu. MonoDTR: Monocular 3D object detection withdepth-aware transformer. In Proceedings of the IEEE/CVFConference on Computer Vision and Pattern Recognition(CVPR), pages 4012–4021, 2022. 1, 3[25] Lin Huang, Weisheng Li, Linlin Shen, Haojie Fu, Xue Xiao,and Suihan Xiao. YOLOCS: Object detection based on densechannel compression for feature spatial solidification. arXivpreprint arXiv:2305.04170, 2023. 3[26] Andrew Jaegle, Felix Gimeno, Andy Brock, Oriol Vinyals,Andrew Zisserman, and Joao Carreira. Perceiver: Generalperception with iterative attention. In International Confer-ence on Machine Learning (ICML), pages 4651–4664, 2021.1[27] Jacob Devlin Ming-Wei Chang Kenton and Lee KristinaToutanova. BERT: Pre-training of deep bidirectional trans-In Proceedings offormers for language understanding.NAACL-HLT, volume 1, page 2, 2019. 211[28] Chen-Yu Lee, Saining Xie, Patrick Gallagher, ZhengyouIn Ar-Zhang, and Zhuowen Tu. Deeply-supervised nets.tificial Intelligence and Statistics, pages 562–570, 2015. 1,2, 3[29] Alex Levinshtein, Alborz Rezazadeh Sereshkeh, and Kon-stantinos Derpanis. DATNet: Dense auxiliary tasks for ob-ject detection. In Proceedings of the IEEE/CVF Winter Con-ference on Applications of Computer Vision (WACV), pages1419–1427, 2020. 1, 3[30] Chuyi Li, Lulu Li, Yifei Geng, Hongliang Jiang, MengCheng, Bo Zhang, Zaidan Ke, Xiaoming Xu, and Xiangx-iang Chu. YOLOv6 v3.0: A full-scale reloading. arXivpreprint arXiv:2301.05586, 2023. 3, 7, 2, 4[31] Chuyi Li, Lulu Li, Hongliang Jiang, Kaiheng Weng, YifeiGeng, Liang Li, Zaidan Ke, Qingyuan Li, Meng Cheng,Weiqiang Nie, et al. YOLOv6: A single-stage object de-tection framework for industrial applications. arXiv preprintarXiv:2209.02976, 2022. 3[32] Hao Li, Jinguo Zhu, Xiaohu Jiang, Xizhou Zhu, HongshengLi, Chun Yuan, Xiaohua Wang, Yu Qiao, Xiaogang Wang,Wenhai Wang, et al. Uni-perceiver v2: A generalist modelfor large-scale vision and vision-language tasks. In Proceed-ings of the IEEE/CVF Conference on Computer Vision andPattern Recognition (CVPR), pages 2691–2700, 2023. 1[33] Shuai Li, Chenhang He, Ruihuang Li, and Lei Zhang. Adual weighting label assignment scheme for object detection.In Proceedings of the IEEE/CVF Conference on ComputerVision and Pattern Recognition (CVPR), pages 9387–9396,2022. 1[34] Tingting Liang, Xiaojie Chu, Yudong Liu, Yongtao Wang,Zhi Tang, Wei Chu, Jingdong Chen, and Haibin Ling. CB-Net: A composite backbone network architecture for objectIEEE Transactions on Image Processing (TIP),detection.2022. 3, 9[35] Tsung-Yi Lin, Piotr Doll´ar, Ross Girshick, Kaiming He,Feature pyra-Bharath Hariharan, and Serge Belongie.In Proceedings of themid networks for object detection.IEEE/CVF Conference on Computer Vision and PatternRecognition (CVPR), pages 2117–2125, 2017. 3[36] Zhihao Lin, Yongtao Wang, Jinhe Zhang, and Xiaojie Chu.DynamicDet: A unified dynamic architecture for object de-In Proceedings of the IEEE/CVF Conference ontection.Computer Vision and Pattern Recognition (CVPR), pages6282–6291, 2023. 3, 6, 2, 4[37] Shu Liu, Lu Qi, Haifang Qin, Jianping Shi, and Jiaya Jia.Path aggregation network for instance segmentation. In Pro-ceedings of the IEEE/CVF Conference on Computer Visionand Pattern Recognition (CVPR), pages 8759–8768, 2018.3, 5[38] Yue Liu, Yunjie Tian, Yuzhong Zhao, Hongtian Yu, LingxiXie, Yaowei Wang, Qixiang Ye, and Yunfan Liu. Vmamba:Visual state space model. arXiv preprint arXiv:2401.10166,2024. 1[39] Yudong Liu, Yongtao Wang, Siwei Wang, TingTing Liang,Qijie Zhao, Zhi Tang, and Haibin Ling. CBNet: A novelcomposite backbone network architecture for object detec-In Proceedings of the AAAI Conference on Artificialtion.Intelligence (AAAI), pages 11653–11660, 2020. 3[40] Ze Liu, Han Hu, Yutong Lin, Zhuliang Yao, Zhenda Xie,Yixuan Wei, Jia Ning, Yue Cao, Zheng Zhang, Li Dong, et al.Swin transformer v2: Scaling up capacity and resolution. InProceedings of the IEEE/CVF Conference on Computer Vi-sion and Pattern Recognition (CVPR), 2022. 1[41] Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, ZhengZhang, Stephen Lin, and Baining Guo. Swin transformer:Hierarchical vision transformer using shifted windows.InProceedings of the IEEE/CVF International Conference onComputer Vision (ICCV), pages 10012–10022, 2021. 1[42] Zhuang Liu, Hanzi Mao, Chao-Yuan Wu, Christoph Feicht-enhofer, Trevor Darrell, and Saining Xie. A ConvNet for the2020s. In Proceedings of the IEEE/CVF Conference on Com-puter Vision and Pattern Recognition (CVPR), pages 11976–11986, 2022. 1[43] Wenyu Lv, Shangliang Xu, Yian Zhao, Guanzhong Wang,Jinman Wei, Cheng Cui, Yuning Du, Qingqing Dang, andYi Liu. DETRs beat YOLOs on real-time object detection.arXiv preprint arXiv:2304.08069, 2023. 1, 3, 7, 8, 2, 4[44] Chengqi Lyu, Wenwei Zhang, Haian Huang, Yue Zhou,Yudong Wang, Yanyi Liu, Shilong Zhang, and Kai Chen.RTMDet: An empirical study of designing real-time objectdetectors. arXiv preprint arXiv:2212.07784, 2022. 8, 2, 3, 4[45] Kemal Oksuz, Baris Can Cam, Emre Akbas, and SinanKalkan. A ranking-based, balanced loss function unify-ing classification and localisation in object detection. Ad-vances in Neural Information Processing Systems (NeurIPS),33:15534–15545, 2020. 1[46] Kemal Oksuz, Baris Can Cam, Emre Akbas, and SinanKalkan. Rank & sort loss for object detection and instanceIn Proceedings of the IEEE/CVF Interna-segmentation.tional Conference on Computer Vision (ICCV), pages 3009–3018, 2021. 1[47] Joseph Redmon, Santosh Divvala, Ross Girshick, and AliFarhadi. You only look once: Unified, real-time object detec-tion. In Proceedings of the IEEE/CVF Conference on Com-puter Vision and Pattern Recognition (CVPR), pages 779–788, 2016. 3[48] Joseph Redmon and Ali Farhadi. YOLO9000: better, faster,In Proceedings of the IEEE/CVF Conference onstronger.Computer Vision and Pattern Recognition (CVPR), pages7263–7271, 2017. 3[49] Joseph Redmon and Ali Farhadi. YOLOv3: An incrementalimprovement. arXiv preprint arXiv:1804.02767, 2018. 3, 8[50] Hamid Rezatofighi, Nathan Tsoi, JunYoung Gwak, AmirSadeghian, Ian Reid, and Silvio Savarese. Generalized in-tersection over union: A metric and a loss for bounding boxIn Proceedings of the IEEE/CVF Conferenceregression.on Computer Vision and Pattern Recognition (CVPR), pages658–666, 2019. 1[51] Zhiqiang Shen, Zhuang Liu, Jianguo Li, Yu-Gang Jiang,Yurong Chen, and Xiangyang Xue. Object detection fromscratch with deep supervision. IEEE Transactions on PatternAnalysis and Machine Intelligence (TPAMI), 42(2):398–412,2019. 1, 2[52] Mohit Shridhar, Lucas Manuelli, and Dieter Fox. Perceiver-actor: A multi-task transformer for robotic manipulation.12In Conference on Robot Learning (CoRL), pages 785–799,2023. 1[53] Peize Sun, Yi Jiang, Enze Xie, Wenqi Shao, Zehuan Yuan,Changhu Wang, and Ping Luo. What makes for end-to-endobject detection? In International Conference on MachineLearning (ICML), pages 9934–9944, 2021. 3[54] Christian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet,Scott Reed, Dragomir Anguelov, Dumitru Erhan, VincentVanhoucke, and Andrew Rabinovich. Going deeper withconvolutions. In Proceedings of the IEEE/CVF Conferenceon Computer Vision and Pattern Recognition (CVPR), pages1–9, 2015. 1, 2, 3[55] Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, JonShlens, and Zbigniew Wojna. Rethinking the inception archi-tecture for computer vision. In Proceedings of the IEEE/CVFConference on Computer Vision and Pattern Recognition(CVPR), pages 2818–2826, 2016. 1[56] Zineng Tang, Jaemin Cho, Jie Lei, and Mohit Bansal.Perceiver-VL: Efficient vision-and-language modeling withIn Proceedings of the IEEE/CVFiterative latent attention.Winter Conference on Applications of Computer Vision(WACV), pages 4410–4420, 2023. 1[57] Zhi Tian, Chunhua Shen, Hao Chen, and Tong He. FCOS:Fully convolutional one-stage object detection. In Proceed-ings of the IEEE/CVF International Conference on Com-puter Vision (ICCV), pages 9627–9636, 2019. 3[58] Zhi Tian, Chunhua Shen, Hao Chen, and Tong He. FCOS:IEEEA simple and strong anchor-free object detector.Transactions on Pattern Analysis and Machine Intelligence(TPAMI), 44(4):1922–1933, 2022. 3[59] Naftali Tishby and Noga Zaslavsky. Deep learning and theinformation bottleneck principle. In IEEE Information The-ory Workshop (ITW), pages 1–5, 2015. 2, 4[60] Zhengzhong Tu, Hossein Talebi, Han Zhang, Feng Yang,Peyman Milanfar, Alan Bovik, and Yinxiao Li. MaxVIT:Multi-axis vision transformer. In Proceedings of the Euro-pean Conference on Computer Vision (ECCV), pages 459–479, 2022. 1[61] Chengcheng Wang, Wei He, Ying Nie, Jianyuan Guo,Chuanjian Liu, Kai Han, and Yunhe Wang. Gold-YOLO:Efficient object detector via gather-and-distribute mecha-nism. Advances in Neural Information Processing Systems(NeurIPS), 2023. 3, 7, 2, 4[62] Chien-Yao Wang, Alexey Bochkovskiy,and Hong-Yuan Mark Liao. Scaled-YOLOv4: Scaling cross stageIn Proceedings of the IEEE/CVF Confer-partial network.ence on Computer Vision and Pattern Recognition (CVPR),pages 13029–13038, 2021. 3[63] Chien-Yao Wang, Alexey Bochkovskiy,and Hong-Yuan Mark Liao. YOLOv7: Trainable bag-of-freebiessets new state-of-the-art for real-time object detectors.InProceedings of the IEEE/CVF Conference on ComputerVision and Pattern Recognition (CVPR), pages 7464–7475,2023. 3, 6, 7, 9, 1In Proceedings of the IEEE/CVF Conference on ComputerVision and Pattern Recognition Workshops (CVPRW), pages390–391, 2020. 3, 6, 8[65] Chien-Yao Wang, Hong-Yuan Mark Liao, and I-Hau Yeh.Designing network design strategies through gradient pathanalysis. Journal of Information Science and Engineering(JISE), 39(4):975–995, 2023. 2, 3, 6[66] Chien-Yao Wang, I-Hau Yeh, and Hong-Yuan Mark Liao.You only learn one representation: Unified network for mul-tiple tasks. Journal of Information Science & Engineering(JISE), 39(3):691–709, 2023. 2, 3, 4[67] Jianfeng Wang, Lin Song, Zeming Li, Hongbin Sun, JianEnd-to-end object detectionSun, and Nanning Zheng.In Proceedings of thewith fully convolutional network.IEEE/CVF Conference on Computer Vision and PatternRecognition (CVPR), pages 15849–15858, 2021. 1, 3[68] Liwei Wang, Chen-Yu Lee, Zhuowen Tu, and SvetlanaLazebnik. Training deeper convolutional networks with deepsupervision. arXiv preprint arXiv:1505.02496, 2015. 1, 2, 3[69] Wenhai Wang, Enze Xie, Xiang Li, Deng-Ping Fan, KaitaoSong, Ding Liang, Tong Lu, Ping Luo, and Ling Shao.Pyramid vision transformer: A versatile backbone for denseIn Proceedings of theprediction without convolutions.IEEE/CVF International Conference on Computer Vision(ICCV), pages 568–578, 2021. 1[70] Wenhai Wang, Enze Xie, Xiang Li, Deng-Ping Fan, KaitaoSong, Ding Liang, Tong Lu, Ping Luo, and Ling Shao. PVTImproved baselines with pyramid vision transformer.v2:Computational Visual Media, 8(3):415–424, 2022. 1[71] Sanghyun Woo, Shoubhik Debnath, Ronghang Hu, XinleiChen, Zhuang Liu, In So Kweon, and Saining Xie. Con-vNeXt v2: Co-designing and scaling convnets with maskedautoencoders. In Proceedings of the IEEE/CVF Conferenceon Computer Vision and Pattern Recognition (CVPR), pages16133–16142, 2023. 1, 2[72] Saining Xie, Ross Girshick, Piotr Doll´ar, Zhuowen Tu, andKaiming He. Aggregated residual transformations for deepneural networks. In Proceedings of the IEEE/CVF Confer-ence on Computer Vision and Pattern Recognition (CVPR),pages 1492–1500, 2017. 1[73] Zhenda Xie, Zheng Zhang, Yue Cao, Yutong Lin, JianminBao, Zhuliang Yao, Qi Dai, and Han Hu. SimMIM: A simpleframework for masked image modeling. In Proceedings ofthe IEEE/CVF Conference on Computer Vision and PatternRecognition (CVPR), pages 9653–9663, 2022. 2[74] Shangliang Xu, Xinxin Wang, Wenyu Lv, Qinyao Chang,Cheng Cui, Kaipeng Deng, Guanzhong Wang, QingqingDang, Shengyu Wei, Yuning Du, et al. PP-YOLOE: Anevolved version of YOLO. arXiv preprint arXiv:2203.16250,2022. 3, 8, 2, 4[75] Xianzhe Xu, Yiqi Jiang, Weihua Chen, Yilun Huang,Yuan Zhang, and Xiuyu Sun.DAMO-YOLO: A re-port on real-time object detection design. arXiv preprintarXiv:2211.15444, 2022. 3, 7, 2, 4[64] Chien-Yao Wang, Hong-Yuan Mark Liao, Yueh-Hua Wu,Ping-Yang Chen, Jun-Wei Hsieh, and I-Hau Yeh. CSPNet: Anew backbone that can enhance learning capability of CNN.[76] Renrui Zhang, Han Qiu, Tai Wang, Ziyu Guo, Ziteng Cui, YuQiao, Hongsheng Li, and Peng Gao. MonoDETR: Depth-Inguided transformer for monocular 3D object detection.13Proceedings of the IEEE/CVF International Conference onComputer Vision (ICCV), pages 9155–9166, 2023. 1, 3[77] Zhaohui Zheng, Ping Wang, Wei Liu, Jinze Li, RongguangYe, and Dongwei Ren. Distance-IoU loss: Faster and bet-ter learning for bounding box regression. In Proceedings ofthe AAAI Conference on Artificial Intelligence (AAAI), vol-ume 34, pages 12993–13000, 2020. 1[78] Dingfu Zhou, Jin Fang, Xibin Song, Chenye Guan, JunboYin, Yuchao Dai, and Ruigang Yang.IoU loss for 2D/3Dobject detection. In International Conference on 3D Vision(3DV), pages 85–94, 2019. 1[79] Benjin Zhu, Jianfeng Wang, Zhengkai Jiang, Fuhang Zong,Songtao Liu, Zeming Li, and Jian Sun. AutoAssign: Differ-entiable label assignment for dense object detection. arXivpreprint arXiv:2007.03496, 2020. 1[80] Lianghui Zhu, Bencheng Liao, Qian Zhang, Xinlong Wang,Wenyu Liu, and Xinggang Wang. Vision mamba: Efficientvisual representation learning with bidirectional state spacemodel. arXiv preprint arXiv:2401.09417, 2024. 1[81] Xizhou Zhu, Jinguo Zhu, Hao Li, Xiaoshi Wu, HongshengLi, Xiaohua Wang, and Jifeng Dai. Uni-perceiver: Pre-training unified architecture for generic perception for zero-In Proceedings of the IEEE/CVFshot and few-shot tasks.Conference on Computer Vision and Pattern Recognition(CVPR), pages 16804–16815, 2022. 1[82] Zhuofan Zong, Guanglu Song, and Yu Liu. DETRs withcollaborative hybrid assignments training. In Proceedings ofthe IEEE/CVF Conference on Computer Vision and PatternRecognition (CVPR), pages 6748–6758, 2023. 314A. Implementation DetailsTable 2. Network configurations of YOLOv9.AppendixTable 1. Hyper parameter settings of YOLOv9.hyper parameterepochsoptimizerinitial learning ratefinish learning ratelearning rate decaymomentumweight decaywarm-up epochswarm-up momentumwarm-up bias learning ratebox loss gainclass loss gainDFL loss gainHSV saturation augmentationHSV value augmentationtranslation augmentationscale augmentationmosaic augmentationMixUp augmentationcopy & paste augmentationclose mosaic epochsvalue500SGD0.010.0001linear0.9370.000530.80.17.50.51.50.70.40.10.91.00.150.315The training parameters of YOLOv9 are shown in Ta-ble 1. We fully follow the settings of YOLOv7 AF [63],which is to use SGD optimizer to train 500 epochs. We firstwarm-up for 3 epochs and only update the bias during thewarm-up stage. Next we step down from the initial learningrate 0.01 to 0.0001 in linear decay manner, and the data aug-mentation settings are listed in the bottom part of Table 1.We shut down mosaic data augmentation operations on thelast 15 epochs.Index Module012345678910111213141516171819202122ConvConvCSP-ELANDOWNCSP-ELANDOWNCSP-ELANDOWNCSP-ELANSPP-ELANUpConcatCSP-ELANUpConcatCSP-ELANDOWNConcatCSP-ELANDOWNConcatCSP-ELANPredictRoute–012345678910, 6111213, 4141516, 12171819, 92015, 18, 21Filters64128256, 128, 64256––––2, 1–Depth Size Stride33–3–3–3––––––––3––3–––2212121211211211211211––––512, 256, 128 2, 1512, 512, 256 2, 1512, 512, 256 2, 1512, 256, 256 3, 15125125121024512, 512, 256 2, 12567685121024––256, 256, 128 2, 1––512, 512, 256 2, 1––512, 512, 256 2, 15121024–The network topology of YOLOv9 completely followsYOLOv7 AF [63], that is, we replace ELAN with the pro-posed CSP-ELAN block. As listed in Table 2, the depthparameters of CSP-ELAN are represented as ELAN depthand CSP depth, respectively. As for the parameters of CSP-ELAN filters, they are represented as ELAN output fil-ter, CSP output filter, and CSP inside filter. In the down-sampling module part, we simplify CSP-DOWN module toDOWN module. DOWN module is composed of a poolinglayer with size 2 and stride 1, and a Conv layer with size 3and stride 2. Finally, we optimized the prediction layer andreplaced top, left, bottom, and right in the regression branchwith decoupled branch.1Table 3. Comparison of state-of-the-art object detectors with different training settings.Model#Param. (M) FLOPs (G) AP50:95 (%) AP50 (%) AP75 (%) APS (%) APM (%) APL (%)h Dy-YOLOv7 [36]ctarcs-morf-niarTdeniarterPteNegamInoitallitsiDegdelwonKgnitteSxelpmoCDy-YOLOv7-X [36]YOLOv9-S (Ours)YOLOv9-M (Ours)YOLOv9-C (Ours)YOLOv9-E (Ours)YOLOv9-E (Ours)YOLOv9-E (Ours)RTMDet-T [44]RTMDet-S [44]RTMDet-M [44]RTMDet-L [44]RTMDet-X [44]PPYOLOE-S [74]PPYOLOE-M [74]PPYOLOE-L [74]PPYOLOE-X [74]RT DETR-L [43]RT DETR-X [43]RT DETR-R18 [43]RT DETR-R34 [43]RT DETR-R50M [43]RT DETR-R50 [43]RT DETR-R101 [43]Gold YOLO-S [61]Gold YOLO-M [61]Gold YOLO-L [61]YOLOv6-N v3.0 [30]YOLOv6-S v3.0 [30]YOLOv6-M v3.0 [30]YOLOv6-L v3.0 [30]DAMO YOLO-T [75]DAMO YOLO-S [75]DAMO YOLO-M [75]DAMO YOLO-L [75]Gold YOLO-N [61]Gold YOLO-S [61]Gold YOLO-M [61]Gold YOLO-L [61]Gold YOLO-S [61]Gold YOLO-M [61]Gold YOLO-L [61]YOLOR-CSP [66]YOLOR-CSP-X [66]PPYOLOE+-S [74]PPYOLOE+-M [74]PPYOLOE+-L [74]PPYOLOE+-X [74]––7.120.025.334.744.057.34.89.024.752.394.97.923.452.298.43267203136427621.541.375.14.718.534.959.68.516.328.242.15.621.541.375.121.541.375.152.996.97.923.452.298.4181.7307.926.476.3102.1147.1183.9189.012.625.678.6160.4283.414.449.9110.1206.6110234609210013625946.057.5151.711.445.385.8150.718.137.861.897.312.146.057.5151.746.057.5151.7120.4226.814.449.9110.1206.653.955.046.851.453.054.555.155.641.144.649.451.552.843.049.051.452.353.054.846.548.951.353.154.345.550.252.337.545.050.052.843.647.750.451.939.946.150.953.246.451.153.352.854.843.749.852.954.772.273.263.468.170.271.772.372.857.961.966.868.870.460.566.568.969.571.673.163.866.869.671.372.762.267.569.653.161.866.970.359.463.567.268.555.963.368.270.563.468.570.971.273.160.667.170.172.058.760.050.756.157.859.260.760.6–––––46.653.055.656.857.359.4–––57.758.6–––––––46.651.155.156.7–––––––57.659.747.954.557.959.935.336.626.633.636.238.138.740.2–––––23.228.631.435.134.635.7–––34.836.0–––––––23.326.931.633.3–––––––––23.231.835.237.957.658.756.057.058.559.960.661.0–––––46.452.955.357.057.359.6–––58.058.8–––––––47.451.755.357.0–––––––––46.453.957.559.366.468.564.568.069.370.371.471.4–––––56.963.866.168.671.272.9–––70.072.1–––––––61.064.967.167.6–––––––––56.966.269.170.4B. More ComparisonWe compare YOLOv9 to state-of-the-art real-time objectdetectors trained with different methods. It mainly includesfour different training methods: (1) train-from-scratch: wehave completed most of the comparisons in the text. Hereare only list of additional data of DynamicDet [36] for com-parisons; (2) Pretrained by ImageNet:this includes twomethods of using ImageNet for supervised pretrain and self-supervised pretrain; (3) knowledge distillation: a methodto perform additional self-distillation after training is com-pleted; and (4) a more complex training process: a combi-nation of steps including pretrained by ImageNet, knowl-edge distillation, DAMO-YOLO and even additional pre-trained large object detection dataset. We show the resultsin Table 3. From this table, we can see that our proposedYOLOv9 performed better than all other methods. Com-pared with PPYOLOE+-X trained using ImageNet and Ob-jects365, our method still reduces the number of parame-ters by 55% and the amount of computation by 11%, andimproving 0.4% AP.2L (%)Model#Param. (M) FLOPs (G) APvalTable 4. Comparison of state-of-the-art object detectors with different training settings (sorted by number of parameters).M (%) APval–––64.556.956.961.0–64.9––68.0–––66.263.8–69.367.1–71.270.3–––––70.067.671.469.166.1––71.4–72.9–––72.1––70.468.650 (%) APval50:95 (%) APval37.5–53.141.1–57.9–55.939.946.850.763.447.960.643.746.660.543.046.659.443.6–61.944.647.751.163.5–61.845.0–63.846.551.456.168.1–63.446.4–63.346.1–62.245.554.567.149.853.066.549.0–66.849.453.057.870.255.167.250.4–66.848.957.371.653.054.559.271.7–66.950.0–69.651.3–68.551.1–68.250.9–67.550.257.771.353.156.768.551.955.160.772.357.970.152.955.668.951.4–68.851.557.671.252.855.660.672.8–70.352.859.473.154.8–70.953.3–70.553.2–69.652.358.672.754.3–70.452.859.773.154.859.972.054.756.869.552.3YOLOv6-N v3.0 [30] (D)RTMDet-T [44] (I)Gold YOLO-N [61] (D)YOLOv9-S (S)PPYOLOE+-S [74] (C)PPYOLOE-S [74] (I)DAMO YOLO-T [75] (D)RTMDet-S [44] (I)DAMO YOLO-S [75] (D)YOLOv6-S v3.0 [30] (D)RT DETR-R18 [43] (I)YOLOv9-M (S)Gold YOLO-S [61] (C)Gold YOLO-S [61] (D)Gold YOLO-S [61] (I)PPYOLOE+-M [74] (C)PPYOLOE-M [74] (I)RTMDet-M [44] (I)YOLOv9-C (S)DAMO YOLO-M [75] (D)RT DETR-R34 [43] (I)RT DETR-L [43] (I)YOLOv9-E (S)YOLOv6-M v3.0 [30] (D)RT DETR-R50M [43] (I)Gold YOLO-M [61] (C)Gold YOLO-M [61] (D)Gold YOLO-M [61] (I)RT DETR-R50 [43] (I)DAMO YOLO-L [75] (D)YOLOv9-E (S)PPYOLOE+-L [74] (C)PPYOLOE-L [74] (I)RTMDet-L [44] (I)YOLOR-CSP [66] (C)YOLOv9-E (S)YOLOv6-L v3.0 [30] (D)RT DETR-X [43] (I)Gold YOLO-L [61] (C)Gold YOLO-L [61] (D)Gold YOLO-L [61] (I)RT DETR-R101 [43] (I)RTMDet-X [44] (I)YOLOR-CSP-X [66] (C)PPYOLOE+-X [74] (C)PPYOLOE-X [74] (I)S (%) APval–––56.046.446.447.4–51.7––57.0–––53.952.9–58.555.3–57.359.9–––––58.057.060.657.555.3––61.0–59.6–––58.8––59.357.075 (%) APval–––26.623.223.223.3–26.9––33.6–––31.828.6–36.231.6–34.638.1–––––34.833.338.735.231.4––40.2–35.7–––36.0––37.935.111.412.612.126.414.414.418.125.637.845.36076.346.046.046.049.949.978.6102.161.892110147.185.810057.557.557.513697.3183.9110.1110.1160.4120.4189.0150.7234151.7151.7151.7259283.4226.8206.6206.64.74.85.67.17.97.98.59.016.318.52020.021.521.521.523.423.424.725.328.2313234.734.93641.341.341.34242.144.052.252.252.352.957.359.66775.175.175.17694.996.998.498.41 (S), (I), (D), (C) indicate train-from-scratch, ImageNet pretrained, knowledge distillation, and complex setting, respectively.Table 4 shows the performance of all models sorted byparameter size. Our proposed YOLOv9 is Pareto optimalin all models of different sizes. Among them, we found noother method for Pareto optimal in models with more than20M parameters. The above experimental data shows thatour YOLOv9 has excellent parameter usage efficiency.Shown in Table 5 is the performance of all participat-ing models sorted by the amount of computation. Our pro-posed YOLOv9 is Pareto optimal in all models with differ-ent scales. Among models with more than 60 GFLOPs, onlyELAN-based DAMO-YOLO and DETR-based RT DETRcan rival the proposed YOLOv9. The above comparisonresults show that YOLOv9 has the most outstanding per-formance in the trade-off between computation complexityand accuracy.3L (%)Model#Param. (M) FLOPs (G) APvalTable 5. Comparison of state-of-the-art object detectors with different training settings (sorted by amount of computation).M (%) APval–––56.956.961.0–64.564.9––––66.263.8––––67.168.0–––67.6–69.371.269.166.1–70.070.3–––––66.471.471.470.468.6–72.972.1–68.550 (%) APval50:95 (%) APval37.5–53.139.9–55.941.1–57.943.747.960.646.660.543.046.659.443.644.6–61.946.850.763.447.751.163.5–61.845.0–63.446.4–63.346.1–62.245.549.854.567.153.066.549.051.1–68.5–68.250.9–67.550.2–63.846.555.167.250.451.456.168.1–66.849.4–66.950.0–66.848.951.956.768.5–69.651.353.057.870.257.371.653.057.970.152.955.668.951.457.671.252.853.157.771.354.559.271.7–70.352.8–70.953.3–70.553.2–69.652.3–68.851.558.772.253.955.160.772.355.660.672.859.972.054.756.869.552.359.773.154.859.473.154.858.672.754.3–70.452.860.073.255.0YOLOv6-N v3.0 [30] (D)Gold YOLO-N [61] (D)RTMDet-T [44] (I)PPYOLOE+-S [74] (C)PPYOLOE-S [74] (I)DAMO YOLO-T [75] (D)RTMDet-S [44] (I)YOLOv9-S (S)DAMO YOLO-S [75] (D)YOLOv6-S v3.0 [30] (D)Gold YOLO-S [61] (C)Gold YOLO-S [61] (D)Gold YOLO-S [61] (I)PPYOLOE+-M [74] (C)PPYOLOE-M [74] (I)Gold YOLO-M [61] (C)Gold YOLO-M [61] (D)Gold YOLO-M [61] (I)RT DETR-R18 [43] (I)DAMO YOLO-M [75] (D)YOLOv9-M (S)RTMDet-M [44] (I)YOLOv6-M v3.0 [30] (D)RT DETR-R34 [43] (I)DAMO YOLO-L [75] (D)RT DETR-R50M [43] (I)YOLOv9-C (S)RT DETR-L [43] (I)PPYOLOE+-L [74] (C)PPYOLOE-L [74] (I)YOLOR-CSP [66] (C)RT DETR-R50 [43] (I)YOLOv9-E (S)YOLOv6-L v3.0 [30] (D)Gold YOLO-L [61] (C)Gold YOLO-L [61] (D)Gold YOLO-L [61] (I)RTMDet-L [44] (I)Dy-YOLOv7 [36] (S)YOLOv9-E (S)YOLOv9-E (S)PPYOLOE+-X [74] (C)PPYOLOE-X [74] (I)YOLOR-CSP-X [66] (C)RT DETR-X [43] (I)RT DETR-R101 [43] (I)RTMDet-X [44] (I)Dy-YOLOv7-X [36] (S)S (%) APval–––46.446.447.4–56.051.7––––53.952.9––––55.357.0–––57.0–58.557.357.555.3–58.059.9–––––57.660.661.059.357.0–59.658.8–58.775 (%) APval–––23.223.223.3–26.626.9––––31.828.6––––31.633.6–––33.3–36.234.635.231.4–34.838.1–––––35.338.740.237.935.1–35.736.0–36.611.412.112.614.414.418.125.626.437.845.346.046.046.049.949.957.557.557.56061.876.378.685.89297.3100102.1110110.1110.1120.4136147.1150.7151.7151.7151.7160.4181.7183.9189.0206.6206.6226.8234259283.4307.94.75.64.87.97.98.59.07.116.318.521.521.521.523.423.441.341.341.32028.220.024.734.93142.13625.33252.252.252.94234.759.675.175.175.152.3–44.057.398.498.496.9677694.9–1 (S), (I), (D), (C) indicate train-from-scratch, ImageNet pretrained, knowledge distillation, and complex setting, respectively.4', 'pdf/YOLOv9__Learning_What_You_Want_to_Learn_Using_Programmable_Gradient_Information.pdf', '[[-0.2461393624544143676757812500000000000000\n  0.0882047787308692932128906250000000000000\n  0.2085456252098083496093750000000000000000\n  0.0881516784429550170898437500000000000000\n  -0.0903084948658943176269531250000000000000\n  -0.1088174134492874145507812500000000000000\n  0.2339711487293243408203125000000000000000\n  -0.0863623246550559997558593750000000000000\n  0.0498214140534400939941406250000000000000\n  -0.2520729005336761474609375000000000000000\n  -0.2378430813550949096679687500000000000000\n  -0.1103397384285926818847656250000000000000\n  0.2766186892986297607421875000000000000000\n  0.2200381457805633544921875000000000000000\n  -0.0715140625834465026855468750000000000000\n  0.0869259834289550781250000000000000000000\n  0.1057398766279220581054687500000000000000\n  0.1999591588973999023437500000000000000000\n  0.0177912004292011260986328125000000000000\n  0.0898907259106636047363281250000000000000\n  0.1494992673397064208984375000000000000000\n  -0.1696832776069641113281250000000000000000\n  0.4749993681907653808593750000000000000000\n  0.1318902522325515747070312500000000000000\n  -0.1594199836254119873046875000000000000000\n  0.0831205546855926513671875000000000000000\n  0.0637756437063217163085937500000000000000\n  0.1990060806274414062500000000000000000000\n  0.1345200836658477783203125000000000000000\n  0.1792190521955490112304687500000000000000\n  -0.0390117056667804718017578125000000000000\n  -0.0820741206407546997070312500000000000000\n  0.2259791940450668334960937500000000000000\n  -0.2386192977428436279296875000000000000000\n  0.0567798279225826263427734375000000000000\n  -0.1075563728809356689453125000000000000000\n  -0.0228120610117912292480468750000000000000\n  -0.0441335551440715789794921875000000000000\n  0.3750110268592834472656250000000000000000\n  -0.0838597714900970458984375000000000000000\n  0.0261066313832998275756835937500000000000\n  -0.0376925989985466003417968750000000000000\n  -0.0612859316170215606689453125000000000000\n  -0.0858886539936065673828125000000000000000\n  -0.0065208673477172851562500000000000000000\n  -0.1490971297025680541992187500000000000000\n  -3.1390938758850097656250000000000000000000\n  0.1730481833219528198242187500000000000000\n  -0.1819009184837341308593750000000000000000\n  -0.2209491133689880371093750000000000000000\n  0.3118023872375488281250000000000000000000\n  0.2231430113315582275390625000000000000000\n  0.0592205785214900970458984375000000000000\n  0.2120884954929351806640625000000000000000\n  0.1319353282451629638671875000000000000000\n  0.1309136152267456054687500000000000000000\n  -0.0067018251866102218627929687500000000000\n  0.1271063387393951416015625000000000000000\n  0.2418195009231567382812500000000000000000\n  0.0676378011703491210937500000000000000000\n  0.2746869325637817382812500000000000000000\n  0.3097370862960815429687500000000000000000\n  -0.2260760068893432617187500000000000000000\n  0.1998261958360671997070312500000000000000\n  -0.1744097471237182617187500000000000000000\n  0.2434797883033752441406250000000000000000\n  -0.2703727483749389648437500000000000000000\n  0.2707164287567138671875000000000000000000\n  -0.5532405376434326171875000000000000000000\n  0.5515757799148559570312500000000000000000\n  -0.2475742995738983154296875000000000000000\n  -0.0575789809226989746093750000000000000000\n  0.1175919398665428161621093750000000000000\n  -0.0108095221221446990966796875000000000000\n  0.1979035437107086181640625000000000000000\n  -0.0984210073947906494140625000000000000000\n  0.0538098961114883422851562500000000000000\n  0.1592347919940948486328125000000000000000\n  0.2023340612649917602539062500000000000000\n  0.3294335901737213134765625000000000000000\n  0.1654319912195205688476562500000000000000\n  -0.1228573471307754516601562500000000000000\n  0.3485898971557617187500000000000000000000\n  -0.3107722401618957519531250000000000000000\n  0.0622463151812553405761718750000000000000\n  0.3456447422504425048828125000000000000000\n  0.0471116416156291961669921875000000000000\n  -0.0560880452394485473632812500000000000000\n  0.1698657870292663574218750000000000000000\n  0.1775410771369934082031250000000000000000\n  0.0792137160897254943847656250000000000000\n  -0.0385878123342990875244140625000000000000\n  0.2745848298072814941406250000000000000000\n  0.0426662489771842956542968750000000000000\n  0.1226086765527725219726562500000000000000\n  -0.1520171612501144409179687500000000000000\n  -0.1607234328985214233398437500000000000000\n  -0.1028274223208427429199218750000000000000\n  0.1671809703111648559570312500000000000000\n  0.0685109943151473999023437500000000000000\n  0.1854675263166427612304687500000000000000\n  -0.0143609233200550079345703125000000000000\n  0.1752227246761322021484375000000000000000\n  -0.5259374976158142089843750000000000000000\n  0.1441472768783569335937500000000000000000\n  -0.1840795725584030151367187500000000000000\n  -0.3452319502830505371093750000000000000000\n  -0.2865119576454162597656250000000000000000\n  -0.0657364502549171447753906250000000000000\n  -2.0030977725982666015625000000000000000000\n  0.0085263773798942565917968750000000000000\n  0.5211985707283020019531250000000000000000\n  -0.1867371648550033569335937500000000000000\n  -0.3776238858699798583984375000000000000000\n  0.2272499203681945800781250000000000000000\n  0.0522743687033653259277343750000000000000\n  0.0415123105049133300781250000000000000000\n  0.2871179580688476562500000000000000000000\n  -0.0263218432664871215820312500000000000000\n  -0.0633720234036445617675781250000000000000\n  -0.1274340003728866577148437500000000000000\n  0.3934170901775360107421875000000000000000\n  0.2116629034280776977539062500000000000000\n  -0.2514107525348663330078125000000000000000\n  -0.1361854672431945800781250000000000000000\n  0.2135167121887207031250000000000000000000\n  0.1291496753692626953125000000000000000000\n  -0.4377849400043487548828125000000000000000\n  0.4111848771572113037109375000000000000000\n  0.3286845982074737548828125000000000000000\n  -0.0216963458806276321411132812500000000000\n  0.0593284554779529571533203125000000000000\n  -0.2069549709558486938476562500000000000000\n  0.0585554949939250946044921875000000000000\n  -0.1869541406631469726562500000000000000000\n  -0.1175692453980445861816406250000000000000\n  0.2379885613918304443359375000000000000000\n  -0.0786385312676429748535156250000000000000\n  0.0203449204564094543457031250000000000000\n  0.0657728165388107299804687500000000000000\n  -0.4176194369792938232421875000000000000000\n  0.0087185408920049667358398437500000000000\n  -2.2773988246917724609375000000000000000000\n  0.0753852576017379760742187500000000000000\n  0.3023571670055389404296875000000000000000\n  -0.0682531371712684631347656250000000000000\n  -0.0344682708382606506347656250000000000000\n  0.3046592473983764648437500000000000000000\n  -0.4976996183395385742187500000000000000000\n  -0.0700605660676956176757812500000000000000\n  0.0628771260380744934082031250000000000000\n  -0.1733363717794418334960937500000000000000\n  -0.3038735389709472656250000000000000000000\n  0.0253869779407978057861328125000000000000\n  -0.1102646067738533020019531250000000000000\n  0.1781984865665435791015625000000000000000\n  -0.0973328873515129089355468750000000000000\n  -0.1269816607236862182617187500000000000000\n  0.2127669453620910644531250000000000000000\n  0.0712857693433761596679687500000000000000\n  -0.2272799313068389892578125000000000000000\n  -0.0015827938914299011230468750000000000000\n  -0.1317319422960281372070312500000000000000\n  -0.0941962003707885742187500000000000000000\n  -0.0435748547315597534179687500000000000000\n  -0.1717087179422378540039062500000000000000\n  0.2176391929388046264648437500000000000000\n  0.0626058727502822875976562500000000000000\n  -0.1780248880386352539062500000000000000000\n  -0.1521709263324737548828125000000000000000\n  0.3377269506454467773437500000000000000000\n  -0.0360486358404159545898437500000000000000\n  0.2447282075881958007812500000000000000000\n  -0.2523934245109558105468750000000000000000\n  0.2263683080673217773437500000000000000000\n  0.0529856793582439422607421875000000000000\n  0.0161749757826328277587890625000000000000\n  0.0631292536854743957519531250000000000000\n  -0.0351786725223064422607421875000000000000\n  0.0546712949872016906738281250000000000000\n  -0.3211489915847778320312500000000000000000\n  0.3097837269306182861328125000000000000000\n  -0.0331487357616424560546875000000000000000\n  -0.2617855668067932128906250000000000000000\n  -0.2660098373889923095703125000000000000000\n  0.0811107903718948364257812500000000000000\n  0.1677567660808563232421875000000000000000\n  -0.0905912593007087707519531250000000000000\n  0.0747216045856475830078125000000000000000\n  0.2461571246385574340820312500000000000000\n  -0.3206529021263122558593750000000000000000\n  -0.1025016158819198608398437500000000000000\n  0.3093702793121337890625000000000000000000\n  0.0647298991680145263671875000000000000000\n  0.4014000892639160156250000000000000000000\n  0.3273402750492095947265625000000000000000\n  0.0105446688830852508544921875000000000000\n  -0.0636376589536666870117187500000000000000\n  0.1566754430532455444335937500000000000000\n  -0.1696810126304626464843750000000000000000\n  -0.0982775986194610595703125000000000000000\n  0.0533539094030857086181640625000000000000\n  -0.2546462714672088623046875000000000000000\n  0.1863445341587066650390625000000000000000\n  -0.2349715679883956909179687500000000000000\n  2.7595617771148681640625000000000000000000\n  0.1642679274082183837890625000000000000000\n  0.0312285572290420532226562500000000000000\n  -0.0243219733238220214843750000000000000000\n  -0.1278279572725296020507812500000000000000\n  -0.0568500608205795288085937500000000000000\n  -0.2377797961235046386718750000000000000000\n  0.0093121249228715896606445312500000000000\n  -0.3455348908901214599609375000000000000000\n  0.0187149122357368469238281250000000000000\n  -0.0875069499015808105468750000000000000000\n  0.1027254760265350341796875000000000000000\n  -0.1293140053749084472656250000000000000000\n  0.0897065252065658569335937500000000000000\n  -0.1657909154891967773437500000000000000000\n  0.0973890423774719238281250000000000000000\n  0.1193153858184814453125000000000000000000\n  0.0174333825707435607910156250000000000000\n  0.3032774627208709716796875000000000000000\n  -0.1241253912448883056640625000000000000000\n  0.0664047673344612121582031250000000000000\n  -0.0139688104391098022460937500000000000000\n  -0.3802901506423950195312500000000000000000\n  0.0912517309188842773437500000000000000000\n  -1.2156615257263183593750000000000000000000\n  0.0996197015047073364257812500000000000000\n  -0.2912312746047973632812500000000000000000\n  -0.0026281699538230895996093750000000000000\n  -0.0314938575029373168945312500000000000000\n  -0.2146741896867752075195312500000000000000\n  -0.0746098607778549194335937500000000000000\n  -0.0494356341660022735595703125000000000000\n  -0.2379589378833770751953125000000000000000\n  0.2574024796485900878906250000000000000000\n  -0.2064388692378997802734375000000000000000\n  -0.0998527258634567260742187500000000000000\n  0.2805440127849578857421875000000000000000\n  -0.0183202233165502548217773437500000000000\n  -0.0465963408350944519042968750000000000000\n  -0.2582532763481140136718750000000000000000\n  0.1256517618894577026367187500000000000000\n  0.0996021628379821777343750000000000000000\n  0.0531621500849723815917968750000000000000\n  0.0381929427385330200195312500000000000000\n  -0.1122504398226737976074218750000000000000\n  0.2919421792030334472656250000000000000000\n  -0.0364064052700996398925781250000000000000\n  -0.0692528113722801208496093750000000000000\n  -0.0191909782588481903076171875000000000000\n  0.3174722790718078613281250000000000000000\n  0.0748561248183250427246093750000000000000\n  0.1316939890384674072265625000000000000000\n  -0.0423889681696891784667968750000000000000\n  -0.3883496224880218505859375000000000000000\n  -0.1245800256729125976562500000000000000000\n  -0.2104495018720626831054687500000000000000\n  0.0117888972163200378417968750000000000000\n  0.1437007188796997070312500000000000000000\n  0.1286375522613525390625000000000000000000\n  -0.1313569545745849609375000000000000000000\n  -0.0393867082893848419189453125000000000000\n  0.3966639041900634765625000000000000000000\n  -0.4354940652847290039062500000000000000000\n  0.2881127893924713134765625000000000000000\n  -0.1250502765178680419921875000000000000000\n  -0.1582488268613815307617187500000000000000\n  -0.0085663590580224990844726562500000000000\n  -0.3136597871780395507812500000000000000000\n  -2.4099645614624023437500000000000000000000\n  -0.2543191015720367431640625000000000000000\n  -0.0640358328819274902343750000000000000000\n  0.4623171389102935791015625000000000000000\n  -0.1987777799367904663085937500000000000000\n  -0.0217185430228710174560546875000000000000\n  -0.0055454149842262268066406250000000000000\n  0.0984120666980743408203125000000000000000\n  0.2381324917078018188476562500000000000000\n  0.0821723192930221557617187500000000000000\n  -0.0316595956683158874511718750000000000000\n  0.0982425063848495483398437500000000000000\n  0.1374568492174148559570312500000000000000\n  0.0872362256050109863281250000000000000000\n  -0.1852555274963378906250000000000000000000\n  0.5775738954544067382812500000000000000000\n  -0.0121443746611475944519042968750000000000\n  0.0052598849870264530181884765625000000000\n  0.2459695339202880859375000000000000000000\n  -0.1089938431978225708007812500000000000000\n  0.1431892812252044677734375000000000000000\n  0.1310418993234634399414062500000000000000\n  -0.3389140963554382324218750000000000000000\n  0.2016223073005676269531250000000000000000\n  -0.0119520425796508789062500000000000000000\n  -0.0357715822756290435791015625000000000000\n  -0.0110669359564781188964843750000000000000\n  -0.4539188146591186523437500000000000000000\n  -0.0874293893575668334960937500000000000000\n  0.0391261391341686248779296875000000000000\n  0.0826519951224327087402343750000000000000\n  -0.0254659708589315414428710937500000000000\n  0.1103206425905227661132812500000000000000\n  -0.1915007233619689941406250000000000000000\n  -0.1348630189895629882812500000000000000000\n  -3.9218652248382568359375000000000000000000\n  -0.1694570779800415039062500000000000000000\n  -0.3115774691104888916015625000000000000000\n  -0.3581146895885467529296875000000000000000\n  0.1551073938608169555664062500000000000000\n  -0.1078411936759948730468750000000000000000\n  0.5359758138656616210937500000000000000000\n  0.0648907572031021118164062500000000000000\n  0.0330517776310443878173828125000000000000\n  0.0094308182597160339355468750000000000000\n  -0.1117596700787544250488281250000000000000\n  0.1945133954286575317382812500000000000000\n  -0.1671983450651168823242187500000000000000\n  -0.1291748434305191040039062500000000000000\n  -0.1658311784267425537109375000000000000000\n  0.1321360915899276733398437500000000000000\n  0.4245670437812805175781250000000000000000\n  0.0092207752168178558349609375000000000000\n  0.0947068259119987487792968750000000000000\n  -0.0025634169578552246093750000000000000000\n  -0.2477008104324340820312500000000000000000\n  -0.1090551614761352539062500000000000000000\n  0.0518953204154968261718750000000000000000\n  0.2684487998485565185546875000000000000000\n  0.2429763525724411010742187500000000000000\n  0.2226807326078414916992187500000000000000\n  -0.5902801752090454101562500000000000000000\n  -0.1019605845212936401367187500000000000000\n  0.0374027937650680541992187500000000000000\n  -0.1399046033620834350585937500000000000000\n  0.1068362668156623840332031250000000000000\n  -0.4787433445453643798828125000000000000000\n  -0.1859681010246276855468750000000000000000\n  0.1776347756385803222656250000000000000000\n  -0.1828810572624206542968750000000000000000\n  -0.2096273452043533325195312500000000000000\n  0.0275973435491323471069335937500000000000\n  0.1692434251308441162109375000000000000000\n  0.3727343082427978515625000000000000000000\n  -0.2180633842945098876953125000000000000000\n  0.2538617253303527832031250000000000000000\n  0.3806513249874114990234375000000000000000\n  0.0819949358701705932617187500000000000000\n  -0.0702247023582458496093750000000000000000\n  0.3844337761402130126953125000000000000000\n  -0.0635504499077796936035156250000000000000\n  0.0723332315683364868164062500000000000000\n  0.4337463974952697753906250000000000000000\n  0.0890959203243255615234375000000000000000\n  0.0811980366706848144531250000000000000000\n  -0.0094064883887767791748046875000000000000\n  0.1506208330392837524414062500000000000000\n  1.0657607316970825195312500000000000000000\n  -0.0907155349850654602050781250000000000000\n  0.0814225375652313232421875000000000000000\n  -0.0825072154402732849121093750000000000000\n  0.1124553233385086059570312500000000000000\n  -0.0101514235138893127441406250000000000000\n  -0.0923375338315963745117187500000000000000\n  0.2414999306201934814453125000000000000000\n  0.3571192622184753417968750000000000000000\n  -0.1430454105138778686523437500000000000000\n  0.1953193694353103637695312500000000000000\n  -0.1344188451766967773437500000000000000000\n  0.0554036423563957214355468750000000000000\n  -0.1524603962898254394531250000000000000000\n  0.2946031987667083740234375000000000000000\n  -0.0560186728835105895996093750000000000000\n  -0.2230214476585388183593750000000000000000\n  -0.1054693460464477539062500000000000000000\n  -0.4688873589038848876953125000000000000000\n  0.1867841184139251708984375000000000000000\n  0.2454743981361389160156250000000000000000\n  -0.4551137387752532958984375000000000000000\n  0.1937617659568786621093750000000000000000\n  -0.0771100595593452453613281250000000000000\n  -0.1051967293024063110351562500000000000000\n  0.1801799237728118896484375000000000000000\n  -0.0568943843245506286621093750000000000000\n  0.1039177030324935913085937500000000000000\n  -0.7451091408729553222656250000000000000000\n  0.0925897434353828430175781250000000000000\n  0.1202854961156845092773437500000000000000\n  0.3312881588935852050781250000000000000000\n  -0.1847933679819107055664062500000000000000\n  0.1745192259550094604492187500000000000000\n  -0.0176103282719850540161132812500000000000\n  -0.1233124360442161560058593750000000000000\n  -0.2102144360542297363281250000000000000000\n  0.4747673869132995605468750000000000000000\n  -0.1388841122388839721679687500000000000000\n  -0.0357521735131740570068359375000000000000\n  0.2905557155609130859375000000000000000000\n  0.0942327380180358886718750000000000000000\n  -0.0764425545930862426757812500000000000000\n  0.0555362664163112640380859375000000000000\n  0.2208127379417419433593750000000000000000\n  -0.2850230932235717773437500000000000000000\n  0.1117046251893043518066406250000000000000\n  -0.3806998431682586669921875000000000000000\n  0.3403900265693664550781250000000000000000\n  -0.3167297840118408203125000000000000000000\n  -0.1641069352626800537109375000000000000000\n  -0.1211013868451118469238281250000000000000\n  -0.1713717132806777954101562500000000000000\n  -0.0165398120880126953125000000000000000000\n  -0.2979701161384582519531250000000000000000\n  0.0575418472290039062500000000000000000000\n  0.0969747230410575866699218750000000000000\n  0.0823877453804016113281250000000000000000\n  -0.1726796627044677734375000000000000000000\n  0.4742178618907928466796875000000000000000\n  -0.2516765892505645751953125000000000000000\n  0.2118124067783355712890625000000000000000\n  1.0172204971313476562500000000000000000000\n  -0.3347363770008087158203125000000000000000\n  -0.2714084684848785400390625000000000000000\n  0.1272054016590118408203125000000000000000\n  0.1931281834840774536132812500000000000000\n  -0.0451032519340515136718750000000000000000\n  -0.0583672672510147094726562500000000000000\n  -0.0390630364418029785156250000000000000000\n  0.0527629777789115905761718750000000000000\n  0.1891848444938659667968750000000000000000\n  -0.2071588486433029174804687500000000000000\n  0.1234417632222175598144531250000000000000\n  0.1635489314794540405273437500000000000000\n  -0.1157438009977340698242187500000000000000\n  -0.2257174253463745117187500000000000000000\n  -0.2135993838310241699218750000000000000000\n  0.1829434335231781005859375000000000000000\n  -0.1006477326154708862304687500000000000000\n  -0.4037601351737976074218750000000000000000\n  -0.2953754067420959472656250000000000000000\n  -0.0085442811250686645507812500000000000000\n  -0.1247043013572692871093750000000000000000\n  -0.0672230497002601623535156250000000000000\n  0.1753425002098083496093750000000000000000\n  0.1768881976604461669921875000000000000000\n  -0.0080845095217227935791015625000000000000\n  0.2982315719127655029296875000000000000000\n  0.2570765018463134765625000000000000000000\n  0.0681419596076011657714843750000000000000\n  0.3605852425098419189453125000000000000000\n  0.2794806957244873046875000000000000000000\n  0.1764125078916549682617187500000000000000\n  0.0416663438081741333007812500000000000000\n  0.1061236262321472167968750000000000000000\n  -0.0975102037191390991210937500000000000000\n  0.0981106162071228027343750000000000000000\n  0.0555628687143325805664062500000000000000\n  -0.6566325426101684570312500000000000000000\n  0.0342925265431404113769531250000000000000\n  -0.0714370682835578918457031250000000000000\n  0.2643816471099853515625000000000000000000\n  0.4466491341590881347656250000000000000000\n  -0.1176705136895179748535156250000000000000\n  -0.1328645348548889160156250000000000000000\n  -0.2622300982475280761718750000000000000000\n  -0.0921615287661552429199218750000000000000\n  0.3328664302825927734375000000000000000000\n  0.1012666746973991394042968750000000000000\n  -1.7530581951141357421875000000000000000000\n  0.1331389099359512329101562500000000000000\n  0.1562172621488571166992187500000000000000\n  0.0001130402088165283203125000000000000000\n  0.1617526412010192871093750000000000000000\n  -0.0491286888718605041503906250000000000000\n  -0.0536105446517467498779296875000000000000\n  -0.0433621108531951904296875000000000000000\n  0.2732418179512023925781250000000000000000\n  0.1195808798074722290039062500000000000000\n  -0.1911835968494415283203125000000000000000\n  -0.1495657265186309814453125000000000000000\n  0.1244155019521713256835937500000000000000\n  0.0395801067352294921875000000000000000000\n  0.2780255675315856933593750000000000000000\n  -0.0449091494083404541015625000000000000000\n  -0.0084963738918304443359375000000000000000\n  0.2365524172782897949218750000000000000000\n  -0.0845640301704406738281250000000000000000\n  0.2004884183406829833984375000000000000000\n  0.0191909335553646087646484375000000000000\n  0.7547170519828796386718750000000000000000\n  0.1222203224897384643554687500000000000000\n  0.2128672301769256591796875000000000000000\n  0.2029897272586822509765625000000000000000\n  -0.1820976585149765014648437500000000000000\n  -0.1618736833333969116210937500000000000000\n  0.4298117756843566894531250000000000000000\n  0.1246978640556335449218750000000000000000\n  -0.1165871322154998779296875000000000000000\n  0.1655751764774322509765625000000000000000\n  -0.4172059893608093261718750000000000000000\n  0.0686027705669403076171875000000000000000\n  -0.0246389284729957580566406250000000000000\n  0.4705267250537872314453125000000000000000\n  0.2144178152084350585937500000000000000000\n  -0.1330263167619705200195312500000000000000\n  0.0715989693999290466308593750000000000000\n  0.0209740716964006423950195312500000000000\n  0.3056932687759399414062500000000000000000\n  -0.2777669131755828857421875000000000000000\n  0.3376619219779968261718750000000000000000\n  0.4707831144332885742187500000000000000000\n  0.1092545986175537109375000000000000000000\n  0.1177945062518119812011718750000000000000\n  0.1574075520038604736328125000000000000000\n  -0.1404619365930557250976562500000000000000\n  -0.3582990765571594238281250000000000000000\n  0.0601831674575805664062500000000000000000\n  0.0348888263106346130371093750000000000000\n  0.1875944137573242187500000000000000000000\n  0.1710089147090911865234375000000000000000\n  0.0901354402303695678710937500000000000000\n  -0.3103052675724029541015625000000000000000\n  -0.2639082074165344238281250000000000000000\n  -0.0319280475378036499023437500000000000000\n  -0.0309523437172174453735351562500000000000\n  -0.1308518648147583007812500000000000000000\n  -0.0136653287336230278015136718750000000000\n  -0.0317221544682979583740234375000000000000\n  0.0853547826409339904785156250000000000000\n  -0.4494339227676391601562500000000000000000\n  0.2382058501243591308593750000000000000000\n  0.1163923591375350952148437500000000000000\n  -0.1160649210214614868164062500000000000000\n  -0.6607686281204223632812500000000000000000\n  0.3061181008815765380859375000000000000000\n  -0.0295340046286582946777343750000000000000\n  -0.0414401292800903320312500000000000000000\n  -0.2722190320491790771484375000000000000000\n  0.2979911267757415771484375000000000000000\n  -0.0356567725539207458496093750000000000000\n  -0.4912999570369720458984375000000000000000\n  0.0068951044231653213500976562500000000000\n  0.0514637678861618041992187500000000000000\n  0.1659497469663619995117187500000000000000\n  0.2797828912734985351562500000000000000000\n  0.3712728917598724365234375000000000000000\n  -0.1637519598007202148437500000000000000000\n  -0.3503734171390533447265625000000000000000\n  0.1270700544118881225585937500000000000000\n  -0.4130846560001373291015625000000000000000\n  -0.2859075963497161865234375000000000000000\n  0.3406848013401031494140625000000000000000\n  0.0202673505991697311401367187500000000000\n  -0.0505675375461578369140625000000000000000\n  0.0271953828632831573486328125000000000000\n  0.1140924245119094848632812500000000000000\n  0.1131599247455596923828125000000000000000\n  0.0048690643161535263061523437500000000000\n  -0.2907350063323974609375000000000000000000\n  -0.2854954302310943603515625000000000000000\n  0.0674394965171813964843750000000000000000\n  0.3815332055091857910156250000000000000000\n  0.3025884032249450683593750000000000000000\n  -0.2125575542449951171875000000000000000000\n  -0.0557054542005062103271484375000000000000\n  -0.1537322998046875000000000000000000000000\n  -0.2171000689268112182617187500000000000000\n  0.0149295944720506668090820312500000000000\n  -0.1469361484050750732421875000000000000000\n  0.0452229306101799011230468750000000000000\n  -0.1534119993448257446289062500000000000000\n  0.1713307797908782958984375000000000000000\n  -0.0462095364928245544433593750000000000000\n  -0.0875146389007568359375000000000000000000\n  0.3733239471912384033203125000000000000000\n  -0.2657512426376342773437500000000000000000\n  -0.2532340288162231445312500000000000000000\n  0.1434055268764495849609375000000000000000\n  -0.0253078490495681762695312500000000000000\n  0.1396377235651016235351562500000000000000\n  0.0734023898839950561523437500000000000000\n  -0.0405264720320701599121093750000000000000\n  0.1800656914710998535156250000000000000000\n  0.0282125920057296752929687500000000000000\n  -0.2980645596981048583984375000000000000000\n  -0.0862734615802764892578125000000000000000\n  1.6407508850097656250000000000000000000000\n  0.1686923950910568237304687500000000000000\n  0.2666181921958923339843750000000000000000\n  0.0961234346032142639160156250000000000000\n  0.3100343942642211914062500000000000000000\n  0.1507060527801513671875000000000000000000\n  0.1912916004657745361328125000000000000000\n  -0.0502870231866836547851562500000000000000\n  -0.0795570984482765197753906250000000000000\n  0.0460341796278953552246093750000000000000\n  -0.2079335749149322509765625000000000000000\n  0.0525466762483119964599609375000000000000\n  -0.1474486440420150756835937500000000000000\n  -0.0400157496333122253417968750000000000000\n  0.1479229629039764404296875000000000000000\n  0.4179852008819580078125000000000000000000\n  0.0077438428997993469238281250000000000000\n  -0.1744948923587799072265625000000000000000\n  -0.3045710921287536621093750000000000000000\n  -0.0527788661420345306396484375000000000000\n  -0.0978608578443527221679687500000000000000\n  0.0213126409798860549926757812500000000000\n  0.3648539781570434570312500000000000000000\n  -0.0688353553414344787597656250000000000000\n  -0.3154717385768890380859375000000000000000\n  0.0666189864277839660644531250000000000000\n  0.2471508830785751342773437500000000000000\n  -0.1614445596933364868164062500000000000000\n  -0.1221379339694976806640625000000000000000\n  -0.1627447456121444702148437500000000000000\n  -0.0565656386315822601318359375000000000000\n  0.1181027144193649291992187500000000000000\n  0.2234182059764862060546875000000000000000\n  0.1106685921549797058105468750000000000000\n  0.0230056792497634887695312500000000000000\n  -0.0080451145768165588378906250000000000000\n  -0.1007806360721588134765625000000000000000\n  -0.1686504036188125610351562500000000000000\n  -0.0509558431804180145263671875000000000000\n  0.0601478703320026397705078125000000000000\n  0.0135938832536339759826660156250000000000\n  -0.1509208083152770996093750000000000000000\n  0.3374243676662445068359375000000000000000\n  -0.2084261029958724975585937500000000000000\n  0.0230191107839345932006835937500000000000\n  0.1130037009716033935546875000000000000000\n  0.1009765490889549255371093750000000000000\n  -0.4371096193790435791015625000000000000000\n  0.3362319469451904296875000000000000000000\n  0.5527070164680480957031250000000000000000\n  -0.0590620934963226318359375000000000000000\n  -0.1161796227097511291503906250000000000000\n  -0.1139035448431968688964843750000000000000\n  0.1017178297042846679687500000000000000000\n  -0.1640778183937072753906250000000000000000\n  0.1613727807998657226562500000000000000000\n  -0.0230339001864194869995117187500000000000\n  -0.3601811528205871582031250000000000000000\n  -0.1676610410213470458984375000000000000000\n  0.2694737613201141357421875000000000000000\n  -0.0915691554546356201171875000000000000000\n  0.4951941668987274169921875000000000000000\n  0.4447844028472900390625000000000000000000\n  -0.4444936513900756835937500000000000000000\n  -0.0617646053433418273925781250000000000000\n  0.0513172671198844909667968750000000000000\n  0.0045258132740855216979980468750000000000\n  0.0726811364293098449707031250000000000000\n  -0.2364112734794616699218750000000000000000\n  0.0178222544491291046142578125000000000000\n  0.0736520215868949890136718750000000000000\n  0.1679363697767257690429687500000000000000\n  0.2805446684360504150390625000000000000000\n  0.3929924964904785156250000000000000000000\n  0.0112199578434228897094726562500000000000\n  -0.1753184944391250610351562500000000000000\n  0.0182890668511390686035156250000000000000\n  -0.1444738656282424926757812500000000000000\n  -0.0936412364244461059570312500000000000000\n  -1.8951078653335571289062500000000000000000\n  0.0382210016250610351562500000000000000000\n  0.3000581562519073486328125000000000000000\n  0.5490064620971679687500000000000000000000\n  0.1382215917110443115234375000000000000000\n  -0.0727246329188346862792968750000000000000\n  -0.0722062811255455017089843750000000000000\n  0.0425523407757282257080078125000000000000\n  -0.0565874017775058746337890625000000000000\n  0.1658155918121337890625000000000000000000\n  0.2414480298757553100585937500000000000000\n  0.2571003139019012451171875000000000000000\n  0.3583932220935821533203125000000000000000\n  0.0837895348668098449707031250000000000000\n  -0.0389387458562850952148437500000000000000\n  0.0811660960316658020019531250000000000000\n  -0.0637571215629577636718750000000000000000\n  0.0387665480375289916992187500000000000000\n  -0.1873923540115356445312500000000000000000\n  -0.1071285307407379150390625000000000000000\n  -0.1072445660829544067382812500000000000000\n  0.3385023176670074462890625000000000000000\n  -0.1968974769115447998046875000000000000000\n  -0.1870239078998565673828125000000000000000\n  -0.1680403500795364379882812500000000000000\n  0.4517403542995452880859375000000000000000\n  0.0564493983983993530273437500000000000000\n  -0.3377945125102996826171875000000000000000\n  0.2676101922988891601562500000000000000000\n  -0.0313538089394569396972656250000000000000\n  0.0741268992424011230468750000000000000000\n  0.2932779788970947265625000000000000000000\n  -0.0542796254158020019531250000000000000000\n  0.1352778971195220947265625000000000000000\n  0.1820324361324310302734375000000000000000\n  -0.4265912175178527832031250000000000000000\n  0.2766666710376739501953125000000000000000\n  -0.4029114246368408203125000000000000000000\n  -0.0900552421808242797851562500000000000000\n  0.2603954374790191650390625000000000000000\n  -0.0184593945741653442382812500000000000000\n  0.3267085552215576171875000000000000000000\n  0.0122863724827766418457031250000000000000\n  0.2355808913707733154296875000000000000000\n  -0.0872364491224288940429687500000000000000\n  0.0976960062980651855468750000000000000000\n  0.2949783504009246826171875000000000000000\n  -0.2903108000755310058593750000000000000000\n  0.5915802121162414550781250000000000000000\n  -0.2713888287544250488281250000000000000000\n  -0.0341483056545257568359375000000000000000\n  0.0519502125680446624755859375000000000000\n  0.0080667026340961456298828125000000000000\n  0.0869495645165443420410156250000000000000\n  0.1324618011713027954101562500000000000000\n  0.1168187558650970458984375000000000000000\n  0.0663327947258949279785156250000000000000\n  -0.0063837999477982521057128906250000000000\n  -0.0262173358350992202758789062500000000000\n  -0.1935252547264099121093750000000000000000\n  -0.2373812794685363769531250000000000000000\n  0.2620297670364379882812500000000000000000\n  -0.0504738539457321166992187500000000000000\n  0.2500510811805725097656250000000000000000\n  0.1601144671440124511718750000000000000000\n  -0.1656916588544845581054687500000000000000\n  -0.3112721145153045654296875000000000000000\n  0.0582664608955383300781250000000000000000\n  -0.0942983776330947875976562500000000000000\n  -0.1452835202217102050781250000000000000000\n  -0.0785308480262756347656250000000000000000\n  -0.2361581474542617797851562500000000000000\n  0.0952342376112937927246093750000000000000\n  0.0266237147152423858642578125000000000000\n  0.1866415739059448242187500000000000000000\n  -0.1721097677946090698242187500000000000000\n  0.1840407699346542358398437500000000000000\n  0.2830873429775238037109375000000000000000\n  0.1324076652526855468750000000000000000000\n  0.3897080719470977783203125000000000000000\n  -0.0114605315029621124267578125000000000000\n  -0.1118944510817527770996093750000000000000\n  0.1574939638376235961914062500000000000000\n  -0.2669536769390106201171875000000000000000\n  0.2641922235488891601562500000000000000000\n  -5.2806782722473144531250000000000000000000\n  -0.1700581610202789306640625000000000000000\n  -0.3054938018321990966796875000000000000000\n  -0.0195060577243566513061523437500000000000\n  -0.3304924070835113525390625000000000000000\n  -0.2716582119464874267578125000000000000000\n  0.2568496465682983398437500000000000000000\n  -0.1814253926277160644531250000000000000000\n  -0.0077776387333869934082031250000000000000\n  -0.1420368999242782592773437500000000000000\n  0.1951476931571960449218750000000000000000\n  -0.0897792950272560119628906250000000000000\n  -0.0942759662866592407226562500000000000000\n  0.1441369503736495971679687500000000000000\n  0.5550062656402587890625000000000000000000\n  0.1288063079118728637695312500000000000000]]');

-- --------------------------------------------------------

--
-- Table structure for table `ms_user`
--
-- Creation: Jun 02, 2024 at 09:15 AM
--

CREATE TABLE `ms_user` (
  `UserNim` char(10) NOT NULL,
  `UserName` varchar(255) NOT NULL,
  `UserEmail` varchar(255) NOT NULL,
  `UserPassword` varchar(255) NOT NULL
) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4 COLLATE=utf8mb4_general_ci;

--
-- Dumping data for table `ms_user`
--

INSERT INTO `ms_user` (`UserNim`, `UserName`, `UserEmail`, `UserPassword`) VALUES
('2602069886', 'Reynaldi Adidarma', 'reynaldi@binus.ac.id', 'reynaldi');

--
-- Indexes for dumped tables
--

--
-- Indexes for table `ms_file`
--
ALTER TABLE `ms_file`
  ADD PRIMARY KEY (`file_id`);

--
-- Indexes for table `ms_user`
--
ALTER TABLE `ms_user`
  ADD PRIMARY KEY (`UserNim`);

--
-- AUTO_INCREMENT for dumped tables
--

--
-- AUTO_INCREMENT for table `ms_file`
--
ALTER TABLE `ms_file`
  MODIFY `file_id` int(11) NOT NULL AUTO_INCREMENT, AUTO_INCREMENT=9;
COMMIT;

/*!40101 SET CHARACTER_SET_CLIENT=@OLD_CHARACTER_SET_CLIENT */;
/*!40101 SET CHARACTER_SET_RESULTS=@OLD_CHARACTER_SET_RESULTS */;
/*!40101 SET COLLATION_CONNECTION=@OLD_COLLATION_CONNECTION */;
